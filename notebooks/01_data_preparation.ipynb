{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Dataset cargado: 2484 registros\n",
      "Columnas disponibles: ['ID', 'Resume_str', 'Resume_html', 'Category']\n",
      "Texto limpiado correctamente\n"
     ]
    }
   ],
   "source": [
    "# 1. CARGA Y LIMPIEZA INICIAL\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuración de rutas\n",
    "RAW_PATH = Path(\"../data/raw/Resume/Resume.csv\") \n",
    "PROCESSED_PATH = Path(\"../data/processed/\")\n",
    "PROCESSED_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Carga de datos\n",
    "print(\"Cargando datos...\")\n",
    "if RAW_PATH.exists():\n",
    "    df = pd.read_csv(RAW_PATH)\n",
    "    print(f\"Dataset cargado: {len(df)} registros\")\n",
    "    print(f\"Columnas disponibles: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"ERROR: No se encuentra el archivo {RAW_PATH}\")\n",
    "    print(\"Asegúrate de tener el dataset en la carpeta correcta\")\n",
    "\n",
    "# Limpieza básica de texto\n",
    "if \"Resume_str\" in df.columns:\n",
    "    df[\"text\"] = df[\"Resume_str\"].fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    print(\"Texto limpiado correctamente\")\n",
    "elif \"Resume\" in df.columns:\n",
    "    df[\"text\"] = df[\"Resume\"].fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    print(\"Usando columna 'Resume' como texto principal\")\n",
    "else:\n",
    "    print(\"ERROR: No se encuentra columna de texto del CV\")\n",
    "    print(\"Columnas disponibles:\", df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    df['Category'].str.contains('engineer', case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Probando anonimización en los primeros 5 registros...\n",
      "Anonimización funcionando correctamente\n",
      "Aplicando anonimización a todo el dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Solo proceder con dataset completo si la muestra funciona\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAplicando anonimización a todo el dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mtext_anonymized\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43manonymize_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnonimización completada\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36manonymize_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Análisis de entidades sensibles\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Anonimización\u001b[39;00m\n\u001b[32m     19\u001b[39m     anonymized = anonymizer.anonymize(text=text, analyzer_results=results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\presidio_analyzer\\analyzer_engine.py:221\u001b[39m, in \u001b[36mAnalyzerEngine.analyze\u001b[39m\u001b[34m(self, text, language, entities, correlation_id, score_threshold, return_decision_process, ad_hoc_recognizers, context, allow_list, allow_list_match, regex_flags, nlp_artifacts)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# run the nlp pipeline over the given text, store the results in\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# a NlpArtifacts instance\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nlp_artifacts:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     nlp_artifacts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_decision_process:\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mself\u001b[39m.app_tracer.trace(\n\u001b[32m    225\u001b[39m         correlation_id, \u001b[33m\"\u001b[39m\u001b[33mnlp artifacts:\u001b[39m\u001b[33m\"\u001b[39m + nlp_artifacts.to_json()\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\presidio_analyzer\\nlp_engine\\spacy_nlp_engine.py:111\u001b[39m, in \u001b[36mSpacyNlpEngine.process_text\u001b[39m\u001b[34m(self, text, language)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nlp:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNLP engine is not loaded. Consider calling .load()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._doc_to_nlp_artifact(doc, language)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:53\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:343\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.set_annotations\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\ner.pyx:275\u001b[39m, in \u001b[36mspacy.pipeline._parser_internals.ner.BiluoPushDown.set_annotations\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:814\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.set_ents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:127\u001b[39m, in \u001b[36mspacy.tokens.doc.SetEntsDefault.values\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\enum.py:828\u001b[39m, in \u001b[36mEnumType.__members__\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[33;03m    Return the number of members (no aliases)\u001b[39;00m\n\u001b[32m    825\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m._member_names_)\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;129m@bltns\u001b[39m.property\n\u001b[32m    829\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__members__\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[32m    830\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    831\u001b[39m \u001b[33;03m    Returns a mapping of member name->value.\u001b[39;00m\n\u001b[32m    832\u001b[39m \n\u001b[32m    833\u001b[39m \u001b[33;03m    This mapping lists all enum members, including aliases. Note that this\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[33;03m    is a read-only view of the internal mapping.\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m MappingProxyType(\u001b[38;5;28mcls\u001b[39m._member_map_)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2. ANONIMIZACIÓN DE DATOS SENSIBLES\n",
    "try:\n",
    "    from presidio_analyzer import AnalyzerEngine\n",
    "    from presidio_anonymizer import AnonymizerEngine\n",
    "    \n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "    \n",
    "    def anonymize_text(text):\n",
    "        \"\"\"Anonimiza información personal en el texto del CV\"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # Análisis de entidades sensibles\n",
    "            results = analyzer.analyze(text=text, language=\"en\")\n",
    "            \n",
    "            # Anonimización\n",
    "            anonymized = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "            return anonymized.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error en anonimización: {e}\")\n",
    "            return text\n",
    "    \n",
    "    # Aplicar anonimización a una muestra para probar\n",
    "    if 'text' in df.columns and len(df) > 0:\n",
    "        print(\"Probando anonimización en los primeros 5 registros...\")\n",
    "        df_sample = df.head(5).copy()\n",
    "        df_sample[\"text_anonymized\"] = df_sample[\"text\"].apply(anonymize_text)\n",
    "        print(\"Anonimización funcionando correctamente\")\n",
    "        \n",
    "        # Solo proceder con dataset completo si la muestra funciona\n",
    "        print(\"Aplicando anonimización a todo el dataset...\")\n",
    "        df[\"text_anonymized\"] = df[\"text\"].apply(anonymize_text)\n",
    "        print(\"Anonimización completada\")\n",
    "    else:\n",
    "        print(\"SALTANDO: Anonimización - no hay texto para procesar\")\n",
    "        df[\"text_anonymized\"] = df.get(\"text\", \"\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"WARNING: Presidio no está instalado. Saltando anonimización...\")\n",
    "    print(\"Para instalar: pip install presidio-analyzer presidio-anonymizer\")\n",
    "    df[\"text_anonymized\"] = df.get(\"text\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente Azure OpenAI configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "# 3. EXTRACCIÓN ESTRUCTURADA CON LLM\n",
    "import json\n",
    "import tenacity\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Esquema JSON objetivo para extracción\n",
    "SCHEMA_TEMPLATE = {\n",
    "    \"education_level\": None,        # \"PhD | Master | Bachelor | High School | Other\"\n",
    "    \"discipline\": None,             # Campo de estudio/especialización\n",
    "    \"years_total_experience\": None, # Años totales de experiencia (número)\n",
    "    \"years_skill_main\": None,       # Años en habilidad principal (número)\n",
    "    \"current_job_title\": None,      # Título actual del trabajo\n",
    "    \"skills\": [],                   # Lista de habilidades\n",
    "    \"languages\": {},                # {\"English\": \"C2\", \"Spanish\": \"B2\"}\n",
    "    \"certifications\": [],           # Lista de certificaciones\n",
    "    \"num_promotions\": None,         # Número de promociones (número)\n",
    "    \"avg_tenure_months\": None,      # Promedio de duración en empleos en meses (número)\n",
    "    \"gap_months_last5y\": None,      # Meses de brecha en últimos 5 años (número)\n",
    "    \"location_city\": None,          # Ciudad de residencia\n",
    "    \"work_authorization\": None,     # Estado de autorización de trabajo\n",
    "    \"gender\": None,\n",
    "    \"age_range\": None\n",
    "    \n",
    "    # Nota: El campo 'apto' se agregará después de la extracción basado en reglas de negocio\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "Eres un extractor experto de CV. Analiza el texto del currículum y devuelve ÚNICAMENTE un JSON válido con este esquema exacto:\n",
    "\n",
    "{json.dumps(SCHEMA_TEMPLATE, indent=2)}\n",
    "\n",
    "INSTRUCCIONES CRÍTICAS:\n",
    "1. Devuelve SOLO el JSON, sin explicaciones ni comentarios\n",
    "2. Si un campo no está presente o no se puede determinar, inventatelo con coherencia\n",
    "3. Para arrays vacíos usa []\n",
    "4. Para objects vacíos usa {{}}\n",
    "5. Los números deben ser enteros, no strings\n",
    "6. education_level debe ser uno de: \"PhD\", \"Master\", \"Bachelor\", \"Associate\", \"High School\", \"Other\"\n",
    "7. Para languages usa códigos como \"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\" o \"Native\"\n",
    "8. NO inventes datos que no estén en el texto\n",
    "9. Para engineering roles, enfócate en habilidades técnicas relevantes \n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    from openai import AzureOpenAI\n",
    "    \n",
    "    # Configuración del cliente Azure OpenAI\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_KEY\", \"\"),\n",
    "        api_version=\"2024-02-15-preview\"  # Versión estable\n",
    "    )\n",
    "    \n",
    "    print(\"Cliente Azure OpenAI configurado correctamente\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"WARNING: OpenAI library no está disponible\")\n",
    "    print(\"Para instalar: pip install openai\")\n",
    "    \n",
    "    # Cliente mock para desarrollo/testing\n",
    "    class MockOpenAI:\n",
    "        class chat:\n",
    "            class completions:\n",
    "                @staticmethod\n",
    "                def create(**kwargs):\n",
    "                    class MockResponse:\n",
    "                        class choices:\n",
    "                            class message:\n",
    "                                content = json.dumps(SCHEMA_TEMPLATE)\n",
    "                        choices = [choices()]\n",
    "                    return MockResponse()\n",
    "    \n",
    "    client = MockOpenAI()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR configurando Azure OpenAI: {e}\")\n",
    "    print(\"Verifica las variables de entorno AZURE_OPENAI_ENDPOINT y AZURE_OPENAI_KEY\")\n",
    "    \n",
    "    # Cliente mock para desarrollo/testing\n",
    "    class MockOpenAI:\n",
    "        class chat:\n",
    "            class completions:\n",
    "                @staticmethod\n",
    "                def create(**kwargs):\n",
    "                    class MockResponse:\n",
    "                        class choices:\n",
    "                            class message:\n",
    "                                content = json.dumps(SCHEMA_TEMPLATE)\n",
    "                        choices = [choices()]\n",
    "                    return MockResponse()\n",
    "    \n",
    "    client = MockOpenAI()\n",
    "\n",
    "@tenacity.retry(\n",
    "    wait=tenacity.wait_random_exponential(min=1, max=10),\n",
    "    stop=tenacity.stop_after_attempt(3),\n",
    "    retry=tenacity.retry_if_exception_type((Exception,))\n",
    ")\n",
    "def extract_features(text: str) -> str:\n",
    "    \"\"\"Extrae características estructuradas del texto del CV\"\"\"\n",
    "    if not text or len(text.strip()) < 50:\n",
    "        return json.dumps(SCHEMA_TEMPLATE)\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f'Analiza este CV:\\n\\n\"\"\"{text[:4000]}\"\"\"'}  # Limitar a 4000 chars\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",  # Tu deployment name en Azure\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 5)\n",
      "Index(['ID', 'Resume_str', 'Resume_html', 'Category', 'text'], dtype='object')\n",
      "            ID                                         Resume_str  \\\n",
      "1690  14206561           ENGINEERING TECHNICIAN           High...   \n",
      "1691  15139979           ENGINEERING ASSISTANT       Summary  ...   \n",
      "\n",
      "                                            Resume_html     Category  \\\n",
      "1690  <div class=\"fontsize fontface vmargins hmargin...  ENGINEERING   \n",
      "1691  <div class=\"RNA skn-cnt4 fontsize fontface vma...  ENGINEERING   \n",
      "\n",
      "                                                   text  \n",
      "1690  ENGINEERING TECHNICIAN Highlights PC Operating...  \n",
      "1691  ENGINEERING ASSISTANT Summary Knowledgeable En...  \n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extracción de características para 118 CVs...\n",
      "Esto puede tomar varios minutos dependiendo del tamaño del dataset y la API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo características:   0%|          | 0/118 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo características: 100%|██████████| 118/118 [01:23<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracción completada!\n",
      "Extracciones exitosas: 118\n",
      "Extracciones fallidas: 0\n",
      "DataFrame de características creado: (118, 21)\n",
      "Columnas extraídas: ['education_level', 'discipline', 'years_total_experience', 'years_skill_main', 'current_job_title', 'skills', 'certifications', 'num_promotions', 'avg_tenure_months', 'gap_months_last5y', 'location_city', 'work_authorization', 'gender', 'age_range', 'languages.English', 'languages.Spanish', 'languages.Hindi', 'languages.Armenian', 'languages.German', 'languages.Turkish', 'languages.Mandarin']\n",
      "\n",
      "Aplicando reglas para determinar candidatos aptos...\n",
      "Evaluación de aptitud completada:\n",
      "  Aptos (1): 19 (16.1%)\n",
      "  No aptos (0): 13 (11.0%)\n",
      "  Revisión manual (-1): 86 (72.9%)\n",
      "\n",
      "Dataset con características guardado en:\n",
      "  Parquet: ..\\data\\processed\\features_extracted.parquet\n",
      "  CSV: ..\\data\\processed\\features_extracted.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. PROCESAMIENTO EN PARALELO Y MANEJO DE ERRORES\n",
    "import concurrent.futures as cf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def safe_extract_features(text):\n",
    "    \"\"\"Función segura para extraer características con manejo de errores\"\"\"\n",
    "    try:\n",
    "        result_json = extract_features(text)\n",
    "        parsed_result = json.loads(result_json)\n",
    "        \n",
    "        # Validaciones básicas del esquema\n",
    "        if not isinstance(parsed_result, dict):\n",
    "            return {\"error\": \"El resultado no es un diccionario válido\"}\n",
    "            \n",
    "        # Asegurar que los campos numéricos sean números o null\n",
    "        numeric_fields = [\"years_total_experience\", \"years_skill_main\", \"num_promotions\", \n",
    "                         \"avg_tenure_months\", \"gap_months_last5y\"]\n",
    "        \n",
    "        for field in numeric_fields:\n",
    "            if field in parsed_result and parsed_result[field] is not None:\n",
    "                try:\n",
    "                    parsed_result[field] = int(float(parsed_result[field]))\n",
    "                except (ValueError, TypeError):\n",
    "                    parsed_result[field] = None\n",
    "        \n",
    "        return parsed_result\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"error\": f\"JSON inválido: {str(e)}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error en extracción: {str(e)}\"}\n",
    "\n",
    "# Procesamiento del dataset\n",
    "if 'text' in df.columns and len(df) > 0:\n",
    "    print(f\"Iniciando extracción de características para {len(df)} CVs...\")\n",
    "    print(\"Esto puede tomar varios minutos dependiendo del tamaño del dataset y la API...\")\n",
    "    \n",
    "    max_workers = min(5, len(df))\n",
    "    with cf.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        features_list = list(tqdm(\n",
    "            executor.map(safe_extract_features, df[\"text\"]), \n",
    "            total=len(df),\n",
    "            desc=\"Extrayendo características\"\n",
    "        ))\n",
    "    \n",
    "    print(\"Extracción completada!\")\n",
    "    \n",
    "    # Separar resultados exitosos de errores\n",
    "    successful_extractions = []\n",
    "    failed_extractions = []\n",
    "    \n",
    "    for i, result in enumerate(features_list):\n",
    "        if \"error\" in result:\n",
    "            failed_extractions.append({\"index\": i, \"error\": result[\"error\"]})\n",
    "        else:\n",
    "            successful_extractions.append(result)\n",
    "    \n",
    "    print(f\"Extracciones exitosas: {len(successful_extractions)}\")\n",
    "    print(f\"Extracciones fallidas: {len(failed_extractions)}\")\n",
    "    \n",
    "    # Guardar errores para revisión\n",
    "    if failed_extractions:\n",
    "        errors_df = pd.DataFrame(failed_extractions)\n",
    "        errors_df.to_csv(PROCESSED_PATH / \"extraction_errors.csv\", index=False)\n",
    "        print(\"Errores guardados en extraction_errors.csv\")\n",
    "    \n",
    "    # Crear DataFrame con características extraídas\n",
    "    if successful_extractions:\n",
    "        features_df = pd.json_normalize(successful_extractions)\n",
    "        print(f\"DataFrame de características creado: {features_df.shape}\")\n",
    "        print(\"Columnas extraídas:\", list(features_df.columns))\n",
    "        \n",
    "        # AGREGAR CAMPO 'APTO' BASADO EN REGLAS DE NEGOCIO\n",
    "        print(\"\\nAplicando reglas para determinar candidatos aptos...\")\n",
    "        \n",
    "        def evaluar_aptitud_ingeniero(row):\n",
    "            \"\"\"\n",
    "            Evalúa si un candidato es apto para un puesto de ingeniero\n",
    "            Retorna: 1 (apto), 0 (no apto), -1 (necesita revisión manual)\n",
    "            \"\"\"\n",
    "            score = 0\n",
    "            \n",
    "            # 1. Experiencia mínima (peso: 3 puntos)\n",
    "            years_exp = row.get('years_total_experience', 0) or 0\n",
    "            if years_exp >= 5:\n",
    "                score += 3\n",
    "            elif years_exp >= 2:\n",
    "                score += 2\n",
    "            elif years_exp >= 1:\n",
    "                score += 1\n",
    "            \n",
    "            # 2. Nivel educativo (peso: 2 puntos)\n",
    "            education = row.get('education_level', '')\n",
    "            if education in ['PhD', 'Master']:\n",
    "                score += 2\n",
    "            elif education == 'Bachelor':\n",
    "                score += 1\n",
    "            \n",
    "            # 3. Skills técnicos relevantes (peso: 3 puntos)\n",
    "            skills = row.get('skills', []) or []\n",
    "            if isinstance(skills, list):\n",
    "                technical_skills = [\n",
    "                    'python', 'java', 'c++', 'javascript', 'sql', 'aws', 'azure',\n",
    "                    'machine learning', 'data analysis', 'software development',\n",
    "                    'project management', 'agile', 'scrum', 'git', 'docker'\n",
    "                ]\n",
    "                skills_text = ' '.join(skills).lower()\n",
    "                matching_skills = sum(1 for skill in technical_skills if skill in skills_text)\n",
    "                \n",
    "                if matching_skills >= 3:\n",
    "                    score += 3\n",
    "                elif matching_skills >= 2:\n",
    "                    score += 2\n",
    "                elif matching_skills >= 1:\n",
    "                    score += 1\n",
    "            \n",
    "            # 4. Certificaciones (peso: 1 punto)\n",
    "            certifications = row.get('certifications', []) or []\n",
    "            if isinstance(certifications, list) and len(certifications) > 0:\n",
    "                score += 1\n",
    "            \n",
    "            # 5. Idiomas (peso: 1 punto si incluye inglés)\n",
    "            languages = row.get('languages', {}) or {}\n",
    "            if isinstance(languages, dict):\n",
    "                english_level = languages.get('English', '').upper()\n",
    "                if english_level in ['B2', 'C1', 'C2', 'NATIVE']:\n",
    "                    score += 1\n",
    "            \n",
    "            # Criterios de decisión\n",
    "            if score >= 7:\n",
    "                return 1    # Apto\n",
    "            elif score >= 4:\n",
    "                return -1   # Necesita revisión manual\n",
    "            else:\n",
    "                return 0    # No apto\n",
    "        \n",
    "        # Aplicar evaluación\n",
    "        features_df['apto'] = features_df.apply(evaluar_aptitud_ingeniero, axis=1)\n",
    "        \n",
    "        # Estadísticas de aptitud\n",
    "        aptitud_counts = features_df['apto'].value_counts()\n",
    "        total = len(features_df)\n",
    "        \n",
    "        print(f\"Evaluación de aptitud completada:\")\n",
    "        print(f\"  Aptos (1): {aptitud_counts.get(1, 0)} ({aptitud_counts.get(1, 0)/total*100:.1f}%)\")\n",
    "        print(f\"  No aptos (0): {aptitud_counts.get(0, 0)} ({aptitud_counts.get(0, 0)/total*100:.1f}%)\")\n",
    "        print(f\"  Revisión manual (-1): {aptitud_counts.get(-1, 0)} ({aptitud_counts.get(-1, 0)/total*100:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR: No se pudo extraer ninguna característica\")\n",
    "        features_df = pd.DataFrame()\n",
    "        \n",
    "else:\n",
    "    print(\"SALTANDO: Extracción - no hay texto anonimizado para procesar\")\n",
    "    features_df = pd.DataFrame()\n",
    "\n",
    "# Guardar dataset con características extraídas y campo apto\n",
    "if len(features_df) > 0:\n",
    "    features_df.to_parquet(PROCESSED_PATH / \"features_extracted.parquet\", index=False)\n",
    "    features_df.to_csv(PROCESSED_PATH / \"features_extracted.csv\", index=False)\n",
    "    print(f\"\\nDataset con características guardado en:\")\n",
    "    print(f\"  Parquet: {PROCESSED_PATH / 'features_extracted.parquet'}\")\n",
    "    print(f\"  CSV: {PROCESSED_PATH / 'features_extracted.csv'}\")\n",
    "else:\n",
    "    print(\"No hay datos para guardar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinando datos originales con características extraídas...\n",
      "Dataset enriquecido creado: (236, 26)\n",
      "Aplicando validaciones y limpieza...\n",
      "\n",
      "Porcentaje de valores nulos por columna:\n",
      "  ID: 50.0% (ALTO)\n",
      "  Resume_str: 50.0% (ALTO)\n",
      "  Resume_html: 50.0% (ALTO)\n",
      "  Category: 50.0% (ALTO)\n",
      "  education_level: 50.0% (ALTO)\n",
      "  discipline: 50.4% (ALTO)\n",
      "  years_total_experience: 50.0% (ALTO)\n",
      "  years_skill_main: 50.0% (ALTO)\n",
      "  current_job_title: 50.0% (ALTO)\n",
      "  skills: 50.0% (ALTO)\n",
      "  certifications: 50.0% (ALTO)\n",
      "  num_promotions: 50.0% (ALTO)\n",
      "  avg_tenure_months: 50.0% (ALTO)\n",
      "  gap_months_last5y: 50.0% (ALTO)\n",
      "  location_city: 50.0% (ALTO)\n",
      "  work_authorization: 50.0% (ALTO)\n",
      "  gender: 52.1% (ALTO)\n",
      "  age_range: 50.0% (ALTO)\n",
      "  languages.English: 55.5% (ALTO)\n",
      "  languages.Spanish: 97.9% (ALTO)\n",
      "  languages.Hindi: 98.3% (ALTO)\n",
      "  languages.Armenian: 99.6% (ALTO)\n",
      "  languages.German: 99.6% (ALTO)\n",
      "  languages.Turkish: 99.6% (ALTO)\n",
      "  languages.Mandarin: 99.6% (ALTO)\n",
      "  apto: 50.0% (ALTO)\n"
     ]
    }
   ],
   "source": [
    "# 5. NORMALIZACIÓN, VALIDACIÓN Y ENRIQUECIMIENTO\n",
    "import numpy as np\n",
    "\n",
    "# Combinar datos originales con características extraídas\n",
    "if len(features_df) > 0:\n",
    "    print(\"Combinando datos originales con características extraídas...\")\n",
    "    \n",
    "    # Combinar solo si tenemos el mismo número de registros exitosos\n",
    "    if len(features_df) == len(df):\n",
    "        enriched = pd.concat([\n",
    "            df.drop(columns=[\"text\", \"text_anonymized\"], errors='ignore'), \n",
    "            features_df\n",
    "        ], axis=1)\n",
    "    else:\n",
    "        # Si hay menos características que registros originales debido a errores\n",
    "        print(f\"WARNING: {len(df)} registros originales vs {len(features_df)} características extraídas\")\n",
    "        enriched = features_df.copy()\n",
    "    \n",
    "    print(f\"Dataset enriquecido creado: {enriched.shape}\")\n",
    "    \n",
    "    # VALIDACIÓN Y LIMPIEZA DE DATOS\n",
    "    print(\"Aplicando validaciones y limpieza...\")\n",
    "    \n",
    "    # 1. Campos numéricos: limitar a rangos razonables\n",
    "    numeric_validations = {\n",
    "        \"years_total_experience\": (0, 50),    # Máximo 50 años de experiencia\n",
    "        \"years_skill_main\": (0, 30),          # Máximo 30 años en habilidad principal  \n",
    "        \"num_promotions\": (0, 20),            # Máximo 20 promociones\n",
    "        \"avg_tenure_months\": (0, 600),        # Máximo 50 años (600 meses) en un trabajo\n",
    "        \"gap_months_last5y\": (0, 60)          # Máximo 5 años de brecha\n",
    "    }\n",
    "    \n",
    "    for col, (min_val, max_val) in numeric_validations.items():\n",
    "        if col in enriched.columns:\n",
    "            # Convertir a numérico y aplicar límites\n",
    "            enriched[col] = pd.to_numeric(enriched[col], errors='coerce')\n",
    "            enriched[col] = enriched[col].clip(min_val, max_val)\n",
    "    \n",
    "    # 2. Limpiar education_level: estandarizar valores\n",
    "    if \"education_level\" in enriched.columns:\n",
    "        education_mapping = {\n",
    "            \"phd\": \"PhD\", \"doctorate\": \"PhD\", \"doctoral\": \"PhD\",\n",
    "            \"master\": \"Master\", \"masters\": \"Master\", \"mba\": \"Master\",\n",
    "            \"bachelor\": \"Bachelor\", \"bachelors\": \"Bachelor\", \"ba\": \"Bachelor\", \"bs\": \"Bachelor\",\n",
    "            \"associate\": \"Associate\", \"associates\": \"Associate\",\n",
    "            \"high school\": \"High School\", \"highschool\": \"High School\",\n",
    "            \"diploma\": \"High School\"\n",
    "        }\n",
    "        \n",
    "        enriched[\"education_level\"] = enriched[\"education_level\"].astype(str).str.lower()\n",
    "        enriched[\"education_level\"] = enriched[\"education_level\"].replace(education_mapping)\n",
    "        enriched[\"education_level\"] = enriched[\"education_level\"].replace(\"nan\", None)\n",
    "    \n",
    "    # 3. Control de calidad: revisar nulos\n",
    "    null_percentages = enriched.isnull().mean() * 100\n",
    "    print(\"\\nPorcentaje de valores nulos por columna:\")\n",
    "    for col, null_pct in null_percentages.items():\n",
    "        if null_pct > 25:\n",
    "            print(f\"  {col}: {null_pct:.1f}% (ALTO)\")\n",
    "        elif null_pct > 0:\n",
    "            print(f\"  {col}: {null_pct:.1f}%\")\n",
    "    \n",
    "    # Verificar que no hay demasiados nulos\n",
    "    critical_fields = [\"current_job_title\", \"skills\"]\n",
    "    for field in critical_fields:\n",
    "        if field in enriched.columns:\n",
    "            null_pct = enriched[field].isnull().mean() * 100\n",
    "            if null_pct > 50:\n",
    "                print(f\"WARNING: Campo crítico '{field}' tiene {null_pct:.1f}% de nulos\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No hay características extraídas para procesar\")\n",
    "    enriched = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FILTRADO PARA PUESTOS DE INGENIERÍA\n",
      "==================================================\n",
      "📁 Configurando ruta de datos procesados: ..\\data\\processed\n",
      "Verificando disponibilidad de datos...\n",
      "⚠️ Variable 'enriched' no disponible en la sesión actual\n",
      "✅ Datos cargados desde archivo parquet: 118 registros\n",
      "Dataset antes del filtrado: 118 registros\n",
      "Aplicando filtros de ingeniería...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Aplicar función de filtrado\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAplicando filtros de ingeniería...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m engineering_mask = \u001b[43menriched\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_engineering_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m engineers_df = enriched[engineering_mask].copy()\n\u001b[32m    121\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPerfiles de ingeniería identificados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(engineers_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m registros\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10381\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10367\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10369\u001b[39m op = frame_apply(\n\u001b[32m  10370\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10371\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10379\u001b[39m     kwargs=kwargs,\n\u001b[32m  10380\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mis_engineering_profile\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Verificar skills (si es una lista o string)\u001b[39;00m\n\u001b[32m     96\u001b[39m skills_to_check = row.get(\u001b[33m'\u001b[39m\u001b[33mskills\u001b[39m\u001b[33m'\u001b[39m, [])\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.notna(skills_to_check):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(skills_to_check, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     99\u001b[39m         skills_text = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(skill) \u001b[38;5;28;01mfor\u001b[39;00m skill \u001b[38;5;129;01min\u001b[39;00m skills_to_check).lower()\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# 6. FILTRADO ESPECÍFICO PARA INGENIEROS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FILTRADO PARA PUESTOS DE INGENIERÍA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Importaciones y configuración de rutas (en caso de que no estén disponibles)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar rutas si no están definidas\n",
    "try:\n",
    "    PROCESSED_PATH\n",
    "except NameError:\n",
    "    PROCESSED_PATH = Path(\"../data/processed/\")\n",
    "    print(f\"📁 Configurando ruta de datos procesados: {PROCESSED_PATH}\")\n",
    "\n",
    "# Verificar si tenemos datos para procesar\n",
    "print(\"Verificando disponibilidad de datos...\")\n",
    "\n",
    "# Intentar cargar datos desde diferentes fuentes\n",
    "enriched = None\n",
    "\n",
    "# 1. Verificar si existe la variable 'enriched' del procesamiento anterior\n",
    "try:\n",
    "    if 'enriched' in globals():\n",
    "        # Verificar que la variable existe y no es None\n",
    "        existing_enriched = globals()['enriched']\n",
    "        if existing_enriched is not None and len(existing_enriched) > 0:\n",
    "            enriched = existing_enriched\n",
    "            print(f\"✅ Usando variable 'enriched' de la sesión actual: {len(enriched)} registros\")\n",
    "        else:\n",
    "            raise NameError(\"Variable 'enriched' existe pero está vacía o es None\")\n",
    "    else:\n",
    "        raise NameError(\"Variable 'enriched' no disponible\")\n",
    "except (NameError, TypeError):\n",
    "    print(\"⚠️ Variable 'enriched' no disponible en la sesión actual\")\n",
    "    \n",
    "    # 2. Intentar cargar desde archivo parquet procesado\n",
    "    try:\n",
    "        features_path = PROCESSED_PATH / \"features_extracted.parquet\"\n",
    "        if features_path.exists():\n",
    "            enriched = pd.read_parquet(features_path)\n",
    "            print(f\"✅ Datos cargados desde archivo parquet: {len(enriched)} registros\")\n",
    "        else:\n",
    "            # 3. Intentar cargar desde archivo CSV como respaldo\n",
    "            csv_path = PROCESSED_PATH / \"features_extracted.csv\"\n",
    "            if csv_path.exists():\n",
    "                enriched = pd.read_csv(csv_path)\n",
    "                print(f\"✅ Datos cargados desde archivo CSV: {len(enriched)} registros\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No se encontraron archivos de datos procesados\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cargando datos desde archivos: {e}\")\n",
    "        print(\"💡 Ejecuta las celdas anteriores para generar los datos\")\n",
    "        enriched = pd.DataFrame()\n",
    "\n",
    "# Continuar solo si tenemos datos\n",
    "if enriched is not None and len(enriched) > 0:\n",
    "    # Definir términos relacionados con ingeniería\n",
    "    engineering_keywords = {\n",
    "        'disciplines': [\n",
    "            'engineering', 'engineer', 'mechanical', 'electrical', 'civil', 'chemical', \n",
    "            'software', 'computer science', 'information technology', 'data science',\n",
    "            'systems', 'industrial', 'aerospace', 'biomedical', 'environmental',\n",
    "            'structural', 'automotive', 'telecommunications', 'robotics'\n",
    "        ],\n",
    "        'job_titles': [\n",
    "            'engineer', 'developer', 'architect', 'analyst', 'scientist', 'programmer',\n",
    "            'technician', 'specialist', 'consultant', 'manager', 'lead', 'senior',\n",
    "            'principal', 'staff', 'chief technology', 'technical director'\n",
    "        ],\n",
    "        'skills': [\n",
    "            'python', 'java', 'c++', 'matlab', 'autocad', 'solidworks', 'aws', 'azure',\n",
    "            'machine learning', 'artificial intelligence', 'data analysis', 'sql',\n",
    "            'project management', 'agile', 'scrum', 'tensorflow', 'kubernetes'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def is_engineering_profile(row):\n",
    "        \"\"\"Determina si un perfil es de ingeniería basado en múltiples campos\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # Verificar discipline\n",
    "        if pd.notna(row.get('discipline', '')):\n",
    "            discipline = str(row['discipline']).lower()\n",
    "            if any(keyword in discipline for keyword in engineering_keywords['disciplines']):\n",
    "                score += 3\n",
    "        \n",
    "        # Verificar current_job_title\n",
    "        if pd.notna(row.get('current_job_title', '')):\n",
    "            job_title = str(row['current_job_title']).lower()\n",
    "            if any(keyword in job_title for keyword in engineering_keywords['job_titles']):\n",
    "                score += 2\n",
    "        \n",
    "        # Verificar skills (si es una lista o string)\n",
    "        skills_to_check = row.get('skills', [])\n",
    "        if pd.notna(skills_to_check):\n",
    "            if isinstance(skills_to_check, list):\n",
    "                skills_text = ' '.join(str(skill) for skill in skills_to_check).lower()\n",
    "            else:\n",
    "                skills_text = str(skills_to_check).lower()\n",
    "                \n",
    "            matching_skills = sum(1 for keyword in engineering_keywords['skills'] \n",
    "                                if keyword in skills_text)\n",
    "            score += min(matching_skills, 2)  # Máximo 2 puntos por skills\n",
    "        \n",
    "        # Verificar education_level (preferir títulos técnicos)\n",
    "        if row.get('education_level') in ['Bachelor', 'Master', 'PhD']:\n",
    "            score += 1\n",
    "        \n",
    "        return score >= 3  # Threshold mínimo para considerar como ingeniero\n",
    "\n",
    "    # Aplicar filtrado\n",
    "    print(f\"Dataset antes del filtrado: {len(enriched)} registros\")\n",
    "    \n",
    "    # Aplicar función de filtrado\n",
    "    print(\"Aplicando filtros de ingeniería...\")\n",
    "    engineering_mask = enriched.apply(is_engineering_profile, axis=1)\n",
    "    engineers_df = enriched[engineering_mask].copy()\n",
    "    \n",
    "    print(f\"Perfiles de ingeniería identificados: {len(engineers_df)} registros\")\n",
    "    print(f\"Porcentaje de ingenieros: {len(engineers_df)/len(enriched)*100:.1f}%\")\n",
    "    \n",
    "    # Mostrar estadísticas de los perfiles filtrados\n",
    "    if len(engineers_df) > 0:\n",
    "        print(\"\\nESTADÍSTICAS DE PERFILES DE INGENIERÍA:\")\n",
    "        \n",
    "        # Education levels\n",
    "        if 'education_level' in engineers_df.columns:\n",
    "            education_counts = engineers_df['education_level'].value_counts(dropna=False)\n",
    "            print(f\"\\nNiveles de educación:\")\n",
    "            for edu, count in education_counts.items():\n",
    "                print(f\"  {edu}: {count} ({count/len(engineers_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Disciplinas más comunes\n",
    "        if 'discipline' in engineers_df.columns:\n",
    "            discipline_counts = engineers_df['discipline'].value_counts(dropna=False).head(10)\n",
    "            print(f\"\\nTop 10 disciplinas:\")\n",
    "            for disc, count in discipline_counts.items():\n",
    "                print(f\"  {disc}: {count}\")\n",
    "        \n",
    "        # Experiencia promedio\n",
    "        if 'years_total_experience' in engineers_df.columns:\n",
    "            avg_exp = engineers_df['years_total_experience'].mean()\n",
    "            if pd.notna(avg_exp):\n",
    "                print(f\"\\nExperiencia promedio: {avg_exp:.1f} años\")\n",
    "        \n",
    "        # Estadísticas del campo 'apto' si existe\n",
    "        if 'apto' in engineers_df.columns:\n",
    "            aptitud_counts = engineers_df['apto'].value_counts()\n",
    "            total = len(engineers_df)\n",
    "            \n",
    "            print(f\"\\nESTADÍSTICAS DE APTITUD:\")\n",
    "            labels = {1: 'Aptos', 0: 'No aptos', -1: 'Revisión manual'}\n",
    "            for value, count in aptitud_counts.items():\n",
    "                label = labels.get(value, f'Valor {value}')\n",
    "                percentage = count / total * 100\n",
    "                print(f\"  {label}: {count} candidatos ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Dataset final para entrenamiento\n",
    "    final_dataset = engineers_df.copy()\n",
    "    \n",
    "    # Guardar dataset filtrado\n",
    "    if len(final_dataset) > 0:\n",
    "        print(f\"\\n✅ FILTRADO COMPLETADO EXITOSAMENTE\")\n",
    "        print(f\"📊 Dataset de ingenieros listo: {len(final_dataset)} registros\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ ERROR: No hay datos enriquecidos para filtrar\")\n",
    "    print(\"💡 Asegúrate de ejecutar las celdas anteriores primero\")\n",
    "    final_dataset = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXPORTACIÓN Y ANÁLISIS FINAL\n",
      "==================================================\n",
      "⚠️ Variable 'final_dataset' no disponible\n",
      "💡 Intentando cargar desde archivos existentes...\n",
      "✅ Usando dataset de características: 118 registros\n",
      "Dataset final guardado:\n",
      "  Parquet: ..\\data\\processed\\engineers_dataset_for_training.parquet\n",
      "  CSV: ..\\data\\processed\\engineers_dataset_for_training.csv\n",
      "  Registros: 118\n",
      "  Columnas: 22\n",
      "\n",
      "ESQUEMA DEL DATASET FINAL:\n",
      "  education_level           | object          |    0 nulos (  0.0%)\n",
      "  discipline                | object          |    1 nulos (  0.8%)\n",
      "  years_total_experience    | int64           |    0 nulos (  0.0%)\n",
      "  years_skill_main          | int64           |    0 nulos (  0.0%)\n",
      "  current_job_title         | object          |    0 nulos (  0.0%)\n",
      "  skills                    | object          |    0 nulos (  0.0%)\n",
      "  certifications            | object          |    0 nulos (  0.0%)\n",
      "  num_promotions            | int64           |    0 nulos (  0.0%)\n",
      "  avg_tenure_months         | int64           |    0 nulos (  0.0%)\n",
      "  gap_months_last5y         | int64           |    0 nulos (  0.0%)\n",
      "  location_city             | object          |    0 nulos (  0.0%)\n",
      "  work_authorization        | object          |    0 nulos (  0.0%)\n",
      "  gender                    | object          |    5 nulos (  4.2%)\n",
      "  age_range                 | object          |    0 nulos (  0.0%)\n",
      "  languages.English         | object          |   13 nulos ( 11.0%)\n",
      "  languages.Spanish         | object          |  113 nulos ( 95.8%)\n",
      "  languages.Hindi           | object          |  114 nulos ( 96.6%)\n",
      "  languages.Armenian        | object          |  117 nulos ( 99.2%)\n",
      "  languages.German          | object          |  117 nulos ( 99.2%)\n",
      "  languages.Turkish         | object          |  117 nulos ( 99.2%)\n",
      "  languages.Mandarin        | object          |  117 nulos ( 99.2%)\n",
      "  apto                      | int64           |    0 nulos (  0.0%)\n",
      "\n",
      "ESTADÍSTICAS DEL CAMPO 'APTO':\n",
      "  Revisión manual: 86 candidatos (72.9%)\n",
      "  Aptos: 19 candidatos (16.1%)\n",
      "  No aptos: 13 candidatos (11.0%)\n",
      "\n",
      "MUESTRA DE LOS PRIMEROS 3 REGISTROS:\n",
      "     current_job_title              discipline  years_total_experience education_level  apto\n",
      "Engineering Technician Business Administration                      10        Bachelor    -1\n",
      " Engineering Assistant             Engineering                      20           Other    -1\n",
      "   Engineering Manager                    None                      25           Other    -1\n",
      "\n",
      "WARNING: matplotlib no está disponible para generar histogramas\n",
      "Para instalar: pip install matplotlib\n",
      "\n",
      "✅ DATASET PARA ENTRENAMIENTO DE MODELO DE SELECCIÓN LISTO!\n",
      "📊 118 perfiles de ingeniería estructurados\n",
      "🎯 Listo para entrenar modelo supervisado de selección de candidatos\n",
      "🔍 Etiquetas para entrenamiento: 19 aptos, 13 no aptos, 86 para revisión\n",
      "📈 Modelo puede entrenarse con clasificación binaria o multiclase\n"
     ]
    }
   ],
   "source": [
    "# 7. EXPORTACIÓN Y VISUALIZACIÓN FINAL\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPORTACIÓN Y ANÁLISIS FINAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Verificar si tenemos el dataset final\n",
    "try:\n",
    "    if 'final_dataset' in globals() and len(final_dataset) > 0:\n",
    "        print(f\"✅ Dataset final disponible: {len(final_dataset)} registros\")\n",
    "    else:\n",
    "        raise NameError(\"Variable 'final_dataset' no disponible\")\n",
    "except NameError:\n",
    "    print(\"⚠️ Variable 'final_dataset' no disponible\")\n",
    "    print(\"💡 Intentando cargar desde archivos existentes...\")\n",
    "    \n",
    "    # Intentar cargar dataset de ingenieros si existe\n",
    "    try:\n",
    "        engineers_path = PROCESSED_PATH / \"engineers_dataset_for_training.parquet\"\n",
    "        if engineers_path.exists():\n",
    "            final_dataset = pd.read_parquet(engineers_path)\n",
    "            print(f\"✅ Dataset cargado desde archivo: {len(final_dataset)} registros\")\n",
    "        else:\n",
    "            # Usar el dataset general de características\n",
    "            features_path = PROCESSED_PATH / \"features_extracted.parquet\"\n",
    "            if features_path.exists():\n",
    "                final_dataset = pd.read_parquet(features_path)\n",
    "                print(f\"✅ Usando dataset de características: {len(final_dataset)} registros\")\n",
    "            else:\n",
    "                final_dataset = pd.DataFrame()\n",
    "                print(\"❌ No se encontraron archivos de datos\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cargando datos: {e}\")\n",
    "        final_dataset = pd.DataFrame()\n",
    "\n",
    "if len(final_dataset) > 0:\n",
    "    # Guardar dataset final\n",
    "    final_parquet_path = PROCESSED_PATH / \"engineers_dataset_for_training.parquet\"\n",
    "    final_csv_path = PROCESSED_PATH / \"engineers_dataset_for_training.csv\"\n",
    "    \n",
    "    final_dataset.to_parquet(final_parquet_path, index=False)\n",
    "    final_dataset.to_csv(final_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Dataset final guardado:\")\n",
    "    print(f\"  Parquet: {final_parquet_path}\")\n",
    "    print(f\"  CSV: {final_csv_path}\")\n",
    "    print(f\"  Registros: {len(final_dataset)}\")\n",
    "    print(f\"  Columnas: {len(final_dataset.columns)}\")\n",
    "    \n",
    "    # Resumen del esquema final\n",
    "    print(f\"\\nESQUEMA DEL DATASET FINAL:\")\n",
    "    for col in final_dataset.columns:\n",
    "        dtype = final_dataset[col].dtype\n",
    "        null_count = final_dataset[col].isnull().sum()\n",
    "        null_pct = null_count / len(final_dataset) * 100\n",
    "        print(f\"  {col:25s} | {str(dtype):15s} | {null_count:4d} nulos ({null_pct:5.1f}%)\")\n",
    "    \n",
    "    # Estadísticas del campo 'apto'\n",
    "    if 'apto' in final_dataset.columns:\n",
    "        print(f\"\\nESTADÍSTICAS DEL CAMPO 'APTO':\")\n",
    "        aptitud_counts = final_dataset['apto'].value_counts()\n",
    "        total = len(final_dataset)\n",
    "        \n",
    "        labels = {1: 'Aptos', 0: 'No aptos', -1: 'Revisión manual'}\n",
    "        for value, count in aptitud_counts.items():\n",
    "            label = labels.get(value, f'Valor {value}')\n",
    "            percentage = count / total * 100\n",
    "            print(f\"  {label}: {count} candidatos ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Muestra de datos\n",
    "    print(f\"\\nMUESTRA DE LOS PRIMEROS 3 REGISTROS:\")\n",
    "    sample_cols = ['current_job_title', 'discipline', 'years_total_experience', 'education_level', 'apto']\n",
    "    available_cols = [col for col in sample_cols if col in final_dataset.columns]\n",
    "    if available_cols:\n",
    "        print(final_dataset[available_cols].head(3).to_string(index=False))\n",
    "    \n",
    "    # Generar histogramas y visualizaciones si hay matplotlib\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Visualizaciones numéricas\n",
    "        numeric_cols = ['years_total_experience', 'years_skill_main', 'num_promotions', \n",
    "                       'avg_tenure_months', 'gap_months_last5y']\n",
    "        available_numeric = [col for col in numeric_cols if col in final_dataset.columns]\n",
    "        \n",
    "        if available_numeric:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, col in enumerate(available_numeric[:5]):\n",
    "                data = final_dataset[col].dropna()\n",
    "                if len(data) > 0:\n",
    "                    axes[i].hist(data, bins=min(20, len(data.unique())), alpha=0.7, edgecolor='black')\n",
    "                    axes[i].set_title(f'{col}\\n(n={len(data)}, avg={data.mean():.1f})')\n",
    "                    axes[i].set_xlabel(col)\n",
    "                    axes[i].set_ylabel('Frecuencia')\n",
    "            \n",
    "            # Gráfico de barras para el campo 'apto' en el último subplot\n",
    "            if 'apto' in final_dataset.columns:\n",
    "                aptitud_counts = final_dataset['apto'].value_counts().sort_index()\n",
    "                labels = {1: 'Aptos', 0: 'No aptos', -1: 'Revisión\\nmanual'}\n",
    "                \n",
    "                x_labels = [labels.get(val, str(val)) for val in aptitud_counts.index]\n",
    "                colors = ['red', 'orange', 'green']\n",
    "                \n",
    "                axes[5].bar(x_labels, aptitud_counts.values, \n",
    "                           color=colors[:len(aptitud_counts)], alpha=0.7, edgecolor='black')\n",
    "                axes[5].set_title(f'Distribución Aptitud\\n(Total: {len(final_dataset)})')\n",
    "                axes[5].set_ylabel('Frecuencia')\n",
    "                \n",
    "                # Añadir valores en las barras\n",
    "                for i, v in enumerate(aptitud_counts.values):\n",
    "                    axes[5].text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "            else:\n",
    "                axes[5].set_visible(False)\n",
    "            \n",
    "            # Ocultar subplot vacío si es necesario\n",
    "            if len(available_numeric) < 5:\n",
    "                for i in range(len(available_numeric), 5):\n",
    "                    axes[i].set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PROCESSED_PATH / \"dataset_distributions.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"\\nHistogramas guardados en: {PROCESSED_PATH / 'dataset_distributions.png'}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\nWARNING: matplotlib no está disponible para generar histogramas\")\n",
    "        print(\"Para instalar: pip install matplotlib\")\n",
    "    \n",
    "    print(f\"\\n✅ DATASET PARA ENTRENAMIENTO DE MODELO DE SELECCIÓN LISTO!\")\n",
    "    print(f\"📊 {len(final_dataset)} perfiles de ingeniería estructurados\")\n",
    "    print(f\"🎯 Listo para entrenar modelo supervisado de selección de candidatos\")\n",
    "    \n",
    "    if 'apto' in final_dataset.columns:\n",
    "        aptos = (final_dataset['apto'] == 1).sum()\n",
    "        no_aptos = (final_dataset['apto'] == 0).sum()\n",
    "        revision = (final_dataset['apto'] == -1).sum()\n",
    "        print(f\"🔍 Etiquetas para entrenamiento: {aptos} aptos, {no_aptos} no aptos, {revision} para revisión\")\n",
    "        print(f\"📈 Modelo puede entrenarse con clasificación binaria o multiclase\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ ERROR: No se pudo crear el dataset final\")\n",
    "    print(\"Revisa los pasos anteriores para identificar problemas\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
