{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos...\n",
      "Dataset cargado: 2484 registros\n",
      "Columnas disponibles: ['ID', 'Resume_str', 'Resume_html', 'Category']\n",
      "Texto limpiado correctamente\n"
     ]
    }
   ],
   "source": [
    "# 1. CARGA Y LIMPIEZA INICIAL\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuraci√≥n de rutas\n",
    "RAW_PATH = Path(\"../data/raw/Resume/Resume.csv\") \n",
    "PROCESSED_PATH = Path(\"../data/processed/\")\n",
    "PROCESSED_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# Carga de datos\n",
    "print(\"Cargando datos...\")\n",
    "if RAW_PATH.exists():\n",
    "    df = pd.read_csv(RAW_PATH)\n",
    "    print(f\"Dataset cargado: {len(df)} registros\")\n",
    "    print(f\"Columnas disponibles: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"ERROR: No se encuentra el archivo {RAW_PATH}\")\n",
    "    print(\"Aseg√∫rate de tener el dataset en la carpeta correcta\")\n",
    "\n",
    "# Limpieza b√°sica de texto\n",
    "if \"Resume_str\" in df.columns:\n",
    "    df[\"text\"] = df[\"Resume_str\"].fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    print(\"Texto limpiado correctamente\")\n",
    "elif \"Resume\" in df.columns:\n",
    "    df[\"text\"] = df[\"Resume\"].fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    print(\"Usando columna 'Resume' como texto principal\")\n",
    "else:\n",
    "    print(\"ERROR: No se encuentra columna de texto del CV\")\n",
    "    print(\"Columnas disponibles:\", df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    df['Category'].str.contains('engineer', case=False, na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Probando anonimizaci√≥n en los primeros 5 registros...\n",
      "Anonimizaci√≥n funcionando correctamente\n",
      "Aplicando anonimizaci√≥n a todo el dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Solo proceder con dataset completo si la muestra funciona\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAplicando anonimizaci√≥n a todo el dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mtext_anonymized\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43manonymize_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnonimizaci√≥n completada\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36manonymize_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# An√°lisis de entidades sensibles\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     results = \u001b[43manalyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Anonimizaci√≥n\u001b[39;00m\n\u001b[32m     19\u001b[39m     anonymized = anonymizer.anonymize(text=text, analyzer_results=results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\presidio_analyzer\\analyzer_engine.py:221\u001b[39m, in \u001b[36mAnalyzerEngine.analyze\u001b[39m\u001b[34m(self, text, language, entities, correlation_id, score_threshold, return_decision_process, ad_hoc_recognizers, context, allow_list, allow_list_match, regex_flags, nlp_artifacts)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# run the nlp pipeline over the given text, store the results in\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# a NlpArtifacts instance\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nlp_artifacts:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     nlp_artifacts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_decision_process:\n\u001b[32m    224\u001b[39m     \u001b[38;5;28mself\u001b[39m.app_tracer.trace(\n\u001b[32m    225\u001b[39m         correlation_id, \u001b[33m\"\u001b[39m\u001b[33mnlp artifacts:\u001b[39m\u001b[33m\"\u001b[39m + nlp_artifacts.to_json()\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\presidio_analyzer\\nlp_engine\\spacy_nlp_engine.py:111\u001b[39m, in \u001b[36mSpacyNlpEngine.process_text\u001b[39m\u001b[34m(self, text, language)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nlp:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNLP engine is not loaded. Consider calling .load()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._doc_to_nlp_artifact(doc, language)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:53\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:343\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.set_annotations\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\pipeline\\_parser_internals\\ner.pyx:275\u001b[39m, in \u001b[36mspacy.pipeline._parser_internals.ner.BiluoPushDown.set_annotations\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:814\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.set_ents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:127\u001b[39m, in \u001b[36mspacy.tokens.doc.SetEntsDefault.values\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\enum.py:828\u001b[39m, in \u001b[36mEnumType.__members__\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[33;03m    Return the number of members (no aliases)\u001b[39;00m\n\u001b[32m    825\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m._member_names_)\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m \u001b[38;5;129m@bltns\u001b[39m.property\n\u001b[32m    829\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__members__\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[32m    830\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    831\u001b[39m \u001b[33;03m    Returns a mapping of member name->value.\u001b[39;00m\n\u001b[32m    832\u001b[39m \n\u001b[32m    833\u001b[39m \u001b[33;03m    This mapping lists all enum members, including aliases. Note that this\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[33;03m    is a read-only view of the internal mapping.\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m MappingProxyType(\u001b[38;5;28mcls\u001b[39m._member_map_)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2. ANONIMIZACI√ìN DE DATOS SENSIBLES\n",
    "try:\n",
    "    from presidio_analyzer import AnalyzerEngine\n",
    "    from presidio_anonymizer import AnonymizerEngine\n",
    "    \n",
    "    analyzer = AnalyzerEngine()\n",
    "    anonymizer = AnonymizerEngine()\n",
    "    \n",
    "    def anonymize_text(text):\n",
    "        \"\"\"Anonimiza informaci√≥n personal en el texto del CV\"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return text\n",
    "        \n",
    "        try:\n",
    "            # An√°lisis de entidades sensibles\n",
    "            results = analyzer.analyze(text=text, language=\"en\")\n",
    "            \n",
    "            # Anonimizaci√≥n\n",
    "            anonymized = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "            return anonymized.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error en anonimizaci√≥n: {e}\")\n",
    "            return text\n",
    "    \n",
    "    # Aplicar anonimizaci√≥n a una muestra para probar\n",
    "    if 'text' in df.columns and len(df) > 0:\n",
    "        print(\"Probando anonimizaci√≥n en los primeros 5 registros...\")\n",
    "        df_sample = df.head(5).copy()\n",
    "        df_sample[\"text_anonymized\"] = df_sample[\"text\"].apply(anonymize_text)\n",
    "        print(\"Anonimizaci√≥n funcionando correctamente\")\n",
    "        \n",
    "        # Solo proceder con dataset completo si la muestra funciona\n",
    "        print(\"Aplicando anonimizaci√≥n a todo el dataset...\")\n",
    "        df[\"text_anonymized\"] = df[\"text\"].apply(anonymize_text)\n",
    "        print(\"Anonimizaci√≥n completada\")\n",
    "    else:\n",
    "        print(\"SALTANDO: Anonimizaci√≥n - no hay texto para procesar\")\n",
    "        df[\"text_anonymized\"] = df.get(\"text\", \"\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"WARNING: Presidio no est√° instalado. Saltando anonimizaci√≥n...\")\n",
    "    print(\"Para instalar: pip install presidio-analyzer presidio-anonymizer\")\n",
    "    df[\"text_anonymized\"] = df.get(\"text\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliente Azure OpenAI configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "# 3. EXTRACCI√ìN ESTRUCTURADA CON LLM\n",
    "import json\n",
    "import tenacity\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Esquema JSON objetivo para extracci√≥n\n",
    "SCHEMA_TEMPLATE = {\n",
    "    \"education_level\": None,        # \"PhD | Master | Bachelor | High School | Other\"\n",
    "    \"discipline\": None,             # Campo de estudio/especializaci√≥n\n",
    "    \"years_total_experience\": None, # A√±os totales de experiencia (n√∫mero)\n",
    "    \"years_skill_main\": None,       # A√±os en habilidad principal (n√∫mero)\n",
    "    \"current_job_title\": None,      # T√≠tulo actual del trabajo\n",
    "    \"skills\": [],                   # Lista de habilidades\n",
    "    \"languages\": {},                # {\"English\": \"C2\", \"Spanish\": \"B2\"}\n",
    "    \"certifications\": [],           # Lista de certificaciones\n",
    "    \"num_promotions\": None,         # N√∫mero de promociones (n√∫mero)\n",
    "    \"avg_tenure_months\": None,      # Promedio de duraci√≥n en empleos en meses (n√∫mero)\n",
    "    \"gap_months_last5y\": None,      # Meses de brecha en √∫ltimos 5 a√±os (n√∫mero)\n",
    "    \"location_city\": None,          # Ciudad de residencia\n",
    "    \"work_authorization\": None,     # Estado de autorizaci√≥n de trabajo\n",
    "    \"gender\": None,\n",
    "    \"age_range\": None\n",
    "    \n",
    "    # Nota: El campo 'apto' se agregar√° despu√©s de la extracci√≥n basado en reglas de negocio\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = f\"\"\"\n",
    "Eres un extractor experto de CV. Analiza el texto del curr√≠culum y devuelve √öNICAMENTE un JSON v√°lido con este esquema exacto:\n",
    "\n",
    "{json.dumps(SCHEMA_TEMPLATE, indent=2)}\n",
    "\n",
    "INSTRUCCIONES CR√çTICAS:\n",
    "1. Devuelve SOLO el JSON, sin explicaciones ni comentarios\n",
    "2. Si un campo no est√° presente o no se puede determinar, inventatelo con coherencia\n",
    "3. Para arrays vac√≠os usa []\n",
    "4. Para objects vac√≠os usa {{}}\n",
    "5. Los n√∫meros deben ser enteros, no strings\n",
    "6. education_level debe ser uno de: \"PhD\", \"Master\", \"Bachelor\", \"Associate\", \"High School\", \"Other\"\n",
    "7. Para languages usa c√≥digos como \"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\" o \"Native\"\n",
    "8. NO inventes datos que no est√©n en el texto\n",
    "9. Para engineering roles, enf√≥cate en habilidades t√©cnicas relevantes \n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    from openai import AzureOpenAI\n",
    "    \n",
    "    # Configuraci√≥n del cliente Azure OpenAI\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_KEY\", \"\"),\n",
    "        api_version=\"2024-02-15-preview\"  # Versi√≥n estable\n",
    "    )\n",
    "    \n",
    "    print(\"Cliente Azure OpenAI configurado correctamente\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"WARNING: OpenAI library no est√° disponible\")\n",
    "    print(\"Para instalar: pip install openai\")\n",
    "    \n",
    "    # Cliente mock para desarrollo/testing\n",
    "    class MockOpenAI:\n",
    "        class chat:\n",
    "            class completions:\n",
    "                @staticmethod\n",
    "                def create(**kwargs):\n",
    "                    class MockResponse:\n",
    "                        class choices:\n",
    "                            class message:\n",
    "                                content = json.dumps(SCHEMA_TEMPLATE)\n",
    "                        choices = [choices()]\n",
    "                    return MockResponse()\n",
    "    \n",
    "    client = MockOpenAI()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ERROR configurando Azure OpenAI: {e}\")\n",
    "    print(\"Verifica las variables de entorno AZURE_OPENAI_ENDPOINT y AZURE_OPENAI_KEY\")\n",
    "    \n",
    "    # Cliente mock para desarrollo/testing\n",
    "    class MockOpenAI:\n",
    "        class chat:\n",
    "            class completions:\n",
    "                @staticmethod\n",
    "                def create(**kwargs):\n",
    "                    class MockResponse:\n",
    "                        class choices:\n",
    "                            class message:\n",
    "                                content = json.dumps(SCHEMA_TEMPLATE)\n",
    "                        choices = [choices()]\n",
    "                    return MockResponse()\n",
    "    \n",
    "    client = MockOpenAI()\n",
    "\n",
    "@tenacity.retry(\n",
    "    wait=tenacity.wait_random_exponential(min=1, max=10),\n",
    "    stop=tenacity.stop_after_attempt(3),\n",
    "    retry=tenacity.retry_if_exception_type((Exception,))\n",
    ")\n",
    "def extract_features(text: str) -> str:\n",
    "    \"\"\"Extrae caracter√≠sticas estructuradas del texto del CV\"\"\"\n",
    "    if not text or len(text.strip()) < 50:\n",
    "        return json.dumps(SCHEMA_TEMPLATE)\n",
    "        \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f'Analiza este CV:\\n\\n\"\"\"{text[:4000]}\"\"\"'}  # Limitar a 4000 chars\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",  # Tu deployment name en Azure\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 5)\n",
      "Index(['ID', 'Resume_str', 'Resume_html', 'Category', 'text'], dtype='object')\n",
      "            ID                                         Resume_str  \\\n",
      "1690  14206561           ENGINEERING TECHNICIAN           High...   \n",
      "1691  15139979           ENGINEERING ASSISTANT       Summary  ...   \n",
      "\n",
      "                                            Resume_html     Category  \\\n",
      "1690  <div class=\"fontsize fontface vmargins hmargin...  ENGINEERING   \n",
      "1691  <div class=\"RNA skn-cnt4 fontsize fontface vma...  ENGINEERING   \n",
      "\n",
      "                                                   text  \n",
      "1690  ENGINEERING TECHNICIAN Highlights PC Operating...  \n",
      "1691  ENGINEERING ASSISTANT Summary Knowledgeable En...  \n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extracci√≥n de caracter√≠sticas para 118 CVs...\n",
      "Esto puede tomar varios minutos dependiendo del tama√±o del dataset y la API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo caracter√≠sticas:   0%|          | 0/118 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo caracter√≠sticas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [01:23<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracci√≥n completada!\n",
      "Extracciones exitosas: 118\n",
      "Extracciones fallidas: 0\n",
      "DataFrame de caracter√≠sticas creado: (118, 21)\n",
      "Columnas extra√≠das: ['education_level', 'discipline', 'years_total_experience', 'years_skill_main', 'current_job_title', 'skills', 'certifications', 'num_promotions', 'avg_tenure_months', 'gap_months_last5y', 'location_city', 'work_authorization', 'gender', 'age_range', 'languages.English', 'languages.Spanish', 'languages.Hindi', 'languages.Armenian', 'languages.German', 'languages.Turkish', 'languages.Mandarin']\n",
      "\n",
      "Aplicando reglas para determinar candidatos aptos...\n",
      "Evaluaci√≥n de aptitud completada:\n",
      "  Aptos (1): 19 (16.1%)\n",
      "  No aptos (0): 13 (11.0%)\n",
      "  Revisi√≥n manual (-1): 86 (72.9%)\n",
      "\n",
      "Dataset con caracter√≠sticas guardado en:\n",
      "  Parquet: ..\\data\\processed\\features_extracted.parquet\n",
      "  CSV: ..\\data\\processed\\features_extracted.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. PROCESAMIENTO EN PARALELO Y MANEJO DE ERRORES\n",
    "import concurrent.futures as cf\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def safe_extract_features(text):\n",
    "    \"\"\"Funci√≥n segura para extraer caracter√≠sticas con manejo de errores\"\"\"\n",
    "    try:\n",
    "        result_json = extract_features(text)\n",
    "        parsed_result = json.loads(result_json)\n",
    "        \n",
    "        # Validaciones b√°sicas del esquema\n",
    "        if not isinstance(parsed_result, dict):\n",
    "            return {\"error\": \"El resultado no es un diccionario v√°lido\"}\n",
    "            \n",
    "        # Asegurar que los campos num√©ricos sean n√∫meros o null\n",
    "        numeric_fields = [\"years_total_experience\", \"years_skill_main\", \"num_promotions\", \n",
    "                         \"avg_tenure_months\", \"gap_months_last5y\"]\n",
    "        \n",
    "        for field in numeric_fields:\n",
    "            if field in parsed_result and parsed_result[field] is not None:\n",
    "                try:\n",
    "                    parsed_result[field] = int(float(parsed_result[field]))\n",
    "                except (ValueError, TypeError):\n",
    "                    parsed_result[field] = None\n",
    "        \n",
    "        return parsed_result\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"error\": f\"JSON inv√°lido: {str(e)}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error en extracci√≥n: {str(e)}\"}\n",
    "\n",
    "# Procesamiento del dataset\n",
    "if 'text' in df.columns and len(df) > 0:\n",
    "    print(f\"Iniciando extracci√≥n de caracter√≠sticas para {len(df)} CVs...\")\n",
    "    print(\"Esto puede tomar varios minutos dependiendo del tama√±o del dataset y la API...\")\n",
    "    \n",
    "    max_workers = min(5, len(df))\n",
    "    with cf.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        features_list = list(tqdm(\n",
    "            executor.map(safe_extract_features, df[\"text\"]), \n",
    "            total=len(df),\n",
    "            desc=\"Extrayendo caracter√≠sticas\"\n",
    "        ))\n",
    "    \n",
    "    print(\"Extracci√≥n completada!\")\n",
    "    \n",
    "    # Separar resultados exitosos de errores\n",
    "    successful_extractions = []\n",
    "    failed_extractions = []\n",
    "    \n",
    "    for i, result in enumerate(features_list):\n",
    "        if \"error\" in result:\n",
    "            failed_extractions.append({\"index\": i, \"error\": result[\"error\"]})\n",
    "        else:\n",
    "            successful_extractions.append(result)\n",
    "    \n",
    "    print(f\"Extracciones exitosas: {len(successful_extractions)}\")\n",
    "    print(f\"Extracciones fallidas: {len(failed_extractions)}\")\n",
    "    \n",
    "    # Guardar errores para revisi√≥n\n",
    "    if failed_extractions:\n",
    "        errors_df = pd.DataFrame(failed_extractions)\n",
    "        errors_df.to_csv(PROCESSED_PATH / \"extraction_errors.csv\", index=False)\n",
    "        print(\"Errores guardados en extraction_errors.csv\")\n",
    "    \n",
    "    # Crear DataFrame con caracter√≠sticas extra√≠das\n",
    "    if successful_extractions:\n",
    "        features_df = pd.json_normalize(successful_extractions)\n",
    "        print(f\"DataFrame de caracter√≠sticas creado: {features_df.shape}\")\n",
    "        print(\"Columnas extra√≠das:\", list(features_df.columns))\n",
    "        \n",
    "        # AGREGAR CAMPO 'APTO' BASADO EN REGLAS DE NEGOCIO\n",
    "        print(\"\\nAplicando reglas para determinar candidatos aptos...\")\n",
    "        \n",
    "        def evaluar_aptitud_ingeniero(row):\n",
    "            \"\"\"\n",
    "            Eval√∫a si un candidato es apto para un puesto de ingeniero\n",
    "            Retorna: 1 (apto), 0 (no apto), -1 (necesita revisi√≥n manual)\n",
    "            \"\"\"\n",
    "            score = 0\n",
    "            \n",
    "            # 1. Experiencia m√≠nima (peso: 3 puntos)\n",
    "            years_exp = row.get('years_total_experience', 0) or 0\n",
    "            if years_exp >= 5:\n",
    "                score += 3\n",
    "            elif years_exp >= 2:\n",
    "                score += 2\n",
    "            elif years_exp >= 1:\n",
    "                score += 1\n",
    "            \n",
    "            # 2. Nivel educativo (peso: 2 puntos)\n",
    "            education = row.get('education_level', '')\n",
    "            if education in ['PhD', 'Master']:\n",
    "                score += 2\n",
    "            elif education == 'Bachelor':\n",
    "                score += 1\n",
    "            \n",
    "            # 3. Skills t√©cnicos relevantes (peso: 3 puntos)\n",
    "            skills = row.get('skills', []) or []\n",
    "            if isinstance(skills, list):\n",
    "                technical_skills = [\n",
    "                    'python', 'java', 'c++', 'javascript', 'sql', 'aws', 'azure',\n",
    "                    'machine learning', 'data analysis', 'software development',\n",
    "                    'project management', 'agile', 'scrum', 'git', 'docker'\n",
    "                ]\n",
    "                skills_text = ' '.join(skills).lower()\n",
    "                matching_skills = sum(1 for skill in technical_skills if skill in skills_text)\n",
    "                \n",
    "                if matching_skills >= 3:\n",
    "                    score += 3\n",
    "                elif matching_skills >= 2:\n",
    "                    score += 2\n",
    "                elif matching_skills >= 1:\n",
    "                    score += 1\n",
    "            \n",
    "            # 4. Certificaciones (peso: 1 punto)\n",
    "            certifications = row.get('certifications', []) or []\n",
    "            if isinstance(certifications, list) and len(certifications) > 0:\n",
    "                score += 1\n",
    "            \n",
    "            # 5. Idiomas (peso: 1 punto si incluye ingl√©s)\n",
    "            languages = row.get('languages', {}) or {}\n",
    "            if isinstance(languages, dict):\n",
    "                english_level = languages.get('English', '').upper()\n",
    "                if english_level in ['B2', 'C1', 'C2', 'NATIVE']:\n",
    "                    score += 1\n",
    "            \n",
    "            # Criterios de decisi√≥n\n",
    "            if score >= 7:\n",
    "                return 1    # Apto\n",
    "            elif score >= 4:\n",
    "                return -1   # Necesita revisi√≥n manual\n",
    "            else:\n",
    "                return 0    # No apto\n",
    "        \n",
    "        # Aplicar evaluaci√≥n\n",
    "        features_df['apto'] = features_df.apply(evaluar_aptitud_ingeniero, axis=1)\n",
    "        \n",
    "        # Estad√≠sticas de aptitud\n",
    "        aptitud_counts = features_df['apto'].value_counts()\n",
    "        total = len(features_df)\n",
    "        \n",
    "        print(f\"Evaluaci√≥n de aptitud completada:\")\n",
    "        print(f\"  Aptos (1): {aptitud_counts.get(1, 0)} ({aptitud_counts.get(1, 0)/total*100:.1f}%)\")\n",
    "        print(f\"  No aptos (0): {aptitud_counts.get(0, 0)} ({aptitud_counts.get(0, 0)/total*100:.1f}%)\")\n",
    "        print(f\"  Revisi√≥n manual (-1): {aptitud_counts.get(-1, 0)} ({aptitud_counts.get(-1, 0)/total*100:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR: No se pudo extraer ninguna caracter√≠stica\")\n",
    "        features_df = pd.DataFrame()\n",
    "        \n",
    "else:\n",
    "    print(\"SALTANDO: Extracci√≥n - no hay texto anonimizado para procesar\")\n",
    "    features_df = pd.DataFrame()\n",
    "\n",
    "# Guardar dataset con caracter√≠sticas extra√≠das y campo apto\n",
    "if len(features_df) > 0:\n",
    "    features_df.to_parquet(PROCESSED_PATH / \"features_extracted.parquet\", index=False)\n",
    "    features_df.to_csv(PROCESSED_PATH / \"features_extracted.csv\", index=False)\n",
    "    print(f\"\\nDataset con caracter√≠sticas guardado en:\")\n",
    "    print(f\"  Parquet: {PROCESSED_PATH / 'features_extracted.parquet'}\")\n",
    "    print(f\"  CSV: {PROCESSED_PATH / 'features_extracted.csv'}\")\n",
    "else:\n",
    "    print(\"No hay datos para guardar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinando datos originales con caracter√≠sticas extra√≠das...\n",
      "Dataset enriquecido creado: (236, 26)\n",
      "Aplicando validaciones y limpieza...\n",
      "\n",
      "Porcentaje de valores nulos por columna:\n",
      "  ID: 50.0% (ALTO)\n",
      "  Resume_str: 50.0% (ALTO)\n",
      "  Resume_html: 50.0% (ALTO)\n",
      "  Category: 50.0% (ALTO)\n",
      "  education_level: 50.0% (ALTO)\n",
      "  discipline: 50.4% (ALTO)\n",
      "  years_total_experience: 50.0% (ALTO)\n",
      "  years_skill_main: 50.0% (ALTO)\n",
      "  current_job_title: 50.0% (ALTO)\n",
      "  skills: 50.0% (ALTO)\n",
      "  certifications: 50.0% (ALTO)\n",
      "  num_promotions: 50.0% (ALTO)\n",
      "  avg_tenure_months: 50.0% (ALTO)\n",
      "  gap_months_last5y: 50.0% (ALTO)\n",
      "  location_city: 50.0% (ALTO)\n",
      "  work_authorization: 50.0% (ALTO)\n",
      "  gender: 52.1% (ALTO)\n",
      "  age_range: 50.0% (ALTO)\n",
      "  languages.English: 55.5% (ALTO)\n",
      "  languages.Spanish: 97.9% (ALTO)\n",
      "  languages.Hindi: 98.3% (ALTO)\n",
      "  languages.Armenian: 99.6% (ALTO)\n",
      "  languages.German: 99.6% (ALTO)\n",
      "  languages.Turkish: 99.6% (ALTO)\n",
      "  languages.Mandarin: 99.6% (ALTO)\n",
      "  apto: 50.0% (ALTO)\n"
     ]
    }
   ],
   "source": [
    "# 5. NORMALIZACI√ìN, VALIDACI√ìN Y ENRIQUECIMIENTO\n",
    "import numpy as np\n",
    "\n",
    "# Combinar datos originales con caracter√≠sticas extra√≠das\n",
    "if len(features_df) > 0:\n",
    "    print(\"Combinando datos originales con caracter√≠sticas extra√≠das...\")\n",
    "    \n",
    "    # Combinar solo si tenemos el mismo n√∫mero de registros exitosos\n",
    "    if len(features_df) == len(df):\n",
    "        enriched = pd.concat([\n",
    "            df.drop(columns=[\"text\", \"text_anonymized\"], errors='ignore'), \n",
    "            features_df\n",
    "        ], axis=1)\n",
    "    else:\n",
    "        # Si hay menos caracter√≠sticas que registros originales debido a errores\n",
    "        print(f\"WARNING: {len(df)} registros originales vs {len(features_df)} caracter√≠sticas extra√≠das\")\n",
    "        enriched = features_df.copy()\n",
    "    \n",
    "    print(f\"Dataset enriquecido creado: {enriched.shape}\")\n",
    "    \n",
    "    # VALIDACI√ìN Y LIMPIEZA DE DATOS\n",
    "    print(\"Aplicando validaciones y limpieza...\")\n",
    "    \n",
    "    # 1. Campos num√©ricos: limitar a rangos razonables\n",
    "    numeric_validations = {\n",
    "        \"years_total_experience\": (0, 50),    # M√°ximo 50 a√±os de experiencia\n",
    "        \"years_skill_main\": (0, 30),          # M√°ximo 30 a√±os en habilidad principal  \n",
    "        \"num_promotions\": (0, 20),            # M√°ximo 20 promociones\n",
    "        \"avg_tenure_months\": (0, 600),        # M√°ximo 50 a√±os (600 meses) en un trabajo\n",
    "        \"gap_months_last5y\": (0, 60)          # M√°ximo 5 a√±os de brecha\n",
    "    }\n",
    "    \n",
    "    for col, (min_val, max_val) in numeric_validations.items():\n",
    "        if col in enriched.columns:\n",
    "            # Convertir a num√©rico y aplicar l√≠mites\n",
    "            enriched[col] = pd.to_numeric(enriched[col], errors='coerce')\n",
    "            enriched[col] = enriched[col].clip(min_val, max_val)\n",
    "    \n",
    "    # 2. Limpiar education_level: estandarizar valores\n",
    "    if \"education_level\" in enriched.columns:\n",
    "        education_mapping = {\n",
    "            \"phd\": \"PhD\", \"doctorate\": \"PhD\", \"doctoral\": \"PhD\",\n",
    "            \"master\": \"Master\", \"masters\": \"Master\", \"mba\": \"Master\",\n",
    "            \"bachelor\": \"Bachelor\", \"bachelors\": \"Bachelor\", \"ba\": \"Bachelor\", \"bs\": \"Bachelor\",\n",
    "            \"associate\": \"Associate\", \"associates\": \"Associate\",\n",
    "            \"high school\": \"High School\", \"highschool\": \"High School\",\n",
    "            \"diploma\": \"High School\"\n",
    "        }\n",
    "        \n",
    "        enriched[\"education_level\"] = enriched[\"education_level\"].astype(str).str.lower()\n",
    "        enriched[\"education_level\"] = enriched[\"education_level\"].replace(education_mapping)\n",
    "        enriched[\"education_level\"] = enriched[\"education_level\"].replace(\"nan\", None)\n",
    "    \n",
    "    # 3. Control de calidad: revisar nulos\n",
    "    null_percentages = enriched.isnull().mean() * 100\n",
    "    print(\"\\nPorcentaje de valores nulos por columna:\")\n",
    "    for col, null_pct in null_percentages.items():\n",
    "        if null_pct > 25:\n",
    "            print(f\"  {col}: {null_pct:.1f}% (ALTO)\")\n",
    "        elif null_pct > 0:\n",
    "            print(f\"  {col}: {null_pct:.1f}%\")\n",
    "    \n",
    "    # Verificar que no hay demasiados nulos\n",
    "    critical_fields = [\"current_job_title\", \"skills\"]\n",
    "    for field in critical_fields:\n",
    "        if field in enriched.columns:\n",
    "            null_pct = enriched[field].isnull().mean() * 100\n",
    "            if null_pct > 50:\n",
    "                print(f\"WARNING: Campo cr√≠tico '{field}' tiene {null_pct:.1f}% de nulos\")\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No hay caracter√≠sticas extra√≠das para procesar\")\n",
    "    enriched = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FILTRADO PARA PUESTOS DE INGENIER√çA\n",
      "==================================================\n",
      "üìÅ Configurando ruta de datos procesados: ..\\data\\processed\n",
      "Verificando disponibilidad de datos...\n",
      "‚ö†Ô∏è Variable 'enriched' no disponible en la sesi√≥n actual\n",
      "‚úÖ Datos cargados desde archivo parquet: 118 registros\n",
      "Dataset antes del filtrado: 118 registros\n",
      "Aplicando filtros de ingenier√≠a...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Aplicar funci√≥n de filtrado\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAplicando filtros de ingenier√≠a...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m engineering_mask = \u001b[43menriched\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_engineering_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m engineers_df = enriched[engineering_mask].copy()\n\u001b[32m    121\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPerfiles de ingenier√≠a identificados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(engineers_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m registros\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10381\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10367\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10369\u001b[39m op = frame_apply(\n\u001b[32m  10370\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10371\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10379\u001b[39m     kwargs=kwargs,\n\u001b[32m  10380\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alumno_AI\\carlos\\practica_cvs\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mis_engineering_profile\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Verificar skills (si es una lista o string)\u001b[39;00m\n\u001b[32m     96\u001b[39m skills_to_check = row.get(\u001b[33m'\u001b[39m\u001b[33mskills\u001b[39m\u001b[33m'\u001b[39m, [])\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.notna(skills_to_check):\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(skills_to_check, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     99\u001b[39m         skills_text = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(skill) \u001b[38;5;28;01mfor\u001b[39;00m skill \u001b[38;5;129;01min\u001b[39;00m skills_to_check).lower()\n",
      "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# 6. FILTRADO ESPEC√çFICO PARA INGENIEROS\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FILTRADO PARA PUESTOS DE INGENIER√çA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Importaciones y configuraci√≥n de rutas (en caso de que no est√©n disponibles)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar rutas si no est√°n definidas\n",
    "try:\n",
    "    PROCESSED_PATH\n",
    "except NameError:\n",
    "    PROCESSED_PATH = Path(\"../data/processed/\")\n",
    "    print(f\"üìÅ Configurando ruta de datos procesados: {PROCESSED_PATH}\")\n",
    "\n",
    "# Verificar si tenemos datos para procesar\n",
    "print(\"Verificando disponibilidad de datos...\")\n",
    "\n",
    "# Intentar cargar datos desde diferentes fuentes\n",
    "enriched = None\n",
    "\n",
    "# 1. Verificar si existe la variable 'enriched' del procesamiento anterior\n",
    "try:\n",
    "    if 'enriched' in globals():\n",
    "        # Verificar que la variable existe y no es None\n",
    "        existing_enriched = globals()['enriched']\n",
    "        if existing_enriched is not None and len(existing_enriched) > 0:\n",
    "            enriched = existing_enriched\n",
    "            print(f\"‚úÖ Usando variable 'enriched' de la sesi√≥n actual: {len(enriched)} registros\")\n",
    "        else:\n",
    "            raise NameError(\"Variable 'enriched' existe pero est√° vac√≠a o es None\")\n",
    "    else:\n",
    "        raise NameError(\"Variable 'enriched' no disponible\")\n",
    "except (NameError, TypeError):\n",
    "    print(\"‚ö†Ô∏è Variable 'enriched' no disponible en la sesi√≥n actual\")\n",
    "    \n",
    "    # 2. Intentar cargar desde archivo parquet procesado\n",
    "    try:\n",
    "        features_path = PROCESSED_PATH / \"features_extracted.parquet\"\n",
    "        if features_path.exists():\n",
    "            enriched = pd.read_parquet(features_path)\n",
    "            print(f\"‚úÖ Datos cargados desde archivo parquet: {len(enriched)} registros\")\n",
    "        else:\n",
    "            # 3. Intentar cargar desde archivo CSV como respaldo\n",
    "            csv_path = PROCESSED_PATH / \"features_extracted.csv\"\n",
    "            if csv_path.exists():\n",
    "                enriched = pd.read_csv(csv_path)\n",
    "                print(f\"‚úÖ Datos cargados desde archivo CSV: {len(enriched)} registros\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No se encontraron archivos de datos procesados\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando datos desde archivos: {e}\")\n",
    "        print(\"üí° Ejecuta las celdas anteriores para generar los datos\")\n",
    "        enriched = pd.DataFrame()\n",
    "\n",
    "# Continuar solo si tenemos datos\n",
    "if enriched is not None and len(enriched) > 0:\n",
    "    # Definir t√©rminos relacionados con ingenier√≠a\n",
    "    engineering_keywords = {\n",
    "        'disciplines': [\n",
    "            'engineering', 'engineer', 'mechanical', 'electrical', 'civil', 'chemical', \n",
    "            'software', 'computer science', 'information technology', 'data science',\n",
    "            'systems', 'industrial', 'aerospace', 'biomedical', 'environmental',\n",
    "            'structural', 'automotive', 'telecommunications', 'robotics'\n",
    "        ],\n",
    "        'job_titles': [\n",
    "            'engineer', 'developer', 'architect', 'analyst', 'scientist', 'programmer',\n",
    "            'technician', 'specialist', 'consultant', 'manager', 'lead', 'senior',\n",
    "            'principal', 'staff', 'chief technology', 'technical director'\n",
    "        ],\n",
    "        'skills': [\n",
    "            'python', 'java', 'c++', 'matlab', 'autocad', 'solidworks', 'aws', 'azure',\n",
    "            'machine learning', 'artificial intelligence', 'data analysis', 'sql',\n",
    "            'project management', 'agile', 'scrum', 'tensorflow', 'kubernetes'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    def is_engineering_profile(row):\n",
    "        \"\"\"Determina si un perfil es de ingenier√≠a basado en m√∫ltiples campos\"\"\"\n",
    "        score = 0\n",
    "        \n",
    "        # Verificar discipline\n",
    "        if pd.notna(row.get('discipline', '')):\n",
    "            discipline = str(row['discipline']).lower()\n",
    "            if any(keyword in discipline for keyword in engineering_keywords['disciplines']):\n",
    "                score += 3\n",
    "        \n",
    "        # Verificar current_job_title\n",
    "        if pd.notna(row.get('current_job_title', '')):\n",
    "            job_title = str(row['current_job_title']).lower()\n",
    "            if any(keyword in job_title for keyword in engineering_keywords['job_titles']):\n",
    "                score += 2\n",
    "        \n",
    "        # Verificar skills (si es una lista o string)\n",
    "        skills_to_check = row.get('skills', [])\n",
    "        if pd.notna(skills_to_check):\n",
    "            if isinstance(skills_to_check, list):\n",
    "                skills_text = ' '.join(str(skill) for skill in skills_to_check).lower()\n",
    "            else:\n",
    "                skills_text = str(skills_to_check).lower()\n",
    "                \n",
    "            matching_skills = sum(1 for keyword in engineering_keywords['skills'] \n",
    "                                if keyword in skills_text)\n",
    "            score += min(matching_skills, 2)  # M√°ximo 2 puntos por skills\n",
    "        \n",
    "        # Verificar education_level (preferir t√≠tulos t√©cnicos)\n",
    "        if row.get('education_level') in ['Bachelor', 'Master', 'PhD']:\n",
    "            score += 1\n",
    "        \n",
    "        return score >= 3  # Threshold m√≠nimo para considerar como ingeniero\n",
    "\n",
    "    # Aplicar filtrado\n",
    "    print(f\"Dataset antes del filtrado: {len(enriched)} registros\")\n",
    "    \n",
    "    # Aplicar funci√≥n de filtrado\n",
    "    print(\"Aplicando filtros de ingenier√≠a...\")\n",
    "    engineering_mask = enriched.apply(is_engineering_profile, axis=1)\n",
    "    engineers_df = enriched[engineering_mask].copy()\n",
    "    \n",
    "    print(f\"Perfiles de ingenier√≠a identificados: {len(engineers_df)} registros\")\n",
    "    print(f\"Porcentaje de ingenieros: {len(engineers_df)/len(enriched)*100:.1f}%\")\n",
    "    \n",
    "    # Mostrar estad√≠sticas de los perfiles filtrados\n",
    "    if len(engineers_df) > 0:\n",
    "        print(\"\\nESTAD√çSTICAS DE PERFILES DE INGENIER√çA:\")\n",
    "        \n",
    "        # Education levels\n",
    "        if 'education_level' in engineers_df.columns:\n",
    "            education_counts = engineers_df['education_level'].value_counts(dropna=False)\n",
    "            print(f\"\\nNiveles de educaci√≥n:\")\n",
    "            for edu, count in education_counts.items():\n",
    "                print(f\"  {edu}: {count} ({count/len(engineers_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Disciplinas m√°s comunes\n",
    "        if 'discipline' in engineers_df.columns:\n",
    "            discipline_counts = engineers_df['discipline'].value_counts(dropna=False).head(10)\n",
    "            print(f\"\\nTop 10 disciplinas:\")\n",
    "            for disc, count in discipline_counts.items():\n",
    "                print(f\"  {disc}: {count}\")\n",
    "        \n",
    "        # Experiencia promedio\n",
    "        if 'years_total_experience' in engineers_df.columns:\n",
    "            avg_exp = engineers_df['years_total_experience'].mean()\n",
    "            if pd.notna(avg_exp):\n",
    "                print(f\"\\nExperiencia promedio: {avg_exp:.1f} a√±os\")\n",
    "        \n",
    "        # Estad√≠sticas del campo 'apto' si existe\n",
    "        if 'apto' in engineers_df.columns:\n",
    "            aptitud_counts = engineers_df['apto'].value_counts()\n",
    "            total = len(engineers_df)\n",
    "            \n",
    "            print(f\"\\nESTAD√çSTICAS DE APTITUD:\")\n",
    "            labels = {1: 'Aptos', 0: 'No aptos', -1: 'Revisi√≥n manual'}\n",
    "            for value, count in aptitud_counts.items():\n",
    "                label = labels.get(value, f'Valor {value}')\n",
    "                percentage = count / total * 100\n",
    "                print(f\"  {label}: {count} candidatos ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Dataset final para entrenamiento\n",
    "    final_dataset = engineers_df.copy()\n",
    "    \n",
    "    # Guardar dataset filtrado\n",
    "    if len(final_dataset) > 0:\n",
    "        print(f\"\\n‚úÖ FILTRADO COMPLETADO EXITOSAMENTE\")\n",
    "        print(f\"üìä Dataset de ingenieros listo: {len(final_dataset)} registros\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå ERROR: No hay datos enriquecidos para filtrar\")\n",
    "    print(\"üí° Aseg√∫rate de ejecutar las celdas anteriores primero\")\n",
    "    final_dataset = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EXPORTACI√ìN Y AN√ÅLISIS FINAL\n",
      "==================================================\n",
      "‚ö†Ô∏è Variable 'final_dataset' no disponible\n",
      "üí° Intentando cargar desde archivos existentes...\n",
      "‚úÖ Usando dataset de caracter√≠sticas: 118 registros\n",
      "Dataset final guardado:\n",
      "  Parquet: ..\\data\\processed\\engineers_dataset_for_training.parquet\n",
      "  CSV: ..\\data\\processed\\engineers_dataset_for_training.csv\n",
      "  Registros: 118\n",
      "  Columnas: 22\n",
      "\n",
      "ESQUEMA DEL DATASET FINAL:\n",
      "  education_level           | object          |    0 nulos (  0.0%)\n",
      "  discipline                | object          |    1 nulos (  0.8%)\n",
      "  years_total_experience    | int64           |    0 nulos (  0.0%)\n",
      "  years_skill_main          | int64           |    0 nulos (  0.0%)\n",
      "  current_job_title         | object          |    0 nulos (  0.0%)\n",
      "  skills                    | object          |    0 nulos (  0.0%)\n",
      "  certifications            | object          |    0 nulos (  0.0%)\n",
      "  num_promotions            | int64           |    0 nulos (  0.0%)\n",
      "  avg_tenure_months         | int64           |    0 nulos (  0.0%)\n",
      "  gap_months_last5y         | int64           |    0 nulos (  0.0%)\n",
      "  location_city             | object          |    0 nulos (  0.0%)\n",
      "  work_authorization        | object          |    0 nulos (  0.0%)\n",
      "  gender                    | object          |    5 nulos (  4.2%)\n",
      "  age_range                 | object          |    0 nulos (  0.0%)\n",
      "  languages.English         | object          |   13 nulos ( 11.0%)\n",
      "  languages.Spanish         | object          |  113 nulos ( 95.8%)\n",
      "  languages.Hindi           | object          |  114 nulos ( 96.6%)\n",
      "  languages.Armenian        | object          |  117 nulos ( 99.2%)\n",
      "  languages.German          | object          |  117 nulos ( 99.2%)\n",
      "  languages.Turkish         | object          |  117 nulos ( 99.2%)\n",
      "  languages.Mandarin        | object          |  117 nulos ( 99.2%)\n",
      "  apto                      | int64           |    0 nulos (  0.0%)\n",
      "\n",
      "ESTAD√çSTICAS DEL CAMPO 'APTO':\n",
      "  Revisi√≥n manual: 86 candidatos (72.9%)\n",
      "  Aptos: 19 candidatos (16.1%)\n",
      "  No aptos: 13 candidatos (11.0%)\n",
      "\n",
      "MUESTRA DE LOS PRIMEROS 3 REGISTROS:\n",
      "     current_job_title              discipline  years_total_experience education_level  apto\n",
      "Engineering Technician Business Administration                      10        Bachelor    -1\n",
      " Engineering Assistant             Engineering                      20           Other    -1\n",
      "   Engineering Manager                    None                      25           Other    -1\n",
      "\n",
      "WARNING: matplotlib no est√° disponible para generar histogramas\n",
      "Para instalar: pip install matplotlib\n",
      "\n",
      "‚úÖ DATASET PARA ENTRENAMIENTO DE MODELO DE SELECCI√ìN LISTO!\n",
      "üìä 118 perfiles de ingenier√≠a estructurados\n",
      "üéØ Listo para entrenar modelo supervisado de selecci√≥n de candidatos\n",
      "üîç Etiquetas para entrenamiento: 19 aptos, 13 no aptos, 86 para revisi√≥n\n",
      "üìà Modelo puede entrenarse con clasificaci√≥n binaria o multiclase\n"
     ]
    }
   ],
   "source": [
    "# 7. EXPORTACI√ìN Y VISUALIZACI√ìN FINAL\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPORTACI√ìN Y AN√ÅLISIS FINAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Verificar si tenemos el dataset final\n",
    "try:\n",
    "    if 'final_dataset' in globals() and len(final_dataset) > 0:\n",
    "        print(f\"‚úÖ Dataset final disponible: {len(final_dataset)} registros\")\n",
    "    else:\n",
    "        raise NameError(\"Variable 'final_dataset' no disponible\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è Variable 'final_dataset' no disponible\")\n",
    "    print(\"üí° Intentando cargar desde archivos existentes...\")\n",
    "    \n",
    "    # Intentar cargar dataset de ingenieros si existe\n",
    "    try:\n",
    "        engineers_path = PROCESSED_PATH / \"engineers_dataset_for_training.parquet\"\n",
    "        if engineers_path.exists():\n",
    "            final_dataset = pd.read_parquet(engineers_path)\n",
    "            print(f\"‚úÖ Dataset cargado desde archivo: {len(final_dataset)} registros\")\n",
    "        else:\n",
    "            # Usar el dataset general de caracter√≠sticas\n",
    "            features_path = PROCESSED_PATH / \"features_extracted.parquet\"\n",
    "            if features_path.exists():\n",
    "                final_dataset = pd.read_parquet(features_path)\n",
    "                print(f\"‚úÖ Usando dataset de caracter√≠sticas: {len(final_dataset)} registros\")\n",
    "            else:\n",
    "                final_dataset = pd.DataFrame()\n",
    "                print(\"‚ùå No se encontraron archivos de datos\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando datos: {e}\")\n",
    "        final_dataset = pd.DataFrame()\n",
    "\n",
    "if len(final_dataset) > 0:\n",
    "    # Guardar dataset final\n",
    "    final_parquet_path = PROCESSED_PATH / \"engineers_dataset_for_training.parquet\"\n",
    "    final_csv_path = PROCESSED_PATH / \"engineers_dataset_for_training.csv\"\n",
    "    \n",
    "    final_dataset.to_parquet(final_parquet_path, index=False)\n",
    "    final_dataset.to_csv(final_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Dataset final guardado:\")\n",
    "    print(f\"  Parquet: {final_parquet_path}\")\n",
    "    print(f\"  CSV: {final_csv_path}\")\n",
    "    print(f\"  Registros: {len(final_dataset)}\")\n",
    "    print(f\"  Columnas: {len(final_dataset.columns)}\")\n",
    "    \n",
    "    # Resumen del esquema final\n",
    "    print(f\"\\nESQUEMA DEL DATASET FINAL:\")\n",
    "    for col in final_dataset.columns:\n",
    "        dtype = final_dataset[col].dtype\n",
    "        null_count = final_dataset[col].isnull().sum()\n",
    "        null_pct = null_count / len(final_dataset) * 100\n",
    "        print(f\"  {col:25s} | {str(dtype):15s} | {null_count:4d} nulos ({null_pct:5.1f}%)\")\n",
    "    \n",
    "    # Estad√≠sticas del campo 'apto'\n",
    "    if 'apto' in final_dataset.columns:\n",
    "        print(f\"\\nESTAD√çSTICAS DEL CAMPO 'APTO':\")\n",
    "        aptitud_counts = final_dataset['apto'].value_counts()\n",
    "        total = len(final_dataset)\n",
    "        \n",
    "        labels = {1: 'Aptos', 0: 'No aptos', -1: 'Revisi√≥n manual'}\n",
    "        for value, count in aptitud_counts.items():\n",
    "            label = labels.get(value, f'Valor {value}')\n",
    "            percentage = count / total * 100\n",
    "            print(f\"  {label}: {count} candidatos ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Muestra de datos\n",
    "    print(f\"\\nMUESTRA DE LOS PRIMEROS 3 REGISTROS:\")\n",
    "    sample_cols = ['current_job_title', 'discipline', 'years_total_experience', 'education_level', 'apto']\n",
    "    available_cols = [col for col in sample_cols if col in final_dataset.columns]\n",
    "    if available_cols:\n",
    "        print(final_dataset[available_cols].head(3).to_string(index=False))\n",
    "    \n",
    "    # Generar histogramas y visualizaciones si hay matplotlib\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Visualizaciones num√©ricas\n",
    "        numeric_cols = ['years_total_experience', 'years_skill_main', 'num_promotions', \n",
    "                       'avg_tenure_months', 'gap_months_last5y']\n",
    "        available_numeric = [col for col in numeric_cols if col in final_dataset.columns]\n",
    "        \n",
    "        if available_numeric:\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, col in enumerate(available_numeric[:5]):\n",
    "                data = final_dataset[col].dropna()\n",
    "                if len(data) > 0:\n",
    "                    axes[i].hist(data, bins=min(20, len(data.unique())), alpha=0.7, edgecolor='black')\n",
    "                    axes[i].set_title(f'{col}\\n(n={len(data)}, avg={data.mean():.1f})')\n",
    "                    axes[i].set_xlabel(col)\n",
    "                    axes[i].set_ylabel('Frecuencia')\n",
    "            \n",
    "            # Gr√°fico de barras para el campo 'apto' en el √∫ltimo subplot\n",
    "            if 'apto' in final_dataset.columns:\n",
    "                aptitud_counts = final_dataset['apto'].value_counts().sort_index()\n",
    "                labels = {1: 'Aptos', 0: 'No aptos', -1: 'Revisi√≥n\\nmanual'}\n",
    "                \n",
    "                x_labels = [labels.get(val, str(val)) for val in aptitud_counts.index]\n",
    "                colors = ['red', 'orange', 'green']\n",
    "                \n",
    "                axes[5].bar(x_labels, aptitud_counts.values, \n",
    "                           color=colors[:len(aptitud_counts)], alpha=0.7, edgecolor='black')\n",
    "                axes[5].set_title(f'Distribuci√≥n Aptitud\\n(Total: {len(final_dataset)})')\n",
    "                axes[5].set_ylabel('Frecuencia')\n",
    "                \n",
    "                # A√±adir valores en las barras\n",
    "                for i, v in enumerate(aptitud_counts.values):\n",
    "                    axes[5].text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "            else:\n",
    "                axes[5].set_visible(False)\n",
    "            \n",
    "            # Ocultar subplot vac√≠o si es necesario\n",
    "            if len(available_numeric) < 5:\n",
    "                for i in range(len(available_numeric), 5):\n",
    "                    axes[i].set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(PROCESSED_PATH / \"dataset_distributions.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"\\nHistogramas guardados en: {PROCESSED_PATH / 'dataset_distributions.png'}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\nWARNING: matplotlib no est√° disponible para generar histogramas\")\n",
    "        print(\"Para instalar: pip install matplotlib\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ DATASET PARA ENTRENAMIENTO DE MODELO DE SELECCI√ìN LISTO!\")\n",
    "    print(f\"üìä {len(final_dataset)} perfiles de ingenier√≠a estructurados\")\n",
    "    print(f\"üéØ Listo para entrenar modelo supervisado de selecci√≥n de candidatos\")\n",
    "    \n",
    "    if 'apto' in final_dataset.columns:\n",
    "        aptos = (final_dataset['apto'] == 1).sum()\n",
    "        no_aptos = (final_dataset['apto'] == 0).sum()\n",
    "        revision = (final_dataset['apto'] == -1).sum()\n",
    "        print(f\"üîç Etiquetas para entrenamiento: {aptos} aptos, {no_aptos} no aptos, {revision} para revisi√≥n\")\n",
    "        print(f\"üìà Modelo puede entrenarse con clasificaci√≥n binaria o multiclase\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå ERROR: No se pudo crear el dataset final\")\n",
    "    print(\"Revisa los pasos anteriores para identificar problemas\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
