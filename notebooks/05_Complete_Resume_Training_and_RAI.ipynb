{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Entrenamiento de Modelo de Curr√≠culums + Dashboard RAI\n",
        "\n",
        "Este notebook completo:\n",
        "1. **Entrena un modelo de clasificaci√≥n** con los datos de curr√≠culums\n",
        "2. **Registra el modelo** en Azure ML Studio\n",
        "3. **Crea un dashboard de IA Responsable** para analizar el modelo\n",
        "\n",
        "## üéØ Objetivo\n",
        "Entrenar un modelo que clasifique la aptitud de candidatos de ingenier√≠a basado en sus curr√≠culums\n",
        "y generar un an√°lisis completo de IA Responsable.\n",
        "\n",
        "## üìä Dataset\n",
        "- **Fuente**: Curr√≠culums de ingenieros procesados\n",
        "- **Target**: `apto` (valores: -1, 0, 1)\n",
        "- **Features**: educaci√≥n, experiencia, habilidades, certificaciones, etc.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Configuraci√≥n inicial y carga de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar/verificar paquetes necesarios\n",
        "!pip show azure-ai-ml scikit-learn pandas numpy pyarrow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Azure ML\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Model, Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar Azure ML Client\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    credential = InteractiveBrowserCredential()\n",
        "\n",
        "# Conectar al workspace\n",
        "ml_client = MLClient.from_config(credential=credential)\n",
        "print(f\"‚úÖ Conectado al workspace: {ml_client.workspace_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datos de curr√≠culums\n",
        "data_path = \"../data/processed/features_extracted.csv\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(f\"üìä Datos cargados: {df.shape}\")\n",
        "print(f\"üìã Columnas: {list(df.columns)}\")\n",
        "print(f\"üéØ Distribuci√≥n de target 'apto':\")\n",
        "print(df['apto'].value_counts())\n",
        "\n",
        "# Mostrar preview\n",
        "print(\"\\nüìã Preview de los datos:\")\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Preparaci√≥n de datos para entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar features para el modelo\n",
        "def prepare_features(df):\n",
        "    \"\"\"Prepara las features para entrenamiento del modelo\"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # Manejar valores faltantes\n",
        "    df_processed['discipline'] = df_processed['discipline'].fillna('Unknown')\n",
        "    df_processed['gender'] = df_processed['gender'].fillna('Unknown')\n",
        "    df_processed['age_range'] = df_processed['age_range'].fillna('Unknown')\n",
        "    \n",
        "    # Seleccionar features num√©ricas y categ√≥ricas\n",
        "    numeric_features = [\n",
        "        'years_total_experience', 'years_skill_main', 'num_promotions', \n",
        "        'avg_tenure_months', 'gap_months_last5y'\n",
        "    ]\n",
        "    \n",
        "    categorical_features = [\n",
        "        'education_level', 'discipline', 'work_authorization', \n",
        "        'gender', 'age_range'\n",
        "    ]\n",
        "    \n",
        "    # Agregar features de idiomas (ya son num√©ricas - Native/C1/B2/etc se pueden tratar como categ√≥ricas)\n",
        "    language_features = [col for col in df.columns if col.startswith('languages.')]\n",
        "    \n",
        "    # Para este ejemplo, convertiremos idiomas a features binarias (tiene/no tiene)\n",
        "    for lang_col in language_features:\n",
        "        df_processed[f'{lang_col}_has'] = (~df_processed[lang_col].isna()).astype(int)\n",
        "    \n",
        "    # Crear feature de n√∫mero de habilidades (contar elementos en la lista de skills)\n",
        "    def count_skills(skills_str):\n",
        "        if pd.isna(skills_str) or skills_str == '[]':\n",
        "            return 0\n",
        "        try:\n",
        "            # Contar elementos separados por comas dentro de los corchetes\n",
        "            return len([s.strip() for s in skills_str.strip('[]').split(',') if s.strip().replace(\"'\", \"\").replace('\"', '').strip()])\n",
        "        except:\n",
        "            return 0\n",
        "    \n",
        "    df_processed['num_skills'] = df['skills'].apply(count_skills)\n",
        "    \n",
        "    # Crear feature de n√∫mero de certificaciones\n",
        "    def count_certifications(certs_str):\n",
        "        if pd.isna(certs_str) or certs_str == '[]':\n",
        "            return 0\n",
        "        try:\n",
        "            return len([c.strip() for c in certs_str.strip('[]').split(',') if c.strip().replace(\"'\", \"\").replace('\"', '').strip()])\n",
        "        except:\n",
        "            return 0\n",
        "    \n",
        "    df_processed['num_certifications'] = df['certifications'].apply(count_certifications)\n",
        "    \n",
        "    # Actualizar features num√©ricas\n",
        "    numeric_features.extend(['num_skills', 'num_certifications'])\n",
        "    numeric_features.extend([f'{col}_has' for col in language_features])\n",
        "    \n",
        "    return df_processed, numeric_features, categorical_features\n",
        "\n",
        "# Preparar datos\n",
        "df_processed, numeric_features, categorical_features = prepare_features(df)\n",
        "\n",
        "print(f\"üîß Features num√©ricas ({len(numeric_features)}): {numeric_features}\")\n",
        "print(f\"üè∑Ô∏è Features categ√≥ricas ({len(categorical_features)}): {categorical_features}\")\n",
        "print(f\"üìä Dataset procesado: {df_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar X y y\n",
        "feature_columns = numeric_features + categorical_features\n",
        "X = df_processed[feature_columns]\n",
        "y = df_processed['apto']\n",
        "\n",
        "print(f\"üìä Features (X): {X.shape}\")\n",
        "print(f\"üéØ Target (y): {y.shape}\")\n",
        "print(f\"üìà Distribuci√≥n del target:\")\n",
        "print(y.value_counts().sort_index())\n",
        "\n",
        "# Dividir en train y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Divisi√≥n de datos:\")\n",
        "print(f\"  üöÇ Train: {X_train.shape[0]} muestras\")\n",
        "print(f\"  üß™ Test: {X_test.shape[0]} muestras\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Entrenamiento de modelos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear pipeline de preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Definir modelos a entrenar\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# Entrenar modelos\n",
        "trained_models = {}\n",
        "model_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nüöÇ Entrenando {name}...\")\n",
        "    \n",
        "    # Crear pipeline completo\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "    \n",
        "    # Entrenar\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Predecir\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_pred_proba = pipeline.predict_proba(X_test)\n",
        "    \n",
        "    # Calcular m√©tricas\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    # Guardar resultados\n",
        "    trained_models[name] = pipeline\n",
        "    model_results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ {name} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nüèÜ Resumen de modelos:\")\n",
        "for name, results in model_results.items():\n",
        "    print(f\"  {name}: {results['accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionar el mejor modelo\n",
        "best_model_name = max(model_results.keys(), key=lambda k: model_results[k]['accuracy'])\n",
        "best_model = trained_models[best_model_name]\n",
        "best_accuracy = model_results[best_model_name]['accuracy']\n",
        "\n",
        "print(f\"ü•á Mejor modelo: {best_model_name} (Accuracy: {best_accuracy:.4f})\")\n",
        "\n",
        "# Mostrar reporte detallado del mejor modelo\n",
        "y_pred_best = model_results[best_model_name]['y_pred']\n",
        "print(f\"\\nüìä Reporte de clasificaci√≥n para {best_model_name}:\")\n",
        "print(classification_report(y_test, y_pred_best))\n",
        "\n",
        "print(f\"\\nüî• Matriz de confusi√≥n:\")\n",
        "print(confusion_matrix(y_test, y_pred_best))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Guardar y registrar el modelo en Azure ML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear directorio para modelos\n",
        "os.makedirs(\"model_artifacts\", exist_ok=True)\n",
        "\n",
        "# Guardar el mejor modelo\n",
        "model_name = f\"resume_classifier_{best_model_name.lower()}\"\n",
        "model_path = f\"model_artifacts/{model_name}.pkl\"\n",
        "\n",
        "# Guardar con joblib (m√°s eficiente para scikit-learn)\n",
        "joblib.dump(best_model, model_path)\n",
        "\n",
        "# Tambi√©n guardar informaci√≥n adicional\n",
        "model_info = {\n",
        "    'model_type': best_model_name,\n",
        "    'accuracy': best_accuracy,\n",
        "    'feature_columns': feature_columns,\n",
        "    'numeric_features': numeric_features,\n",
        "    'categorical_features': categorical_features,\n",
        "    'classes': best_model.classes_.tolist(),\n",
        "    'training_date': datetime.now().isoformat(),\n",
        "    'data_shape': df_processed.shape\n",
        "}\n",
        "\n",
        "import json\n",
        "with open(f\"model_artifacts/{model_name}_info.json\", 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(f\"üíæ Modelo guardado en: {model_path}\")\n",
        "print(f\"üìã Informaci√≥n del modelo guardada en: {model_name}_info.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Registrar modelo en Azure ML\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "model_azure = Model(\n",
        "    path=\"model_artifacts\",\n",
        "    type=AssetTypes.CUSTOM_MODEL,\n",
        "    name=model_name,\n",
        "    description=f\"Resume classification model using {best_model_name} - Accuracy: {best_accuracy:.4f}\",\n",
        "    version=\"1\",\n",
        "    tags={\n",
        "        \"algorithm\": best_model_name,\n",
        "        \"accuracy\": str(best_accuracy),\n",
        "        \"data_type\": \"resume_classification\",\n",
        "        \"framework\": \"scikit-learn\"\n",
        "    }\n",
        ")\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model_azure)\n",
        "print(f\"‚úÖ Modelo registrado en Azure ML: {registered_model.name} v{registered_model.version}\")\n",
        "print(f\"üéØ Accuracy del modelo: {best_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Preparar datos para RAI Dashboard\n",
        "\n",
        "Ahora vamos a preparar los datos en el formato requerido para el dashboard de IA Responsable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datasets para RAI (deben incluir solo las features que usa el modelo)\n",
        "# RAI necesita los datos en el formato original con las mismas columnas que el modelo espera\n",
        "\n",
        "# Crear datasets con las features que usa el modelo\n",
        "df_train_rai = pd.concat([X_train, y_train], axis=1)\n",
        "df_test_rai = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "print(f\"üìä Dataset RAI Train: {df_train_rai.shape}\")\n",
        "print(f\"üìä Dataset RAI Test: {df_test_rai.shape}\")\n",
        "print(f\"üéØ Columna target: 'apto'\")\n",
        "\n",
        "# Verificar que no hay valores faltantes problem√°ticos\n",
        "print(f\"\\nüîç Valores faltantes en train: {df_train_rai.isnull().sum().sum()}\")\n",
        "print(f\"üîç Valores faltantes en test: {df_test_rai.isnull().sum().sum()}\")\n",
        "\n",
        "# Mostrar preview\n",
        "print(\"\\nüìã Preview RAI Train data:\")\n",
        "display(df_train_rai.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear directorios y convertir a Parquet (requerido para RAI)\n",
        "os.makedirs(\"rai-train-data\", exist_ok=True)\n",
        "os.makedirs(\"rai-test-data\", exist_ok=True)\n",
        "\n",
        "# Convertir a Parquet\n",
        "table_train = pa.Table.from_pandas(df_train_rai)\n",
        "table_test = pa.Table.from_pandas(df_test_rai)\n",
        "\n",
        "pq.write_table(table_train, \"rai-train-data/data.parquet\", version=\"1.0\")\n",
        "pq.write_table(table_test, \"rai-test-data/data.parquet\", version=\"1.0\")\n",
        "\n",
        "print(\"‚úÖ Datos convertidos a formato Parquet para RAI\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear archivos MLTable (requeridos para RAI)\n",
        "\n",
        "# MLTable para datos de entrenamiento\n",
        "mltable_train = '''\n",
        "type: mltable\n",
        "paths:\n",
        "  - pattern: ./*.parquet\n",
        "transformations:\n",
        "  - read_parquet\n",
        "'''\n",
        "\n",
        "with open(\"rai-train-data/MLTable\", \"w\") as f:\n",
        "    f.write(mltable_train)\n",
        "\n",
        "# MLTable para datos de test\n",
        "mltable_test = '''\n",
        "type: mltable\n",
        "paths:\n",
        "  - pattern: ./*.parquet\n",
        "transformations:\n",
        "  - read_parquet\n",
        "'''\n",
        "\n",
        "with open(\"rai-test-data/MLTable\", \"w\") as f:\n",
        "    f.write(mltable_test)\n",
        "\n",
        "print(\"‚úÖ Archivos MLTable creados para RAI\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Registrar datasets para RAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nombres para los datasets\n",
        "train_data_name = f\"{model_name}_train_rai\"\n",
        "test_data_name = f\"{model_name}_test_rai\"\n",
        "data_version = \"1\"\n",
        "\n",
        "# Registrar datos de entrenamiento\n",
        "train_data = Data(\n",
        "    path=\"rai-train-data/\",\n",
        "    type=AssetTypes.MLTABLE,\n",
        "    description=f\"RAI training data para {model_name}\",\n",
        "    name=train_data_name,\n",
        "    version=data_version,\n",
        ")\n",
        "ml_client.data.create_or_update(train_data)\n",
        "\n",
        "# Registrar datos de test\n",
        "test_data = Data(\n",
        "    path=\"rai-test-data/\",\n",
        "    type=AssetTypes.MLTABLE,\n",
        "    description=f\"RAI test data para {model_name}\",\n",
        "    name=test_data_name,\n",
        "    version=data_version,\n",
        ")\n",
        "ml_client.data.create_or_update(test_data)\n",
        "\n",
        "print(f\"‚úÖ Datasets RAI registrados:\")\n",
        "print(f\"  üìä Train: {train_data_name}\")\n",
        "print(f\"  üìä Test: {test_data_name}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Configurar componentes RAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conectar al registro de Azure ML para obtener componentes RAI\n",
        "registry_name = \"azureml\"\n",
        "ml_client_registry = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=ml_client.subscription_id,\n",
        "    resource_group_name=ml_client.resource_group_name,\n",
        "    registry_name=registry_name,\n",
        ")\n",
        "\n",
        "# Obtener componentes RAI\n",
        "label = \"latest\"\n",
        "\n",
        "rai_constructor_component = ml_client_registry.components.get(\n",
        "    name=\"rai_tabular_insight_constructor\", label=label\n",
        ")\n",
        "\n",
        "version = rai_constructor_component.version\n",
        "print(f\"üì¶ Versi√≥n de componentes RAI: {version}\")\n",
        "\n",
        "rai_erroranalysis_component = ml_client_registry.components.get(\n",
        "    name=\"rai_tabular_erroranalysis\", version=version\n",
        ")\n",
        "\n",
        "rai_explanation_component = ml_client_registry.components.get(\n",
        "    name=\"rai_tabular_explanation\", version=version\n",
        ")\n",
        "\n",
        "rai_gather_component = ml_client_registry.components.get(\n",
        "    name=\"rai_tabular_insight_gather\", version=version\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Componentes RAI obtenidos\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Crear el pipeline RAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input, dsl, Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "import uuid\n",
        "\n",
        "# Configuraci√≥n del modelo\n",
        "expected_model_id = f\"{model_name}:1\"\n",
        "azureml_model_id = f\"azureml:{expected_model_id}\"\n",
        "\n",
        "# CAMBIAR aqu√≠ el nombre de tu cluster de c√≥mputo\n",
        "compute_name = \"aml-cluster\"  # Cambia por tu cluster\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=compute_name,\n",
        "    description=f\"RAI dashboard para {model_name}\",\n",
        "    experiment_name=f\"RAI_insights_{model_name}\",\n",
        ")\n",
        "def rai_pipeline_resume_classifier(target_column_name, train_data, test_data):\n",
        "    # Construir RAI dashboard\n",
        "    create_rai_job = rai_constructor_component(\n",
        "        title=f\"RAI Dashboard - Resume Classifier ({best_model_name})\",\n",
        "        task_type=\"classification\",\n",
        "        model_info=expected_model_id,\n",
        "        model_input=Input(type=AssetTypes.CUSTOM_MODEL, path=azureml_model_id),\n",
        "        train_dataset=train_data,\n",
        "        test_dataset=test_data,\n",
        "        target_column_name=target_column_name,\n",
        "    )\n",
        "    create_rai_job.set_limits(timeout=300)\n",
        "\n",
        "    # Agregar an√°lisis de errores\n",
        "    error_job = rai_erroranalysis_component(\n",
        "        rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
        "    )\n",
        "    error_job.set_limits(timeout=300)\n",
        "\n",
        "    # Agregar explicaciones del modelo\n",
        "    explanation_job = rai_explanation_component(\n",
        "        rai_insights_dashboard=create_rai_job.outputs.rai_insights_dashboard,\n",
        "        comment=f\"Explicaciones para clasificador de curr√≠culums - {best_model_name}\", \n",
        "    )\n",
        "    explanation_job.set_limits(timeout=300)\n",
        "\n",
        "    # Combinar todos los insights\n",
        "    rai_gather_job = rai_gather_component(\n",
        "        constructor=create_rai_job.outputs.rai_insights_dashboard,\n",
        "        insight_3=error_job.outputs.error_analysis,\n",
        "        insight_4=explanation_job.outputs.explanation,\n",
        "    )\n",
        "    rai_gather_job.set_limits(timeout=300)\n",
        "\n",
        "    rai_gather_job.outputs.dashboard.mode = \"upload\"\n",
        "\n",
        "    return {\n",
        "        \"dashboard\": rai_gather_job.outputs.dashboard,\n",
        "    }\n",
        "\n",
        "print(f\"‚úÖ Pipeline RAI definido para {model_name}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. Ejecutar el pipeline RAI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input\n",
        "\n",
        "# Preparar inputs del pipeline\n",
        "resume_train_input = Input(\n",
        "    type=\"mltable\",\n",
        "    path=f\"azureml:{train_data_name}:{data_version}\",\n",
        "    mode=\"download\",\n",
        ")\n",
        "\n",
        "resume_test_input = Input(\n",
        "    type=\"mltable\",\n",
        "    path=f\"azureml:{test_data_name}:{data_version}\",\n",
        "    mode=\"download\",\n",
        ")\n",
        "\n",
        "# Crear instancia del pipeline\n",
        "insights_pipeline_job = rai_pipeline_resume_classifier(\n",
        "    target_column_name=\"apto\",\n",
        "    train_data=resume_train_input,\n",
        "    test_data=resume_test_input,\n",
        ")\n",
        "\n",
        "# Configurar output\n",
        "rand_path = str(uuid.uuid4())\n",
        "insights_pipeline_job.outputs.dashboard = Output(\n",
        "    path=f\"azureml://datastores/workspaceblobstore/paths/{rand_path}/dashboard/\",\n",
        "    mode=\"upload\",\n",
        "    type=\"uri_folder\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Pipeline RAI configurado y listo para ejecutar\")\n",
        "print(f\"üéØ Modelo: {model_name} ({best_model_name})\")\n",
        "print(f\"üìä Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"üéØ Target: apto\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import PipelineJob\n",
        "from IPython.core.display import HTML\n",
        "from IPython.display import display\n",
        "import time\n",
        "\n",
        "def submit_and_wait(ml_client, pipeline_job) -> PipelineJob:\n",
        "    created_job = ml_client.jobs.create_or_update(pipeline_job)\n",
        "    assert created_job is not None\n",
        "\n",
        "    print(\"üöÄ Pipeline RAI enviado. Puedes seguir el progreso en:\")\n",
        "    display(HTML(f'<a href=\"{created_job.studio_url}\" target=\"_blank\">{created_job.studio_url}</a>'))\n",
        "\n",
        "    while created_job.status not in [\n",
        "        \"Completed\",\n",
        "        \"Failed\",\n",
        "        \"Canceled\",\n",
        "        \"NotResponding\",\n",
        "    ]:\n",
        "        time.sleep(30)\n",
        "        created_job = ml_client.jobs.get(created_job.name)\n",
        "        print(f\"üìä Estado actual: {created_job.status}\")\n",
        "        \n",
        "    if created_job.status == \"Completed\":\n",
        "        print(\"‚úÖ Pipeline RAI completado exitosamente!\")\n",
        "        print(\"üéØ Ve al Azure ML Studio para ver tu dashboard RAI\")\n",
        "        print(\"üìç Ubicaci√≥n: Models > Responsible AI\")\n",
        "    else:\n",
        "        print(f\"‚ùå Pipeline termin√≥ con estado: {created_job.status}\")\n",
        "        \n",
        "    return created_job\n",
        "\n",
        "# EJECUTAR EL PIPELINE RAI\n",
        "print(f\"üéØ Ejecutando RAI para el modelo: {model_name}\")\n",
        "print(f\"ü§ñ Algoritmo: {best_model_name}\")\n",
        "print(f\"üìä Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"üéØ Target: apto (clasificaci√≥n de aptitud de candidatos)\")\n",
        "print(\"\\n‚è≥ Esto puede tomar varios minutos...\")\n",
        "\n",
        "insights_job = submit_and_wait(ml_client, insights_pipeline_job)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéâ ¬°Proceso Completado!\n",
        "\n",
        "### ‚úÖ Lo que hemos logrado:\n",
        "\n",
        "1. **üìä An√°lisis de datos** de curr√≠culums de ingenieros\n",
        "2. **ü§ñ Entrenamiento de m√∫ltiples modelos** (RandomForest, GradientBoosting, LogisticRegression, SVM)\n",
        "3. **üèÜ Selecci√≥n del mejor modelo** basado en accuracy\n",
        "4. **üíæ Registro del modelo** en Azure ML Studio\n",
        "5. **üõ°Ô∏è Creaci√≥n del dashboard RAI** para an√°lisis de IA Responsable\n",
        "\n",
        "### üìà Tu modelo entrenado:\n",
        "- **Algoritmo**: El mejor modelo ser√° seleccionado autom√°ticamente\n",
        "- **Task**: Clasificaci√≥n de aptitud de candidatos (apto: -1, 0, 1)\n",
        "- **Features**: Caracter√≠sticas de experiencia, educaci√≥n, habilidades, etc.\n",
        "\n",
        "### üîç Dashboard de IA Responsable incluye:\n",
        "\n",
        "1. **üìà Error Analysis**: Identifica patrones en los errores del modelo\n",
        "2. **üîç Model Explanations**: Muestra qu√© features son m√°s importantes\n",
        "3. **‚öñÔ∏è Fairness Assessment**: Eval√∫a sesgos potenciales\n",
        "4. **üìä Performance Metrics**: M√©tricas detalladas por segmentos\n",
        "\n",
        "### üéØ Para acceder al dashboard:\n",
        "1. Ve a **Azure ML Studio**\n",
        "2. Navega a **Models** > **Responsible AI**\n",
        "3. Busca tu dashboard: `RAI Dashboard - Resume Classifier`\n",
        "\n",
        "### üí° Pr√≥ximos pasos:\n",
        "- **Analiza los insights** del dashboard RAI\n",
        "- **Identifica sesgos** potenciales en las predicciones\n",
        "- **Mejora el modelo** basado en los hallazgos\n",
        "- **Implementa controles** para IA Responsable en producci√≥n\n",
        "\n",
        "### üîÑ Para entrenar con otros algoritmos:\n",
        "Modifica la secci√≥n de modelos y vuelve a ejecutar desde la celda de entrenamiento.\n",
        "\n",
        "### ‚ö†Ô∏è Notas importantes:\n",
        "- El modelo ser√° registrado autom√°ticamente con el mejor accuracy\n",
        "- Los datos est√°n preparados en formato MLTable para RAI\n",
        "- El pipeline puede tardar 15-30 minutos en completarse\n",
        "- Aseg√∫rate de que tu cluster de c√≥mputo est√© disponible\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
