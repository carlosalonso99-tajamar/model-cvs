{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAMIENTO DE MODELO PARA SELECCI√ìN DE CANDIDATOS - AZURE ML STUDIO\n",
    "# =====================================================================\n",
    "\n",
    "# 1. CONFIGURACI√ìN Y CONEXI√ìN A AZURE ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Azure ML imports\n",
    "from azureml.core import Workspace, Dataset, Experiment, Run, Model\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.environment import Environment\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "print(\"üöÄ ENTRENAMIENTO DE MODELO DE SELECCI√ìN DE CANDIDATOS - AZURE ML\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Conectar al workspace\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(f\"‚úÖ Conectado al workspace: {ws.name}\")\n",
    "    print(f\"üìç Suscripci√≥n: {ws.subscription_id}\")\n",
    "    print(f\"üè¢ Grupo de recursos: {ws.resource_group}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error conectando al workspace: {e}\")\n",
    "    print(\"Aseg√∫rate de tener un archivo config.json o usar az login\")\n",
    "    raise\n",
    "\n",
    "# Obtener el contexto del experimento\n",
    "run = Run.get_context()\n",
    "experiment_name = \"candidate-selection-training\"\n",
    "\n",
    "# Si estamos en un experimento real, usar el run actual; si no, crear uno nuevo\n",
    "if hasattr(run, 'experiment'):\n",
    "    print(f\"‚úÖ Usando run existente: {run.id}\")\n",
    "else:\n",
    "    experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "    run = experiment.start_logging()\n",
    "    print(f\"üÜï Nuevo experimento creado: {experiment_name}\")\n",
    "\n",
    "# Habilitar MLflow tracking\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "mlflow.set_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CARGA DE DATOS DESDE AZURE ML DATASET\n",
    "print(\"\\nüìä CARGA DE DATOS DESDE AZURE ML\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configuraci√≥n del dataset\n",
    "DATASET_NAME = \"engineers-dataset-training\"\n",
    "DATASET_VERSION = \"latest\"\n",
    "\n",
    "try:\n",
    "    # Intentar cargar dataset registrado\n",
    "    dataset = Dataset.get_by_name(workspace=ws, name=DATASET_NAME, version=DATASET_VERSION)\n",
    "    df = dataset.to_pandas_dataframe()\n",
    "    print(f\"‚úÖ Dataset cargado desde Azure ML: {DATASET_NAME}\")\n",
    "    print(f\"üìä Forma del dataset: {df.shape}\")\n",
    "    \n",
    "    # Registrar informaci√≥n del dataset en el run\n",
    "    run.log(\"dataset_name\", DATASET_NAME)\n",
    "    run.log(\"dataset_version\", dataset.version)\n",
    "    run.log(\"dataset_size\", len(df))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  No se pudo cargar dataset registrado: {e}\")\n",
    "    print(\"üîÑ Intentando cargar desde datastore...\")\n",
    "    \n",
    "    try:\n",
    "        # Fallback: cargar desde datastore\n",
    "        datastore = ws.get_default_datastore()\n",
    "        dataset_path = \"processed/engineers_dataset_for_training.parquet\"\n",
    "        \n",
    "        # Crear dataset desde archivo\n",
    "        dataset = Dataset.Tabular.from_parquet_files(\n",
    "            path=[(datastore, dataset_path)],\n",
    "            validate=True\n",
    "        )\n",
    "        \n",
    "        df = dataset.to_pandas_dataframe()\n",
    "        print(f\"‚úÖ Dataset cargado desde datastore: {dataset_path}\")\n",
    "        print(f\"üìä Forma del dataset: {df.shape}\")\n",
    "        \n",
    "        # Registrar el dataset para uso futuro\n",
    "        dataset = dataset.register(\n",
    "            workspace=ws,\n",
    "            name=DATASET_NAME,\n",
    "            description=\"Dataset procesado para entrenamiento de selecci√≥n de candidatos\",\n",
    "            tags={\"tipo\": \"entrenamiento\", \"procesado\": \"True\"}\n",
    "        )\n",
    "        print(f\"üìù Dataset registrado como: {DATASET_NAME}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Error cargando datos: {e2}\")\n",
    "        print(\"üí° Sugerencia: Aseg√∫rate de que el archivo est√© en el datastore\")\n",
    "        print(\"   o crea el dataset manualmente en Azure ML Studio\")\n",
    "        \n",
    "        # Como √∫ltimo recurso, crear datos sint√©ticos para pruebas\n",
    "        print(\"üß™ Creando datos sint√©ticos para pruebas...\")\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'years_total_experience': np.random.randint(0, 20, n_samples),\n",
    "            'years_skill_main': np.random.randint(0, 10, n_samples),\n",
    "            'education_level': np.random.choice(['Bachelor', 'Master', 'PhD'], n_samples),\n",
    "            'num_certifications': np.random.randint(0, 5, n_samples),\n",
    "            'english_level': np.random.randint(0, 6, n_samples),\n",
    "            'num_languages': np.random.randint(1, 4, n_samples),\n",
    "            'apto': np.random.choice([0, 1], n_samples, p=[0.4, 0.6])\n",
    "        })\n",
    "        print(f\"üìä Datos sint√©ticos creados: {df.shape}\")\n",
    "\n",
    "# Verificar que tenemos el campo target 'apto'\n",
    "if 'apto' not in df.columns:\n",
    "    print(\"‚ùå ERROR: Campo 'apto' no encontrado en el dataset\")\n",
    "    print(\"Columnas disponibles:\", list(df.columns))\n",
    "    raise ValueError(\"Campo 'apto' requerido para entrenamiento\")\n",
    "\n",
    "print(f\"‚úÖ Dataset preparado con {len(df)} registros\")\n",
    "print(f\"üìã Columnas: {list(df.columns)}\")\n",
    "\n",
    "# Registrar metadatos del dataset\n",
    "run.log(\"total_records\", len(df))\n",
    "run.log(\"total_features\", len(df.columns) - 1)  # Excluir target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. AN√ÅLISIS EXPLORATORIO Y PREPARACI√ìN DE DATOS\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "print(\"\\nüîç AN√ÅLISIS EXPLORATORIO DEL TARGET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# An√°lisis del target\n",
    "target_counts = df['apto'].value_counts().sort_index()\n",
    "target_labels = {1: 'Aptos', 0: 'No aptos', -1: 'Revisi√≥n manual'}\n",
    "\n",
    "print(\"üìä Distribuci√≥n del target:\")\n",
    "total = len(df)\n",
    "class_distribution = {}\n",
    "for value, count in target_counts.items():\n",
    "    label = target_labels.get(value, f'Valor {value}')\n",
    "    percentage = count / total * 100\n",
    "    class_distribution[label] = {\"count\": count, \"percentage\": percentage}\n",
    "    print(f\"  {label}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Registrar en Azure ML\n",
    "    run.log(f\"class_{value}_count\", count)\n",
    "    run.log(f\"class_{value}_percentage\", percentage)\n",
    "\n",
    "# Verificar balance de clases\n",
    "if len(target_counts) > 1:\n",
    "    balance_ratio = target_counts.min() / target_counts.max()\n",
    "    run.log(\"class_balance_ratio\", balance_ratio)\n",
    "    print(f\"\\n‚öñÔ∏è  Balance de clases: {balance_ratio:.2f}\")\n",
    "    \n",
    "    if balance_ratio < 0.3:\n",
    "        print(\"‚ö†Ô∏è  DATASET DESBALANCEADO - Se aplicar√°n t√©cnicas de balanceo\")\n",
    "        run.log(\"dataset_imbalanced\", True)\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset relativamente balanceado\")\n",
    "        run.log(\"dataset_imbalanced\", False)\n",
    "\n",
    "print(\"\\nüîß PREPARACI√ìN DE CARACTER√çSTICAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Identificar tipos de columnas autom√°ticamente\n",
    "def identify_and_process_columns(df):\n",
    "    \"\"\"Identifica y procesa autom√°ticamente diferentes tipos de columnas\"\"\"\n",
    "    \n",
    "    numeric_cols = []\n",
    "    categorical_cols = []\n",
    "    processed_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in ['apto', 'ID']:  # Excluir target y ID\n",
    "            continue\n",
    "            \n",
    "        # Verificar si es num√©rico\n",
    "        if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            numeric_cols.append(col)\n",
    "        # Verificar si contiene datos complejos (listas, dicts como strings)\n",
    "        elif col in ['skills', 'languages', 'certifications']:\n",
    "            print(f\"üîÑ Procesando columna compleja: {col}\")\n",
    "            # Procesar seg√∫n el tipo de datos esperado\n",
    "            if col == 'skills':\n",
    "                # Convertir a features binarias\n",
    "                skills_features = process_skills_for_aml(df[col])\n",
    "                if skills_features is not None:\n",
    "                    processed_features.append(skills_features)\n",
    "            elif col == 'languages':\n",
    "                # Convertir a features num√©ricas\n",
    "                lang_features = process_languages_for_aml(df[col])\n",
    "                if lang_features is not None:\n",
    "                    processed_features.append(lang_features)\n",
    "            elif col == 'certifications':\n",
    "                # Contar certificaciones\n",
    "                cert_features = pd.DataFrame({\n",
    "                    'num_certifications': df[col].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "                }, index=df.index)\n",
    "                processed_features.append(cert_features)\n",
    "        else:\n",
    "            # Resto son categ√≥ricas\n",
    "            categorical_cols.append(col)\n",
    "    \n",
    "    return numeric_cols, categorical_cols, processed_features\n",
    "\n",
    "def process_skills_for_aml(skills_series):\n",
    "    \"\"\"Procesa columna de skills para Azure ML\"\"\"\n",
    "    try:\n",
    "        # Si son listas, convertir a texto\n",
    "        if skills_series.apply(lambda x: isinstance(x, list)).any():\n",
    "            skills_text = skills_series.apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "        else:\n",
    "            skills_text = skills_series.astype(str)\n",
    "        \n",
    "        # Crear features binarias de skills m√°s comunes\n",
    "        vectorizer = CountVectorizer(binary=True, max_features=20, min_df=2)\n",
    "        skills_matrix = vectorizer.fit_transform(skills_text)\n",
    "        \n",
    "        feature_names = [f\"skill_{name}\" for name in vectorizer.get_feature_names_out()]\n",
    "        return pd.DataFrame(skills_matrix.toarray(), columns=feature_names, index=skills_series.index)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error procesando skills: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_languages_for_aml(languages_series):\n",
    "    \"\"\"Procesa columna de idiomas para Azure ML\"\"\"\n",
    "    try:\n",
    "        level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6, 'NATIVE': 6}\n",
    "        \n",
    "        language_features = []\n",
    "        for langs in languages_series:\n",
    "            if isinstance(langs, dict):\n",
    "                features = {\n",
    "                    'num_languages': len(langs),\n",
    "                    'english_level': level_mapping.get(langs.get('English', ''), 0),\n",
    "                    'has_spanish': 1 if 'Spanish' in langs else 0,\n",
    "                    'max_language_level': max([level_mapping.get(level, 0) for level in langs.values()], default=0)\n",
    "                }\n",
    "            else:\n",
    "                features = {'num_languages': 0, 'english_level': 0, 'has_spanish': 0, 'max_language_level': 0}\n",
    "            language_features.append(features)\n",
    "        \n",
    "        return pd.DataFrame(language_features, index=languages_series.index)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error procesando idiomas: {e}\")\n",
    "        return None\n",
    "\n",
    "# Procesar columnas\n",
    "numeric_cols, categorical_cols, processed_features = identify_and_process_columns(df)\n",
    "\n",
    "print(f\"üìä Columnas num√©ricas ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"üè∑Ô∏è  Columnas categ√≥ricas ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"üîß Features procesadas: {len(processed_features)} grupos\")\n",
    "\n",
    "# Registrar informaci√≥n de features\n",
    "run.log(\"numeric_features_count\", len(numeric_cols))\n",
    "run.log(\"categorical_features_count\", len(categorical_cols))\n",
    "run.log(\"processed_features_count\", len(processed_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CONSTRUCCI√ìN DEL DATASET FINAL Y DIVISI√ìN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\nüîó CONSTRUCCI√ìN DEL DATASET FINAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combinar todas las features\n",
    "feature_dfs = []\n",
    "\n",
    "# Agregar columnas num√©ricas\n",
    "if numeric_cols:\n",
    "    numeric_df = df[numeric_cols].fillna(0)\n",
    "    feature_dfs.append(numeric_df)\n",
    "    print(f\"‚úÖ {len(numeric_cols)} columnas num√©ricas agregadas\")\n",
    "\n",
    "# Agregar columnas categ√≥ricas (one-hot encoding)\n",
    "if categorical_cols:\n",
    "    valid_categorical = []\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals <= 10:  # M√°ximo 10 categor√≠as para evitar dispersi√≥n\n",
    "                valid_categorical.append(col)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Saltando '{col}': demasiadas categor√≠as ({unique_vals})\")\n",
    "    \n",
    "    if valid_categorical:\n",
    "        categorical_df = pd.get_dummies(df[valid_categorical], prefix=valid_categorical, dummy_na=True)\n",
    "        feature_dfs.append(categorical_df)\n",
    "        print(f\"‚úÖ {len(valid_categorical)} columnas categ√≥ricas procesadas -> {categorical_df.shape[1]} features\")\n",
    "\n",
    "# Agregar features procesadas\n",
    "feature_dfs.extend(processed_features)\n",
    "\n",
    "# Combinar todo\n",
    "if feature_dfs:\n",
    "    X = pd.concat(feature_dfs, axis=1)\n",
    "    y = df['apto'].copy()\n",
    "    \n",
    "    # Limpiar datos\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Eliminar features con varianza cero\n",
    "    non_zero_var_cols = X.columns[X.var() != 0]\n",
    "    if len(non_zero_var_cols) < len(X.columns):\n",
    "        removed_features = len(X.columns) - len(non_zero_var_cols)\n",
    "        X = X[non_zero_var_cols]\n",
    "        print(f\"üóëÔ∏è  Eliminadas {removed_features} features con varianza cero\")\n",
    "    \n",
    "    print(f\"\\nüéØ Dataset final: {X.shape}\")\n",
    "    \n",
    "    # Registrar informaci√≥n del dataset final\n",
    "    run.log(\"final_features_count\", X.shape[1])\n",
    "    run.log(\"final_samples_count\", X.shape[0])\n",
    "    \n",
    "    # Guardar nombres de features para uso posterior\n",
    "    feature_names = list(X.columns)\n",
    "    run.log_list(\"feature_names\", feature_names)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"‚ùå No se pudieron crear features\")\n",
    "\n",
    "print(\"\\nüìä DIVISI√ìN DEL DATASET\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Manejar clase de revisi√≥n manual (-1) si existe\n",
    "revision_count = (y == -1).sum()\n",
    "if revision_count > 0:\n",
    "    print(f\"üéØ Encontrados {revision_count} casos de revisi√≥n manual\")\n",
    "    print(\"üìã Excluyendo casos de revisi√≥n manual para simplificar el entrenamiento\")\n",
    "    \n",
    "    mask_no_revision = y != -1\n",
    "    X_clean = X[mask_no_revision].reset_index(drop=True)\n",
    "    y_clean = y[mask_no_revision].reset_index(drop=True)\n",
    "    \n",
    "    run.log(\"revision_cases_excluded\", revision_count)\n",
    "    print(f\"‚úÖ Dataset limpio: {X_clean.shape[0]} muestras\")\n",
    "else:\n",
    "    X_clean, y_clean = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "    run.log(\"revision_cases_excluded\", 0)\n",
    "\n",
    "# Divisi√≥n estratificada\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_clean, y_clean, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_clean\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.25,  # 0.25 * 0.8 = 0.2 del total\n",
    "    random_state=42, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"üìÇ Divisi√≥n del dataset:\")\n",
    "print(f\"  Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X_clean)*100:.1f}%)\")\n",
    "print(f\"  Validaci√≥n: {X_val.shape[0]} muestras ({X_val.shape[0]/len(X_clean)*100:.1f}%)\")\n",
    "print(f\"  Test: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X_clean)*100:.1f}%)\")\n",
    "\n",
    "# Registrar tama√±os de conjuntos\n",
    "run.log(\"train_size\", len(X_train))\n",
    "run.log(\"val_size\", len(X_val))\n",
    "run.log(\"test_size\", len(X_test))\n",
    "\n",
    "# Aplicar balanceo si es necesario\n",
    "train_counts = Counter(y_train)\n",
    "if len(train_counts) > 1:\n",
    "    minority_class = min(train_counts.values())\n",
    "    majority_class = max(train_counts.values())\n",
    "    imbalance_ratio = minority_class / majority_class\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è  Ratio de balance en entrenamiento: {imbalance_ratio:.2f}\")\n",
    "    run.log(\"train_balance_ratio\", imbalance_ratio)\n",
    "    \n",
    "    if imbalance_ratio < 0.7:  # Aplicar balanceo si est√° muy desbalanceado\n",
    "        print(\"üîÑ Aplicando SMOTE para balancear clases...\")\n",
    "        try:\n",
    "            smote = SMOTE(random_state=42, k_neighbors=min(5, minority_class-1))\n",
    "            X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "            \n",
    "            balanced_counts = Counter(y_train_balanced)\n",
    "            print(f\"‚úÖ SMOTE aplicado:\")\n",
    "            for class_val, count in sorted(balanced_counts.items()):\n",
    "                label = \"Aptos\" if class_val == 1 else \"No aptos\"\n",
    "                print(f\"    {label}: {count}\")\n",
    "            \n",
    "            run.log(\"smote_applied\", True)\n",
    "            run.log(\"balanced_train_size\", len(X_train_balanced))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error aplicando SMOTE: {e}\")\n",
    "            print(\"üìä Usando dataset original\")\n",
    "            X_train_balanced, y_train_balanced = X_train, y_train\n",
    "            run.log(\"smote_applied\", False)\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset relativamente balanceado - no se requiere balanceo\")\n",
    "        X_train_balanced, y_train_balanced = X_train, y_train\n",
    "        run.log(\"smote_applied\", False)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Solo una clase presente en entrenamiento\")\n",
    "    X_train_balanced, y_train_balanced = X_train, y_train\n",
    "    run.log(\"smote_applied\", False)\n",
    "\n",
    "print(f\"\\n‚úÖ Datos preparados para entrenamiento\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ENTRENAMIENTO DE MODELOS CON MLFLOW TRACKING\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "print(\"\\nü§ñ ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configuraci√≥n de modelos con hiperpar√°metros optimizados para Azure ML\n",
    "models_config = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'requires_scaling': False\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=6,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'requires_scaling': False\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            solver='liblinear'\n",
    "        ),\n",
    "        'requires_scaling': True\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf',\n",
    "            probability=True,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            gamma='scale'\n",
    "        ),\n",
    "        'requires_scaling': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Preparar datos escalados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar modelos y registrar en MLflow\n",
    "trained_models = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    print(f\"\\nüîÑ Entrenando {model_name}...\")\n",
    "    \n",
    "    # Iniciar run hijo para este modelo\n",
    "    with mlflow.start_run(nested=True, run_name=f\"model_{model_name}\") as child_run:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            model = config['model']\n",
    "            \n",
    "            # Registrar hiperpar√°metros\n",
    "            mlflow.log_params(model.get_params())\n",
    "            \n",
    "            # Seleccionar datos (escalados o no)\n",
    "            if config['requires_scaling']:\n",
    "                model.fit(X_train_scaled, y_train_balanced)\n",
    "                y_val_pred = model.predict(X_val_scaled)\n",
    "                y_val_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "            else:\n",
    "                model.fit(X_train_balanced, y_train_balanced)\n",
    "                y_val_pred = model.predict(X_val)\n",
    "                y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "            \n",
    "                         # Calcular m√©tricas\n",
    "             training_time = time.time() - start_time\n",
    "             accuracy = (y_val_pred == y_val).mean()\n",
    "            \n",
    "             try:\n",
    "                 auc_score = roc_auc_score(y_val, y_val_proba)\n",
    "             except:\n",
    "                 auc_score = 0.0\n",
    "            \n",
    "             # Reporte detallado\n",
    "             report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "            \n",
    "             # Registrar m√©tricas en MLflow\n",
    "             mlflow.log_metric(\"accuracy\", accuracy)\n",
    "             mlflow.log_metric(\"auc\", auc_score)\n",
    "             mlflow.log_metric(\"training_time\", training_time)\n",
    "             mlflow.log_metric(\"f1_macro\", report['macro avg']['f1-score'])\n",
    "             mlflow.log_metric(\"precision_macro\", report['macro avg']['precision'])\n",
    "             mlflow.log_metric(\"recall_macro\", report['macro avg']['recall'])\n",
    "            \n",
    "             # Registrar m√©tricas por clase\n",
    "             for class_label in ['0', '1']:\n",
    "                 if class_label in report:\n",
    "                     mlflow.log_metric(f\"precision_class_{class_label}\", report[class_label]['precision'])\n",
    "                     mlflow.log_metric(f\"recall_class_{class_label}\", report[class_label]['recall'])\n",
    "                     mlflow.log_metric(f\"f1_class_{class_label}\", report[class_label]['f1-score'])\n",
    "            \n",
    "             # Registrar modelo en MLflow\n",
    "             mlflow.sklearn.log_model(\n",
    "                 model, \n",
    "                 f\"model_{model_name}\",\n",
    "                 registered_model_name=f\"candidate_selection_{model_name}\"\n",
    "             )\n",
    "            \n",
    "             # Guardar para comparaci√≥n posterior\n",
    "             trained_models[model_name] = {\n",
    "                 'model': model,\n",
    "                 'requires_scaling': config['requires_scaling']\n",
    "             }\n",
    "            \n",
    "             model_results[model_name] = {\n",
    "                 'accuracy': accuracy,\n",
    "                 'auc': auc_score,\n",
    "                 'training_time': training_time,\n",
    "                 'f1_macro': report['macro avg']['f1-score'],\n",
    "                 'precision_macro': report['macro avg']['precision'],\n",
    "                 'recall_macro': report['macro avg']['recall'],\n",
    "                 'predictions': y_val_pred,\n",
    "                 'probabilities': y_val_proba\n",
    "             }\n",
    "            \n",
    "             print(f\"  ‚úÖ Completado en {training_time:.2f}s\")\n",
    "             print(f\"  üìä Accuracy: {accuracy:.3f}\")\n",
    "             print(f\"  üìà AUC: {auc_score:.3f}\")\n",
    "             print(f\"  üéØ F1-Score (macro): {report['macro avg']['f1-score']:.3f}\")\n",
    "            \n",
    "             # Registrar m√©tricas en el run principal tambi√©n\n",
    "             run.log(f\"{model_name}_accuracy\", accuracy)\n",
    "             run.log(f\"{model_name}_auc\", auc_score)\n",
    "             run.log(f\"{model_name}_f1_macro\", report['macro avg']['f1-score'])\n",
    "             run.log(f\"{model_name}_training_time\", training_time)\n",
    "            \n",
    "         except Exception as e:\n",
    "             print(f\"  ‚ùå Error entrenando {model_name}: {e}\")\n",
    "             mlflow.log_param(\"error\", str(e))\n",
    "             model_results[model_name] = None\n",
    "\n",
    "print(f\"\\n‚úÖ Entrenamiento completado\")\n",
    "successful_models = [name for name, result in model_results.items() if result is not None]\n",
    "print(f\"üìä {len(successful_models)} modelos entrenados exitosamente: {successful_models}\")\n",
    "\n",
    "# Registrar resumen en el run principal\n",
    "run.log(\"total_models_trained\", len(successful_models))\n",
    "run.log_list(\"successful_models\", successful_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. SELECCI√ìN DEL MEJOR MODELO Y EVALUACI√ìN FINAL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\nüèÜ SELECCI√ìN Y EVALUACI√ìN DEL MEJOR MODELO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(successful_models) > 0:\n",
    "    # Crear tabla de comparaci√≥n\n",
    "    results_df = pd.DataFrame({\n",
    "        name: result for name, result in model_results.items() \n",
    "        if result is not None\n",
    "    }).T\n",
    "    \n",
    "    # Mostrar tabla de resultados\n",
    "    print(\"\\nüìà TABLA DE RESULTADOS:\")\n",
    "    display_cols = ['accuracy', 'auc', 'f1_macro', 'precision_macro', 'recall_macro', 'training_time']\n",
    "    results_display = results_df[display_cols].round(3)\n",
    "    results_display.columns = ['Accuracy', 'AUC', 'F1-Macro', 'Precision', 'Recall', 'Tiempo(s)']\n",
    "    print(results_display.to_string())\n",
    "    \n",
    "    # Seleccionar mejor modelo basado en F1-score macro\n",
    "    best_model_name = results_df['f1_macro'].idxmax()\n",
    "    best_score = results_df.loc[best_model_name, 'f1_macro']\n",
    "    \n",
    "    print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
    "    print(f\"üéØ F1-Score (macro): {best_score:.3f}\")\n",
    "    print(f\"üìä Accuracy: {results_df.loc[best_model_name, 'accuracy']:.3f}\")\n",
    "    print(f\"üìà AUC: {results_df.loc[best_model_name, 'auc']:.3f}\")\n",
    "    \n",
    "    # Registrar mejor modelo en el run principal\n",
    "    run.log(\"best_model_name\", best_model_name)\n",
    "    run.log(\"best_model_f1_macro\", best_score)\n",
    "    run.log(\"best_model_accuracy\", results_df.loc[best_model_name, 'accuracy'])\n",
    "    run.log(\"best_model_auc\", results_df.loc[best_model_name, 'auc'])\n",
    "    \n",
    "    # Evaluaci√≥n en conjunto de test\n",
    "    best_model_info = trained_models[best_model_name]\n",
    "    best_model = best_model_info['model']\n",
    "    \n",
    "    # Usar datos escalados si es necesario\n",
    "    if best_model_info['requires_scaling']:\n",
    "        y_test_pred = best_model.predict(X_test_scaled)\n",
    "        y_test_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # M√©tricas en test\n",
    "    test_accuracy = (y_test_pred == y_test).mean()\n",
    "    test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "    \n",
    "    print(f\"\\nüß™ EVALUACI√ìN EN CONJUNTO DE TEST:\")\n",
    "    print(f\"üìä Accuracy en test: {test_accuracy:.3f}\")\n",
    "    print(f\"üìà AUC en test: {test_auc:.3f}\")\n",
    "    print(f\"üéØ F1-Score macro en test: {test_report['macro avg']['f1-score']:.3f}\")\n",
    "    \n",
    "    # Registrar m√©tricas de test\n",
    "    run.log(\"test_accuracy\", test_accuracy)\n",
    "    run.log(\"test_auc\", test_auc)\n",
    "    run.log(\"test_f1_macro\", test_report['macro avg']['f1-score'])\n",
    "    run.log(\"test_precision_macro\", test_report['macro avg']['precision'])\n",
    "    run.log(\"test_recall_macro\", test_report['macro avg']['recall'])\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(f\"\\nüî¢ Matriz de Confusi√≥n en Test:\")\n",
    "    print(f\"               Predicho\")\n",
    "    print(f\"Real    No Apto  Apto\")\n",
    "    print(f\"No Apto   {cm[0,0]:3d}    {cm[0,1]:3d}\")\n",
    "    print(f\"Apto      {cm[1,0]:3d}    {cm[1,1]:3d}\")\n",
    "    \n",
    "    # Registrar valores de la matriz de confusi√≥n\n",
    "    run.log(\"test_tn\", int(cm[0,0]))  # True Negatives\n",
    "    run.log(\"test_fp\", int(cm[0,1]))  # False Positives\n",
    "    run.log(\"test_fn\", int(cm[1,0]))  # False Negatives\n",
    "    run.log(\"test_tp\", int(cm[1,1]))  # True Positives\n",
    "    \n",
    "    # Crear visualizaci√≥n de la matriz de confusi√≥n\n",
    "    try:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title(f'Matriz de Confusi√≥n - {best_model_name}')\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # A√±adir etiquetas\n",
    "        tick_marks = np.arange(2)\n",
    "        plt.xticks(tick_marks, ['No Apto', 'Apto'])\n",
    "        plt.yticks(tick_marks, ['No Apto', 'Apto'])\n",
    "        \n",
    "        # A√±adir texto en cada celda\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in np.ndindex(cm.shape):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.ylabel('Etiqueta Real')\n",
    "        plt.xlabel('Etiqueta Predicha')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar como artifact en MLflow\n",
    "        confusion_matrix_path = \"outputs/confusion_matrix.png\"\n",
    "        plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')\n",
    "        mlflow.log_artifact(confusion_matrix_path)\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üìä Matriz de confusi√≥n guardada como artifact\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error creando visualizaci√≥n: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå ERROR: No hay modelos exitosos para evaluar\")\n",
    "    raise ValueError(\"No se entrenaron modelos exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. REGISTRO FINAL DEL MODELO EN AZURE ML Y PREPARACI√ìN PARA RAI\n",
    "import os\n",
    "\n",
    "print(\"\\nüíæ REGISTRO FINAL DEL MODELO EN AZURE ML\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear directorio de outputs si no existe\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Guardar el mejor modelo y artefactos necesarios\n",
    "model_path = \"outputs/model.pkl\"\n",
    "scaler_path = \"outputs/scaler.pkl\"\n",
    "feature_names_path = \"outputs/feature_names.json\"\n",
    "\n",
    "# Guardar modelo\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"‚úÖ Modelo guardado: {model_path}\")\n",
    "\n",
    "# Guardar scaler si es necesario\n",
    "if best_model_info['requires_scaling']:\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"‚úÖ Scaler guardado: {scaler_path}\")\n",
    "    run.log(\"requires_scaling\", True)\n",
    "else:\n",
    "    # Crear un scaler dummy para mantener consistencia\n",
    "    dummy_scaler = StandardScaler()\n",
    "    dummy_scaler.fit(np.zeros((1, X_train.shape[1])))\n",
    "    joblib.dump(dummy_scaler, scaler_path)\n",
    "    run.log(\"requires_scaling\", False)\n",
    "    print(f\"‚úÖ Scaler dummy guardado: {scaler_path}\")\n",
    "\n",
    "# Guardar nombres de features para interpretabilidad\n",
    "feature_metadata = {\n",
    "    \"feature_names\": feature_names,\n",
    "    \"feature_count\": len(feature_names),\n",
    "    \"numeric_features\": numeric_cols,\n",
    "    \"categorical_features\": categorical_cols,\n",
    "    \"processed_features_count\": len(processed_features)\n",
    "}\n",
    "\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "print(f\"‚úÖ Metadatos de features guardados: {feature_names_path}\")\n",
    "\n",
    "# Guardar datasets para evaluaci√≥n y RAI\n",
    "test_data_path = \"outputs/test_data.parquet\"\n",
    "val_data_path = \"outputs/val_data.parquet\"\n",
    "\n",
    "# Combinar features con targets para evaluaci√≥n posterior\n",
    "X_test_with_target = X_test.copy()\n",
    "X_test_with_target['y_true'] = y_test.reset_index(drop=True)\n",
    "X_test_with_target['y_pred'] = y_test_pred\n",
    "X_test_with_target['y_proba'] = y_test_proba\n",
    "\n",
    "X_val_with_target = X_val.copy()\n",
    "X_val_with_target['y_true'] = y_val.reset_index(drop=True)\n",
    "X_val_with_target['y_pred'] = model_results[best_model_name]['predictions']\n",
    "X_val_with_target['y_proba'] = model_results[best_model_name]['probabilities']\n",
    "\n",
    "# Guardar datasets\n",
    "X_test_with_target.to_parquet(test_data_path, index=False)\n",
    "X_val_with_target.to_parquet(val_data_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Datos de test guardados: {test_data_path}\")\n",
    "print(f\"‚úÖ Datos de validaci√≥n guardados: {val_data_path}\")\n",
    "\n",
    "# Registrar todos los artifacts en MLflow\n",
    "artifacts_to_log = [\n",
    "    model_path,\n",
    "    scaler_path, \n",
    "    feature_names_path,\n",
    "    test_data_path,\n",
    "    val_data_path\n",
    "]\n",
    "\n",
    "for artifact_path in artifacts_to_log:\n",
    "    if os.path.exists(artifact_path):\n",
    "        mlflow.log_artifact(artifact_path)\n",
    "        print(f\"üì§ Artifact registrado en MLflow: {artifact_path}\")\n",
    "\n",
    "# Registrar el mejor modelo en Azure ML Model Registry\n",
    "try:\n",
    "    # Registrar modelo con tags descriptivos\n",
    "    model_tags = {\n",
    "        \"model_type\": best_model_name,\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"task\": \"binary_classification\",\n",
    "        \"domain\": \"candidate_selection\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"performance_metric\": \"f1_macro\",\n",
    "        \"performance_value\": str(round(best_score, 3)),\n",
    "        \"test_accuracy\": str(round(test_accuracy, 3)),\n",
    "        \"test_auc\": str(round(test_auc, 3)),\n",
    "        \"requires_scaling\": str(best_model_info['requires_scaling'])\n",
    "    }\n",
    "    \n",
    "    model_description = f\"\"\"\n",
    "    Modelo de selecci√≥n de candidatos entrenado con {best_model_name}.\n",
    "    \n",
    "    M√©tricas de rendimiento:\n",
    "    - F1-Score (macro): {best_score:.3f}\n",
    "    - Accuracy en test: {test_accuracy:.3f}\n",
    "    - AUC en test: {test_auc:.3f}\n",
    "    \n",
    "    Caracter√≠sticas del modelo:\n",
    "    - N√∫mero de features: {len(feature_names)}\n",
    "    - Requiere escalado: {best_model_info['requires_scaling']}\n",
    "    - Datos de entrenamiento: {len(X_train_balanced)} muestras\n",
    "    - Datos de test: {len(X_test)} muestras\n",
    "    \"\"\"\n",
    "    \n",
    "    registered_model = Model.register(\n",
    "        workspace=ws,\n",
    "        model_path=\"outputs/\",  # Directorio con todos los artefactos\n",
    "        model_name=\"candidate-selection-model\",\n",
    "        description=model_description,\n",
    "        tags=model_tags\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Modelo registrado en Azure ML: {registered_model.name}\")\n",
    "    print(f\"üìã Versi√≥n del modelo: {registered_model.version}\")\n",
    "    print(f\"üîó ID del modelo: {registered_model.id}\")\n",
    "    \n",
    "    # Registrar informaci√≥n del modelo registrado\n",
    "    run.log(\"registered_model_name\", registered_model.name)\n",
    "    run.log(\"registered_model_version\", registered_model.version)\n",
    "    run.log(\"registered_model_id\", registered_model.id)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error registrando modelo en Azure ML: {e}\")\n",
    "    print(\"El modelo se guard√≥ localmente y en MLflow\")\n",
    "\n",
    "# Preparar informaci√≥n para notebooks posteriores\n",
    "notebook_info = {\n",
    "    \"experiment_name\": experiment_name,\n",
    "    \"run_id\": run.id,\n",
    "    \"best_model_name\": best_model_name,\n",
    "    \"model_requires_scaling\": best_model_info['requires_scaling'],\n",
    "    \"feature_names\": feature_names,\n",
    "    \"test_metrics\": {\n",
    "        \"accuracy\": test_accuracy,\n",
    "        \"auc\": test_auc,\n",
    "        \"f1_macro\": test_report['macro avg']['f1-score']\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(X_train_balanced),\n",
    "        \"val_size\": len(X_val),\n",
    "        \"test_size\": len(X_test),\n",
    "        \"feature_count\": len(feature_names)\n",
    "    }\n",
    "}\n",
    "\n",
    "notebook_info_path = \"outputs/notebook_info.json\"\n",
    "with open(notebook_info_path, 'w') as f:\n",
    "    json.dump(notebook_info, f, indent=2)\n",
    "\n",
    "mlflow.log_artifact(notebook_info_path)\n",
    "print(f\"‚úÖ Informaci√≥n para notebooks posteriores guardada: {notebook_info_path}\")\n",
    "\n",
    "print(f\"\\nüéâ ENTRENAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üèÜ Mejor modelo: {best_model_name}\")\n",
    "print(f\"üìä F1-Score (macro): {best_score:.3f}\")\n",
    "print(f\"üß™ Accuracy en test: {test_accuracy:.3f}\")\n",
    "print(f\"üìà AUC en test: {test_auc:.3f}\")\n",
    "print(f\"üíæ Modelo registrado en Azure ML\")\n",
    "print(f\"üìã Run ID: {run.id}\")\n",
    "\n",
    "print(f\"\\nüìã PR√ìXIMOS PASOS:\")\n",
    "print(\"1. ‚úÖ Ejecutar notebook 03_evaluation.ipynb para an√°lisis detallado\")\n",
    "print(\"2. ‚úÖ Ejecutar notebook 04_rai_dashboard.ipynb para an√°lisis de responsabilidad\")\n",
    "print(\"3. üöÄ Desplegar modelo como endpoint en Azure ML\")\n",
    "print(\"4. üîÑ Configurar pipeline de inferencia\")\n",
    "\n",
    "# Completar el run principal\n",
    "run.complete()\n",
    "print(f\"\\n‚úÖ Experimento completado: {experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFIGURACI√ìN Y CARGA DE DATOS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de rutas\n",
    "DATA_PATH = Path(\"../data/processed/\")\n",
    "MODELS_PATH = Path(\"../models/\")\n",
    "MODELS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üöÄ ENTRENAMIENTO DE MODELO DE SELECCI√ìN DE CANDIDATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cargar dataset procesado\n",
    "dataset_path = DATA_PATH / \"engineers_dataset_for_training.parquet\"\n",
    "if dataset_path.exists():\n",
    "    df = pd.read_parquet(dataset_path)\n",
    "    print(f\"‚úÖ Dataset cargado: {df.shape}\")\n",
    "    print(f\"üìä Columnas: {list(df.columns)}\")\n",
    "else:\n",
    "    print(f\"‚ùå ERROR: No se encuentra {dataset_path}\")\n",
    "    print(\"Ejecuta primero el notebook 01_data_preparation.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. AN√ÅLISIS EXPLORATORIO DEL TARGET\n",
    "if 'apto' in df.columns:\n",
    "    print(\"\\nüìà AN√ÅLISIS DEL CAMPO TARGET 'apto':\")\n",
    "    \n",
    "    # Distribuci√≥n del target\n",
    "    target_counts = df['apto'].value_counts().sort_index()\n",
    "    target_labels = {1: 'Aptos', 0: 'No aptos', -1: 'Revisi√≥n manual'}\n",
    "    \n",
    "    print(f\"Distribuci√≥n del target:\")\n",
    "    total = len(df)\n",
    "    for value, count in target_counts.items():\n",
    "        label = target_labels.get(value, f'Valor {value}')\n",
    "        percentage = count / total * 100\n",
    "        print(f\"  {label}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Verificar balance de clases\n",
    "    if len(target_counts) > 1:\n",
    "        balance_ratio = target_counts.min() / target_counts.max()\n",
    "        print(f\"\\n‚öñÔ∏è  Balance de clases: {balance_ratio:.2f}\")\n",
    "        if balance_ratio < 0.3:\n",
    "            print(\"‚ö†Ô∏è  DATASET DESBALANCEADO - Considerar t√©cnicas de balanceo\")\n",
    "        else:\n",
    "            print(\"‚úÖ Dataset relativamente balanceado\")\n",
    "    \n",
    "    # Mostrar estad√≠sticas por target\n",
    "    print(f\"\\nüìä ESTAD√çSTICAS POR GRUPO:\")\n",
    "    numeric_cols = ['years_total_experience', 'years_skill_main', 'num_promotions']\n",
    "    available_numeric = [col for col in numeric_cols if col in df.columns]\n",
    "    \n",
    "    if available_numeric:\n",
    "        for target_value, label in target_labels.items():\n",
    "            if target_value in target_counts.index:\n",
    "                subset = df[df['apto'] == target_value]\n",
    "                print(f\"\\n{label} (n={len(subset)}):\")\n",
    "                for col in available_numeric:\n",
    "                    if not subset[col].isna().all():\n",
    "                        mean_val = subset[col].mean()\n",
    "                        print(f\"  {col}: {mean_val:.1f} a√±os promedio\")\n",
    "else:\n",
    "    print(\"‚ùå ERROR: Campo 'apto' no encontrado en el dataset\")\n",
    "    print(\"Ejecuta primero el notebook de preparaci√≥n de datos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PREPARACI√ìN DE CARACTER√çSTICAS (FEATURE ENGINEERING)\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import json\n",
    "\n",
    "print(\"\\nüîß PREPARACI√ìN DE CARACTER√çSTICAS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Identificar tipos de columnas\n",
    "def identify_column_types(df):\n",
    "    \"\"\"Identifica autom√°ticamente los tipos de columnas\"\"\"\n",
    "    numeric_cols = []\n",
    "    categorical_cols = []\n",
    "    list_cols = []\n",
    "    dict_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in ['apto', 'ID']:  # Excluir target y ID\n",
    "            continue\n",
    "            \n",
    "        # Verificar si es num√©rico\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            numeric_cols.append(col)\n",
    "        # Verificar si contiene listas o diccionarios\n",
    "        elif df[col].apply(lambda x: isinstance(x, (list, dict))).any():\n",
    "            if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "                list_cols.append(col)\n",
    "            else:\n",
    "                dict_cols.append(col)\n",
    "        # Resto son categ√≥ricas\n",
    "        else:\n",
    "            categorical_cols.append(col)\n",
    "    \n",
    "    return numeric_cols, categorical_cols, list_cols, dict_cols\n",
    "\n",
    "# Identificar columnas\n",
    "numeric_cols, categorical_cols, list_cols, dict_cols = identify_column_types(df)\n",
    "\n",
    "print(f\"üìä Columnas num√©ricas ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"üè∑Ô∏è  Columnas categ√≥ricas ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"üìã Columnas con listas ({len(list_cols)}): {list_cols}\")\n",
    "print(f\"üìñ Columnas con diccionarios ({len(dict_cols)}): {dict_cols}\")\n",
    "\n",
    "# Funci√≥n para procesar columnas complejas\n",
    "def process_skills_column(skills_series):\n",
    "    \"\"\"Convierte listas de skills en features binarias\"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # Convertir listas a strings\n",
    "    skills_text = skills_series.apply(lambda x: ' '.join(x) if isinstance(x, list) else '')\n",
    "    \n",
    "    # Usar CountVectorizer para crear features binarias\n",
    "    vectorizer = CountVectorizer(binary=True, max_features=50, min_df=2)\n",
    "    skills_matrix = vectorizer.fit_transform(skills_text)\n",
    "    \n",
    "    # Crear DataFrame con nombres de features\n",
    "    feature_names = [f\"skill_{name}\" for name in vectorizer.get_feature_names_out()]\n",
    "    skills_df = pd.DataFrame(skills_matrix.toarray(), columns=feature_names, index=skills_series.index)\n",
    "    \n",
    "    return skills_df, vectorizer\n",
    "\n",
    "def process_languages_column(languages_series):\n",
    "    \"\"\"Convierte diccionarios de idiomas en features num√©ricas\"\"\"\n",
    "    language_features = []\n",
    "    \n",
    "    # Mapeo de niveles a n√∫meros\n",
    "    level_mapping = {'A1': 1, 'A2': 2, 'B1': 3, 'B2': 4, 'C1': 5, 'C2': 6, 'NATIVE': 6}\n",
    "    \n",
    "    for languages in languages_series:\n",
    "        if isinstance(languages, dict):\n",
    "            # Extraer caracter√≠sticas clave\n",
    "            features = {\n",
    "                'num_languages': len(languages),\n",
    "                'english_level': level_mapping.get(languages.get('English', ''), 0),\n",
    "                'has_spanish': 1 if 'Spanish' in languages else 0,\n",
    "                'max_language_level': max([level_mapping.get(level, 0) for level in languages.values()], default=0)\n",
    "            }\n",
    "        else:\n",
    "            features = {'num_languages': 0, 'english_level': 0, 'has_spanish': 0, 'max_language_level': 0}\n",
    "        \n",
    "        language_features.append(features)\n",
    "    \n",
    "    return pd.DataFrame(language_features, index=languages_series.index)\n",
    "\n",
    "# Procesar columnas complejas\n",
    "processed_features = []\n",
    "\n",
    "# Skills\n",
    "if 'skills' in list_cols:\n",
    "    print(\"\\nüî® Procesando columna 'skills'...\")\n",
    "    skills_df, skills_vectorizer = process_skills_column(df['skills'])\n",
    "    processed_features.append(skills_df)\n",
    "    print(f\"‚úÖ {skills_df.shape[1]} features de skills creadas\")\n",
    "\n",
    "# Languages\n",
    "if 'languages' in dict_cols:\n",
    "    print(\"\\nüåç Procesando columna 'languages'...\")\n",
    "    languages_df = process_languages_column(df['languages'])\n",
    "    processed_features.append(languages_df)\n",
    "    print(f\"‚úÖ {languages_df.shape[1]} features de idiomas creadas\")\n",
    "\n",
    "# Certifications (contar n√∫mero)\n",
    "if 'certifications' in list_cols:\n",
    "    print(\"\\nüèÜ Procesando columna 'certifications'...\")\n",
    "    cert_df = pd.DataFrame({\n",
    "        'num_certifications': df['certifications'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "    }, index=df.index)\n",
    "    processed_features.append(cert_df)\n",
    "    print(\"‚úÖ Feature de certificaciones creada\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(processed_features)} grupos de features procesadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CREACI√ìN DEL DATASET FINAL PARA ENTRENAMIENTO\n",
    "print(\"\\nüîó COMBINANDO TODAS LAS CARACTER√çSTICAS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Combinar todas las features\n",
    "feature_dfs = []\n",
    "\n",
    "# Agregar columnas num√©ricas (limpiar valores nulos)\n",
    "if numeric_cols:\n",
    "    numeric_df = df[numeric_cols].fillna(0)  # Rellenar nulos con 0\n",
    "    feature_dfs.append(numeric_df)\n",
    "    print(f\"‚úÖ {len(numeric_cols)} columnas num√©ricas agregadas\")\n",
    "\n",
    "# Agregar columnas categ√≥ricas (one-hot encoding)\n",
    "if categorical_cols:\n",
    "    # Filtrar columnas categ√≥ricas que no sean muy sparsas\n",
    "    valid_categorical = []\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            # Verificar que la columna no tenga demasiados valores √∫nicos\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals <= 20:  # M√°ximo 20 categor√≠as\n",
    "                valid_categorical.append(col)\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Saltando '{col}': demasiadas categor√≠as ({unique_vals})\")\n",
    "    \n",
    "    if valid_categorical:\n",
    "        categorical_df = pd.get_dummies(df[valid_categorical], prefix=valid_categorical, dummy_na=True)\n",
    "        feature_dfs.append(categorical_df)\n",
    "        print(f\"‚úÖ {len(valid_categorical)} columnas categ√≥ricas procesadas -> {categorical_df.shape[1]} features\")\n",
    "\n",
    "# Agregar features procesadas\n",
    "feature_dfs.extend(processed_features)\n",
    "\n",
    "# Combinar todo\n",
    "if feature_dfs:\n",
    "    X = pd.concat(feature_dfs, axis=1)\n",
    "    print(f\"\\nüéØ Dataset final: {X.shape}\")\n",
    "    print(f\"üìä Total de features: {X.shape[1]}\")\n",
    "    \n",
    "    # Target\n",
    "    y = df['apto'].copy()\n",
    "    \n",
    "    # Verificar completitud\n",
    "    print(f\"\\nüìã VERIFICACI√ìN DE DATOS:\")\n",
    "    print(f\"  Features con valores nulos: {X.isnull().sum().sum()}\")\n",
    "    print(f\"  Target con valores nulos: {y.isnull().sum()}\")\n",
    "    \n",
    "    # Limpiar cualquier valor nulo restante\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Mostrar algunas estad√≠sticas\n",
    "    print(f\"\\nüìà ESTAD√çSTICAS DE FEATURES:\")\n",
    "    print(f\"  Rango de valores: [{X.min().min():.2f}, {X.max().max():.2f}]\")\n",
    "    print(f\"  Features con varianza cero: {(X.var() == 0).sum()}\")\n",
    "    \n",
    "    # Eliminar features con varianza cero\n",
    "    non_zero_var_cols = X.columns[X.var() != 0]\n",
    "    if len(non_zero_var_cols) < len(X.columns):\n",
    "        removed_features = len(X.columns) - len(non_zero_var_cols)\n",
    "        X = X[non_zero_var_cols]\n",
    "        print(f\"üóëÔ∏è  Eliminadas {removed_features} features con varianza cero\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset final preparado: {X.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå ERROR: No se pudieron crear features\")\n",
    "    X, y = None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DIVISI√ìN TRAIN/VALIDATION/TEST Y BALANCEO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "print(\"\\nüìä DIVISI√ìN Y BALANCEO DEL DATASET\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if X is not None and y is not None:\n",
    "    # Manejar clase de \"revisi√≥n manual\" (-1)\n",
    "    print(\"üéØ Estrategia para valores de revisi√≥n manual (-1):\")\n",
    "    revision_count = (y == -1).sum()\n",
    "    \n",
    "    if revision_count > 0:\n",
    "        print(f\"  Encontrados {revision_count} casos de revisi√≥n manual\")\n",
    "        print(\"  Opciones: 1) Excluir, 2) Tratar como clase separada, 3) Reasignar\")\n",
    "        \n",
    "        # Por ahora, vamos a excluir los casos de revisi√≥n manual para simplificar\n",
    "        mask_no_revision = y != -1\n",
    "        X_clean = X[mask_no_revision]\n",
    "        y_clean = y[mask_no_revision]\n",
    "        \n",
    "        print(f\"  ‚úÖ Excluyendo casos de revisi√≥n manual\")\n",
    "        print(f\"  üìä Dataset limpio: {X_clean.shape[0]} muestras\")\n",
    "    else:\n",
    "        X_clean, y_clean = X, y\n",
    "        print(\"  ‚úÖ No hay casos de revisi√≥n manual\")\n",
    "    \n",
    "    # Divisi√≥n inicial\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X_clean, y_clean, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y_clean\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, \n",
    "        test_size=0.25,  # 0.25 * 0.8 = 0.2 del total\n",
    "        random_state=42, \n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìÇ DIVISI√ìN DEL DATASET:\")\n",
    "    print(f\"  Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X_clean)*100:.1f}%)\")\n",
    "    print(f\"  Validaci√≥n: {X_val.shape[0]} muestras ({X_val.shape[0]/len(X_clean)*100:.1f}%)\")\n",
    "    print(f\"  Test: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X_clean)*100:.1f}%)\")\n",
    "    \n",
    "    # Analizar balance en cada conjunto\n",
    "    for name, y_subset in [(\"Entrenamiento\", y_train), (\"Validaci√≥n\", y_val), (\"Test\", y_test)]:\n",
    "        counts = Counter(y_subset)\n",
    "        total = len(y_subset)\n",
    "        print(f\"\\n{name}:\")\n",
    "        for class_val, count in sorted(counts.items()):\n",
    "            label = \"Aptos\" if class_val == 1 else \"No aptos\"\n",
    "            print(f\"  {label}: {count} ({count/total*100:.1f}%)\")\n",
    "    \n",
    "    # Decidir si aplicar balanceo\n",
    "    train_counts = Counter(y_train)\n",
    "    if len(train_counts) > 1:\n",
    "        minority_class = min(train_counts.values())\n",
    "        majority_class = max(train_counts.values())\n",
    "        imbalance_ratio = minority_class / majority_class\n",
    "        \n",
    "        print(f\"\\n‚öñÔ∏è  AN√ÅLISIS DE BALANCE:\")\n",
    "        print(f\"  Ratio de balance: {imbalance_ratio:.2f}\")\n",
    "        \n",
    "        if imbalance_ratio < 0.5:  # Desbalanceado\n",
    "            print(\"  üîÑ Aplicando t√©cnicas de balanceo...\")\n",
    "            \n",
    "            try:\n",
    "                # Aplicar SMOTE para sobremuestreo de la clase minoritaria\n",
    "                smote = SMOTE(random_state=42)\n",
    "                X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "                \n",
    "                balanced_counts = Counter(y_train_balanced)\n",
    "                print(f\"  ‚úÖ SMOTE aplicado:\")\n",
    "                for class_val, count in sorted(balanced_counts.items()):\n",
    "                    label = \"Aptos\" if class_val == 1 else \"No aptos\"\n",
    "                    print(f\"    {label}: {count}\")\n",
    "                \n",
    "                print(f\"  üìà Dataset balanceado: {X_train_balanced.shape}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Error aplicando SMOTE: {e}\")\n",
    "                print(\"  üìä Usando dataset original\")\n",
    "                X_train_balanced, y_train_balanced = X_train, y_train\n",
    "        else:\n",
    "            print(\"  ‚úÖ Dataset relativamente balanceado - no se requiere balanceo\")\n",
    "            X_train_balanced, y_train_balanced = X_train, y_train\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  Solo una clase presente - revisar datos\")\n",
    "        X_train_balanced, y_train_balanced = X_train, y_train\n",
    "    \n",
    "    print(f\"\\n‚úÖ Datos preparados para entrenamiento\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå ERROR: No hay datos para dividir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ENTRENAMIENTO DE M√öLTIPLES MODELOS\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "print(\"\\nü§ñ ENTRENAMIENTO DE MODELOS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Diccionario de modelos a entrenar\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced'\n",
    "    ),\n",
    "    'SVM': SVC(\n",
    "        kernel='rbf',\n",
    "        probability=True,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Scaler para modelos que lo requieren\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "trained_models = {}\n",
    "model_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüîÑ Entrenando {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Usar datos escalados para modelos que lo requieren\n",
    "        if model_name in ['Logistic Regression', 'SVM']:\n",
    "            model.fit(X_train_scaled, y_train_balanced)\n",
    "            \n",
    "            # Predicciones\n",
    "            y_val_pred = model.predict(X_val_scaled)\n",
    "            y_val_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        else:\n",
    "            model.fit(X_train_balanced, y_train_balanced)\n",
    "            \n",
    "            # Predicciones\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # M√©tricas\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # ROC-AUC\n",
    "        try:\n",
    "            auc_score = roc_auc_score(y_val, y_val_proba)\n",
    "        except:\n",
    "            auc_score = 0.0\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = (y_val_pred == y_val).mean()\n",
    "        \n",
    "        # Precision, Recall, F1 por clase\n",
    "        report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "        \n",
    "        # Guardar modelo y resultados\n",
    "        trained_models[model_name] = model\n",
    "        model_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc_score,\n",
    "            'training_time': training_time,\n",
    "            'precision_0': report['0']['precision'] if '0' in report else 0,\n",
    "            'recall_0': report['0']['recall'] if '0' in report else 0,\n",
    "            'f1_0': report['0']['f1-score'] if '0' in report else 0,\n",
    "            'precision_1': report['1']['precision'] if '1' in report else 0,\n",
    "            'recall_1': report['1']['recall'] if '1' in report else 0,\n",
    "            'f1_1': report['1']['f1-score'] if '1' in report else 0,\n",
    "            'macro_f1': report['macro avg']['f1-score'],\n",
    "            'predictions': y_val_pred,\n",
    "            'probabilities': y_val_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ Completado en {training_time:.2f}s\")\n",
    "        print(f\"  üìä Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"  üìà AUC: {auc_score:.3f}\")\n",
    "        print(f\"  üéØ F1-Score (macro): {report['macro avg']['f1-score']:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error entrenando {model_name}: {e}\")\n",
    "        model_results[model_name] = None\n",
    "\n",
    "print(f\"\\n‚úÖ Entrenamiento completado\")\n",
    "print(f\"üìä {len([r for r in model_results.values() if r is not None])} modelos entrenados exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. INTERPRETABILIDAD Y AN√ÅLISIS DE CARACTER√çSTICAS\n",
    "print(\"\\nüîç AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# An√°lisis de importancia seg√∫n el tipo de modelo\n",
    "if len(results_df) > 0:\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    # Feature importance para modelos basados en √°rboles\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüå≥ IMPORTANCIA DE CARACTER√çSTICAS ({best_model_name}):\")\n",
    "        print(\"Top 15 caracter√≠sticas m√°s importantes:\")\n",
    "        print(feature_importance.head(15).to_string(index=False))\n",
    "        \n",
    "        # Visualizaci√≥n\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            top_features = feature_importance.head(15)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'])\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('Importancia')\n",
    "            plt.title(f'Top 15 Caracter√≠sticas M√°s Importantes\\n({best_model_name})')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(MODELS_PATH / \"feature_importance.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print(f\"üìä Gr√°fico guardado en: {MODELS_PATH / 'feature_importance.png'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error creando visualizaci√≥n: {e}\")\n",
    "    \n",
    "    # Coeficientes para modelos lineales\n",
    "    elif hasattr(best_model, 'coef_'):\n",
    "        feature_coef = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'coefficient': best_model.coef_[0]\n",
    "        })\n",
    "        feature_coef['abs_coefficient'] = np.abs(feature_coef['coefficient'])\n",
    "        feature_coef = feature_coef.sort_values('abs_coefficient', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüìä COEFICIENTES DEL MODELO ({best_model_name}):\")\n",
    "        print(\"Top 15 caracter√≠sticas con mayor impacto:\")\n",
    "        display_coef = feature_coef[['feature', 'coefficient']].head(15)\n",
    "        print(display_coef.to_string(index=False))\n",
    "    \n",
    "    # An√°lisis de casos mal clasificados\n",
    "    if best_model_name in ['Logistic Regression', 'SVM']:\n",
    "        y_val_pred_best = best_model.predict(X_val_scaled)\n",
    "        y_val_proba_best = best_model.predict_proba(X_val_scaled)[:, 1]\n",
    "    else:\n",
    "        y_val_pred_best = best_model.predict(X_val)\n",
    "        y_val_proba_best = best_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Casos mal clasificados\n",
    "    misclassified = X_val[y_val != y_val_pred_best]\n",
    "    misclassified_true = y_val[y_val != y_val_pred_best]\n",
    "    misclassified_pred = y_val_pred_best[y_val != y_val_pred_best]\n",
    "    misclassified_proba = y_val_proba_best[y_val != y_val_pred_best]\n",
    "    \n",
    "    print(f\"\\n‚ùå AN√ÅLISIS DE CASOS MAL CLASIFICADOS:\")\n",
    "    print(f\"Total de casos mal clasificados: {len(misclassified)}\")\n",
    "    \n",
    "    if len(misclassified) > 0:\n",
    "        # Falsos positivos (predicho apto, pero no apto)\n",
    "        false_positives = sum((misclassified_true == 0) & (misclassified_pred == 1))\n",
    "        # Falsos negativos (predicho no apto, pero apto)\n",
    "        false_negatives = sum((misclassified_true == 1) & (misclassified_pred == 0))\n",
    "        \n",
    "        print(f\"  Falsos positivos: {false_positives}\")\n",
    "        print(f\"  Falsos negativos: {false_negatives}\")\n",
    "        \n",
    "        # Mostrar algunos casos con probabilidades\n",
    "        print(f\"\\nEjemplos de casos mal clasificados (con probabilidades):\")\n",
    "        for i in range(min(5, len(misclassified))):\n",
    "            true_label = \"Apto\" if misclassified_true.iloc[i] == 1 else \"No apto\"\n",
    "            pred_label = \"Apto\" if misclassified_pred[i] == 1 else \"No apto\"\n",
    "            proba = misclassified_proba[i]\n",
    "            print(f\"  Real: {true_label}, Predicho: {pred_label}, Probabilidad: {proba:.3f}\")\n",
    "\n",
    "# Resumen final\n",
    "print(f\"\\nüéâ RESUMEN DEL ENTRENAMIENTO\")\n",
    "print(\"=\"*40)\n",
    "if len(results_df) > 0:\n",
    "    print(f\"‚úÖ Mejor modelo: {best_model_name}\")\n",
    "    print(f\"üìä Accuracy en test: {test_accuracy:.3f}\")\n",
    "    print(f\"üìà AUC en test: {test_auc:.3f}\")\n",
    "    print(f\"üéØ F1-Score macro: {best_score:.3f}\")\n",
    "    print(f\"üíæ Modelo guardado en: models/\")\n",
    "    print(f\"üöÄ Listo para despliegue en Azure ML!\")\n",
    "else:\n",
    "    print(\"‚ùå No se pudieron entrenar modelos exitosamente\")\n",
    "\n",
    "print(f\"\\nüìã PR√ìXIMOS PASOS:\")\n",
    "print(\"1. Revisar el notebook 03_evaluation.ipynb para an√°lisis detallado\")\n",
    "print(\"2. Usar el notebook 04_rai_dashboard.ipynb para an√°lisis de responsabilidad\")\n",
    "print(\"3. Desplegar el modelo en Azure Machine Learning\")\n",
    "print(\"4. Configurar el pipeline de inferencia\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. COMPARACI√ìN Y SELECCI√ìN DEL MEJOR MODELO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"\\nüìä COMPARACI√ìN DE MODELOS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Crear tabla de comparaci√≥n\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "results_df = results_df.dropna()  # Eliminar modelos que fallaron\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    # Mostrar tabla de resultados\n",
    "    print(\"\\nüìà TABLA DE RESULTADOS:\")\n",
    "    display_cols = ['accuracy', 'auc', 'macro_f1', 'precision_1', 'recall_1', 'training_time']\n",
    "    available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "    \n",
    "    if available_cols:\n",
    "        results_display = results_df[available_cols].round(3)\n",
    "        results_display.columns = ['Accuracy', 'AUC', 'F1-Macro', 'Precision(Aptos)', 'Recall(Aptos)', 'Tiempo(s)']\n",
    "        print(results_display.to_string())\n",
    "    \n",
    "    # Encontrar el mejor modelo basado en F1-score macro\n",
    "    best_model_name = results_df['macro_f1'].idxmax()\n",
    "    best_score = results_df.loc[best_model_name, 'macro_f1']\n",
    "    \n",
    "    print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
    "    print(f\"üéØ F1-Score (macro): {best_score:.3f}\")\n",
    "    print(f\"üìä Accuracy: {results_df.loc[best_model_name, 'accuracy']:.3f}\")\n",
    "    print(f\"üìà AUC: {results_df.loc[best_model_name, 'auc']:.3f}\")\n",
    "    \n",
    "    # Evaluaci√≥n detallada del mejor modelo\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    # Usar datos escalados si es necesario\n",
    "    if best_model_name in ['Logistic Regression', 'SVM']:\n",
    "        y_test_pred = best_model.predict(X_test_scaled)\n",
    "        y_test_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\nüß™ EVALUACI√ìN EN CONJUNTO DE TEST:\")\n",
    "    test_accuracy = (y_test_pred == y_test).mean()\n",
    "    test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    test_report = classification_report(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"üìä Accuracy en test: {test_accuracy:.3f}\")\n",
    "    print(f\"üìà AUC en test: {test_auc:.3f}\")\n",
    "    print(f\"\\nüìã Reporte detallado:\")\n",
    "    print(test_report)\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(f\"\\nüî¢ Matriz de Confusi√≥n:\")\n",
    "    print(f\"               Predicho\")\n",
    "    print(f\"Real    No Apto  Apto\")\n",
    "    print(f\"No Apto   {cm[0,0]:3d}    {cm[0,1]:3d}\")\n",
    "    print(f\"Apto      {cm[1,0]:3d}    {cm[1,1]:3d}\")\n",
    "    \n",
    "    # Guardar el mejor modelo\n",
    "    model_filename = MODELS_PATH / f\"best_model_{best_model_name.lower().replace(' ', '_')}.joblib\"\n",
    "    scaler_filename = MODELS_PATH / \"scaler.joblib\"\n",
    "    \n",
    "    joblib.dump(best_model, model_filename)\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    \n",
    "    # Guardar metadatos del modelo\n",
    "    model_metadata = {\n",
    "        'model_name': best_model_name,\n",
    "        'model_type': str(type(best_model).__name__),\n",
    "        'features': list(X.columns),\n",
    "        'feature_count': len(X.columns),\n",
    "        'train_accuracy': results_df.loc[best_model_name, 'accuracy'],\n",
    "        'train_auc': results_df.loc[best_model_name, 'auc'],\n",
    "        'train_f1_macro': best_score,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_auc': test_auc,\n",
    "        'training_samples': len(X_train_balanced),\n",
    "        'test_samples': len(X_test),\n",
    "        'classes': list(np.unique(y_clean)),\n",
    "        'created_at': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    metadata_filename = MODELS_PATH / \"model_metadata.json\"\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(model_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ MODELO GUARDADO:\")\n",
    "    print(f\"üìÑ Modelo: {model_filename}\")\n",
    "    print(f\"‚öñÔ∏è  Scaler: {scaler_filename}\")\n",
    "    print(f\"üìã Metadatos: {metadata_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå ERROR: No hay modelos v√°lidos para comparar\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
