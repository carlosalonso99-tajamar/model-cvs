{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUACI√ìN DETALLADA DEL MODELO DE SELECCI√ìN DE CANDIDATOS - AZURE ML\n",
    "# ========================================================================\n",
    "\n",
    "# 1. CONFIGURACI√ìN Y CARGA DE ARTEFACTOS DESDE AZURE ML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Azure ML imports\n",
    "from azureml.core import Workspace, Dataset, Experiment, Run, Model\n",
    "from azureml.core.model import Model as AMLModel\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Metrics y visualizaci√≥n\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score, f1_score,\n",
    "    precision_score, recall_score, accuracy_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üîç EVALUACI√ìN DETALLADA DEL MODELO DE SELECCI√ìN DE CANDIDATOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Conectar al workspace\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(f\"‚úÖ Conectado al workspace: {ws.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error conectando al workspace: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "print(f\"üîÑ Cargando artefactos del entrenamiento...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CARGA DEL MODELO Y DATOS DESDE AZURE ML\n",
    "print(\"\\nüì• CARGA DE MODELO Y DATOS REGISTRADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Cargar el modelo registrado m√°s reciente\n",
    "MODEL_NAME = \"candidate-selection-model\"\n",
    "\n",
    "try:\n",
    "    # Obtener la versi√≥n m√°s reciente del modelo\n",
    "    registered_model = AMLModel(ws, name=MODEL_NAME)\n",
    "    print(f\"‚úÖ Modelo encontrado: {MODEL_NAME}\")\n",
    "    print(f\"üìã Versi√≥n: {registered_model.version}\")\n",
    "    print(f\"üè∑Ô∏è  Tags: {registered_model.tags}\")\n",
    "    \n",
    "    # Descargar artefactos del modelo\n",
    "    model_path = registered_model.download(target_dir=\"./model_artifacts\")\n",
    "    print(f\"üìÅ Artefactos descargados en: {model_path}\")\n",
    "    \n",
    "    # Cargar modelo y scaler\n",
    "    model = joblib.load(f\"{model_path}/model.pkl\")\n",
    "    scaler = joblib.load(f\"{model_path}/scaler.pkl\")\n",
    "    \n",
    "    # Cargar metadatos\n",
    "    with open(f\"{model_path}/feature_names.json\", 'r') as f:\n",
    "        feature_metadata = json.load(f)\n",
    "    \n",
    "    with open(f\"{model_path}/notebook_info.json\", 'r') as f:\n",
    "        training_info = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo cargado: {type(model).__name__}\")\n",
    "    print(f\"üìä Features: {feature_metadata['feature_count']}\")\n",
    "    print(f\"üéØ Mejor m√©trica F1: {training_info['test_metrics']['f1_macro']:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cargando modelo registrado: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de haber ejecutado el notebook de entrenamiento primero\")\n",
    "    raise\n",
    "\n",
    "# Cargar datasets de evaluaci√≥n\n",
    "try:\n",
    "    # Cargar datos de test y validaci√≥n\n",
    "    test_data = pd.read_parquet(f\"{model_path}/test_data.parquet\")\n",
    "    val_data = pd.read_parquet(f\"{model_path}/val_data.parquet\")\n",
    "    \n",
    "    print(f\"\\nüìä Datos de test cargados: {test_data.shape}\")\n",
    "    print(f\"üìä Datos de validaci√≥n cargados: {val_data.shape}\")\n",
    "    \n",
    "    # Separar features y targets\n",
    "    feature_names = feature_metadata['feature_names']\n",
    "    \n",
    "    X_test = test_data[feature_names]\n",
    "    y_test_true = test_data['y_true']\n",
    "    y_test_pred = test_data['y_pred']\n",
    "    y_test_proba = test_data['y_proba']\n",
    "    \n",
    "    X_val = val_data[feature_names]\n",
    "    y_val_true = val_data['y_true'] \n",
    "    y_val_pred = val_data['y_pred']\n",
    "    y_val_proba = val_data['y_proba']\n",
    "    \n",
    "    print(f\"‚úÖ Datos organizados:\")\n",
    "    print(f\"  Test: {len(X_test)} muestras, {len(feature_names)} features\")\n",
    "    print(f\"  Validaci√≥n: {len(X_val)} muestras\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error cargando datos: {e}\")\n",
    "    raise\n",
    "\n",
    "# Informaci√≥n del modelo entrenado\n",
    "print(f\"\\nüìã INFORMACI√ìN DEL ENTRENAMIENTO:\")\n",
    "print(f\"  Experimento: {training_info['experiment_name']}\")\n",
    "print(f\"  Run ID: {training_info['run_id']}\")\n",
    "print(f\"  Mejor modelo: {training_info['best_model_name']}\")\n",
    "print(f\"  Requiere escalado: {training_info['model_requires_scaling']}\")\n",
    "print(f\"  Tama√±o entrenamiento: {training_info['dataset_info']['train_size']:,}\")\n",
    "print(f\"  Tama√±o test: {training_info['dataset_info']['test_size']:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. AN√ÅLISIS DETALLADO DE M√âTRICAS DE RENDIMIENTO\n",
    "print(\"\\nüìä AN√ÅLISIS DETALLADO DE M√âTRICAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, y_proba, dataset_name=\"Test\"):\n",
    "    \"\"\"Calcula m√©tricas comprensivas para evaluaci√≥n\"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # M√©tricas b√°sicas\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred, average='macro')\n",
    "    metrics['recall'] = recall_score(y_true, y_pred, average='macro')\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro')\n",
    "    metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # M√©tricas por clase\n",
    "    metrics['precision_class_0'] = precision_score(y_true, y_pred, pos_label=0)\n",
    "    metrics['recall_class_0'] = recall_score(y_true, y_pred, pos_label=0)\n",
    "    metrics['f1_class_0'] = f1_score(y_true, y_pred, pos_label=0)\n",
    "    \n",
    "    metrics['precision_class_1'] = precision_score(y_true, y_pred, pos_label=1)\n",
    "    metrics['recall_class_1'] = recall_score(y_true, y_pred, pos_label=1)\n",
    "    metrics['f1_class_1'] = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    # M√©tricas basadas en probabilidades\n",
    "    try:\n",
    "        metrics['auc_roc'] = roc_auc_score(y_true, y_proba)\n",
    "        metrics['auc_pr'] = average_precision_score(y_true, y_proba)\n",
    "    except:\n",
    "        metrics['auc_roc'] = np.nan\n",
    "        metrics['auc_pr'] = np.nan\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['true_negatives'] = int(cm[0,0])\n",
    "    metrics['false_positives'] = int(cm[0,1])\n",
    "    metrics['false_negatives'] = int(cm[1,0])\n",
    "    metrics['true_positives'] = int(cm[1,1])\n",
    "    \n",
    "    # M√©tricas derivadas\n",
    "    total = len(y_true)\n",
    "    metrics['specificity'] = metrics['true_negatives'] / (metrics['true_negatives'] + metrics['false_positives'])\n",
    "    metrics['sensitivity'] = metrics['true_positives'] / (metrics['true_positives'] + metrics['false_negatives'])\n",
    "    metrics['positive_predictive_value'] = metrics['true_positives'] / (metrics['true_positives'] + metrics['false_positives'])\n",
    "    metrics['negative_predictive_value'] = metrics['true_negatives'] / (metrics['true_negatives'] + metrics['false_negatives'])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calcular m√©tricas para test y validaci√≥n\n",
    "test_metrics = calculate_comprehensive_metrics(y_test_true, y_test_pred, y_test_proba, \"Test\")\n",
    "val_metrics = calculate_comprehensive_metrics(y_val_true, y_val_pred, y_val_proba, \"Validaci√≥n\")\n",
    "\n",
    "# Crear tabla comparativa\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Validaci√≥n': val_metrics,\n",
    "    'Test': test_metrics\n",
    "}).round(3)\n",
    "\n",
    "print(\"üìà M√âTRICAS COMPARATIVAS (Validaci√≥n vs Test):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mostrar m√©tricas principales\n",
    "main_metrics = [\n",
    "    'accuracy', 'precision', 'recall', 'f1_macro', 'f1_weighted', \n",
    "    'auc_roc', 'auc_pr', 'specificity', 'sensitivity'\n",
    "]\n",
    "\n",
    "print(metrics_comparison.loc[main_metrics].to_string())\n",
    "\n",
    "print(f\"\\nüî¢ MATRIZ DE CONFUSI√ìN - CONJUNTO DE TEST:\")\n",
    "print(f\"               Predicho\")\n",
    "print(f\"Real     No Apto   Apto\")\n",
    "print(f\"No Apto    {test_metrics['true_negatives']:3d}     {test_metrics['false_positives']:3d}\")\n",
    "print(f\"Apto       {test_metrics['false_negatives']:3d}     {test_metrics['true_positives']:3d}\")\n",
    "\n",
    "print(f\"\\nüìä M√âTRICAS POR CLASE - TEST:\")\n",
    "print(f\"Clase 0 (No Apto):\")\n",
    "print(f\"  Precision: {test_metrics['precision_class_0']:.3f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall_class_0']:.3f}\")\n",
    "print(f\"  F1-Score:  {test_metrics['f1_class_0']:.3f}\")\n",
    "\n",
    "print(f\"\\nClase 1 (Apto):\")\n",
    "print(f\"  Precision: {test_metrics['precision_class_1']:.3f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall_class_1']:.3f}\")\n",
    "print(f\"  F1-Score:  {test_metrics['f1_class_1']:.3f}\")\n",
    "\n",
    "# An√°lisis de estabilidad entre validaci√≥n y test\n",
    "print(f\"\\nüéØ AN√ÅLISIS DE ESTABILIDAD (Val vs Test):\")\n",
    "stability_analysis = {}\n",
    "for metric in main_metrics:\n",
    "    if metric in val_metrics and metric in test_metrics:\n",
    "        diff = abs(val_metrics[metric] - test_metrics[metric])\n",
    "        stability_analysis[metric] = diff\n",
    "        \n",
    "        if diff < 0.02:\n",
    "            status = \"‚úÖ Estable\"\n",
    "        elif diff < 0.05:\n",
    "            status = \"‚ö†Ô∏è Moderado\"\n",
    "        else:\n",
    "            status = \"‚ùå Inestable\"\n",
    "            \n",
    "        print(f\"  {metric:20s}: Œî={diff:.3f} {status}\")\n",
    "\n",
    "# Guardar m√©tricas para registro\n",
    "evaluation_results = {\n",
    "    'validation_metrics': val_metrics,\n",
    "    'test_metrics': test_metrics,\n",
    "    'stability_analysis': stability_analysis,\n",
    "    'model_info': {\n",
    "        'name': training_info['best_model_name'],\n",
    "        'version': registered_model.version,\n",
    "        'features_count': len(feature_names)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. VISUALIZACIONES AVANZADAS DE RENDIMIENTO\n",
    "print(\"\\nüìä VISUALIZACIONES DE RENDIMIENTO\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Configurar subplot principal\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Curva ROC\n",
    "plt.subplot(2, 4, 1)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test_true, y_test_proba)\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val_true, y_val_proba)\n",
    "\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test (AUC = {test_metrics[\"auc_roc\"]:.3f})', linewidth=2)\n",
    "plt.plot(fpr_val, tpr_val, label=f'Val (AUC = {val_metrics[\"auc_roc\"]:.3f})', linewidth=2, linestyle='--')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Curva Precision-Recall\n",
    "plt.subplot(2, 4, 2)\n",
    "precision_test, recall_test, _ = precision_recall_curve(y_test_true, y_test_proba)\n",
    "precision_val, recall_val, _ = precision_recall_curve(y_val_true, y_val_proba)\n",
    "\n",
    "plt.plot(recall_test, precision_test, label=f'Test (AP = {test_metrics[\"auc_pr\"]:.3f})', linewidth=2)\n",
    "plt.plot(recall_val, precision_val, label=f'Val (AP = {val_metrics[\"auc_pr\"]:.3f})', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Curva Precision-Recall')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribuci√≥n de probabilidades\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.hist(y_test_proba[y_test_true == 0], bins=30, alpha=0.5, label='No Apto', density=True)\n",
    "plt.hist(y_test_proba[y_test_true == 1], bins=30, alpha=0.5, label='Apto', density=True)\n",
    "plt.xlabel('Probabilidad Predicha')\n",
    "plt.ylabel('Densidad')\n",
    "plt.title('Distribuci√≥n de Probabilidades')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Calibration curve\n",
    "plt.subplot(2, 4, 4)\n",
    "try:\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(y_test_true, y_test_proba, n_bins=10)\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Modelo\", linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectamente calibrado\")\n",
    "    plt.xlabel('Probabilidad Media Predicha')\n",
    "    plt.ylabel('Fracci√≥n de Positivos')\n",
    "    plt.title('Calibraci√≥n del Modelo')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "except Exception as e:\n",
    "    plt.text(0.5, 0.5, f'Error en calibraci√≥n: {str(e)[:50]}...', ha='center', va='center')\n",
    "\n",
    "# 5. Matriz de confusi√≥n - Test\n",
    "plt.subplot(2, 4, 5)\n",
    "cm_test = confusion_matrix(y_test_true, y_test_pred)\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Apto', 'Apto'], yticklabels=['No Apto', 'Apto'])\n",
    "plt.title('Matriz Confusi√≥n - Test')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicho')\n",
    "\n",
    "# 6. Matriz de confusi√≥n - Validaci√≥n\n",
    "plt.subplot(2, 4, 6)\n",
    "cm_val = confusion_matrix(y_val_true, y_val_pred)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['No Apto', 'Apto'], yticklabels=['No Apto', 'Apto'])\n",
    "plt.title('Matriz Confusi√≥n - Validaci√≥n')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicho')\n",
    "\n",
    "# 7. Comparaci√≥n de m√©tricas\n",
    "plt.subplot(2, 4, 7)\n",
    "metrics_for_plot = ['accuracy', 'precision', 'recall', 'f1_macro', 'auc_roc']\n",
    "val_values = [val_metrics[m] for m in metrics_for_plot]\n",
    "test_values = [test_metrics[m] for m in metrics_for_plot]\n",
    "\n",
    "x = np.arange(len(metrics_for_plot))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, val_values, width, label='Validaci√≥n', alpha=0.8)\n",
    "plt.bar(x + width/2, test_values, width, label='Test', alpha=0.8)\n",
    "plt.xlabel('M√©tricas')\n",
    "plt.ylabel('Valor')\n",
    "plt.title('Comparaci√≥n de M√©tricas')\n",
    "plt.xticks(x, metrics_for_plot, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. An√°lisis de umbrales\n",
    "plt.subplot(2, 4, 8)\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_test_proba >= threshold).astype(int)\n",
    "    if len(np.unique(y_pred_thresh)) > 1:  # Evitar divisiones por cero\n",
    "        f1_scores.append(f1_score(y_test_true, y_pred_thresh))\n",
    "        precisions.append(precision_score(y_test_true, y_pred_thresh))\n",
    "        recalls.append(recall_score(y_test_true, y_pred_thresh))\n",
    "    else:\n",
    "        f1_scores.append(0)\n",
    "        precisions.append(0)\n",
    "        recalls.append(0)\n",
    "\n",
    "plt.plot(thresholds, f1_scores, label='F1-Score', linewidth=2)\n",
    "plt.plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "plt.plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Umbral 0.5')\n",
    "plt.xlabel('Umbral')\n",
    "plt.ylabel('M√©trica')\n",
    "plt.title('M√©tricas vs Umbral')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Dashboard de evaluaci√≥n generado y guardado como 'model_evaluation_dashboard.png'\")\n",
    "\n",
    "# Encontrar umbral √≥ptimo para F1\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "optimal_f1 = f1_scores[optimal_idx]\n",
    "\n",
    "print(f\"\\nüéØ UMBRAL √ìPTIMO PARA F1-SCORE:\")\n",
    "print(f\"  Umbral √≥ptimo: {optimal_threshold:.3f}\")\n",
    "print(f\"  F1-Score √≥ptimo: {optimal_f1:.3f}\")\n",
    "print(f\"  F1-Score actual (umbral 0.5): {test_metrics['f1_macro']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS\n",
    "print(\"\\nüîç AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# An√°lisis de importancia seg√∫n el tipo de modelo\n",
    "def analyze_feature_importance(model, X_test, y_test_true, feature_names):\n",
    "    \"\"\"Analiza la importancia de las caracter√≠sticas del modelo\"\"\"\n",
    "    \n",
    "    importance_results = {}\n",
    "    \n",
    "    # 1. Feature importance nativa (para modelos basados en √°rboles)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        native_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        importance_results['native'] = native_importance\n",
    "        print(\"üå≥ Importancia nativa del modelo disponible\")\n",
    "    \n",
    "    # 2. Permutation importance (para todos los modelos)\n",
    "    try:\n",
    "        print(\"üîÑ Calculando permutation importance...\")\n",
    "        perm_importance = permutation_importance(\n",
    "            model, X_test, y_test_true, \n",
    "            n_repeats=10, random_state=42, \n",
    "            scoring='f1_macro'\n",
    "        )\n",
    "        \n",
    "        perm_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance_mean': perm_importance.importances_mean,\n",
    "            'importance_std': perm_importance.importances_std\n",
    "        }).sort_values('importance_mean', ascending=False)\n",
    "        \n",
    "        importance_results['permutation'] = perm_df\n",
    "        print(\"‚úÖ Permutation importance calculado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error en permutation importance: {e}\")\n",
    "    \n",
    "    # 3. Coeficientes (para modelos lineales)\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coef_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'coefficient': model.coef_[0] if model.coef_.ndim > 1 else model.coef_,\n",
    "            'abs_coefficient': np.abs(model.coef_[0] if model.coef_.ndim > 1 else model.coef_)\n",
    "        }).sort_values('abs_coefficient', ascending=False)\n",
    "        \n",
    "        importance_results['coefficients'] = coef_df\n",
    "        print(\"üìä Coeficientes del modelo disponibles\")\n",
    "    \n",
    "    return importance_results\n",
    "\n",
    "# Calcular importancia\n",
    "feature_importance = analyze_feature_importance(model, X_test, y_test_true, feature_names)\n",
    "\n",
    "# Visualizar importancia de caracter√≠sticas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "plot_idx = 0\n",
    "\n",
    "# Plot 1: Native feature importance\n",
    "if 'native' in feature_importance:\n",
    "    top_features = feature_importance['native'].head(15)\n",
    "    axes[plot_idx].barh(range(len(top_features)), top_features['importance'])\n",
    "    axes[plot_idx].set_yticks(range(len(top_features)))\n",
    "    axes[plot_idx].set_yticklabels(top_features['feature'])\n",
    "    axes[plot_idx].set_xlabel('Importancia')\n",
    "    axes[plot_idx].set_title('Importancia Nativa del Modelo')\n",
    "    axes[plot_idx].invert_yaxis()\n",
    "    plot_idx += 1\n",
    "\n",
    "# Plot 2: Permutation importance\n",
    "if 'permutation' in feature_importance:\n",
    "    top_perm = feature_importance['permutation'].head(15)\n",
    "    axes[plot_idx].barh(range(len(top_perm)), top_perm['importance_mean'])\n",
    "    axes[plot_idx].set_yticks(range(len(top_perm)))\n",
    "    axes[plot_idx].set_yticklabels(top_perm['feature'])\n",
    "    axes[plot_idx].set_xlabel('Importancia Media')\n",
    "    axes[plot_idx].set_title('Permutation Importance')\n",
    "    axes[plot_idx].invert_yaxis()\n",
    "    plot_idx += 1\n",
    "\n",
    "# Plot 3: Coefficients (if available)\n",
    "if 'coefficients' in feature_importance:\n",
    "    top_coef = feature_importance['coefficients'].head(15)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in top_coef['coefficient']]\n",
    "    axes[plot_idx].barh(range(len(top_coef)), top_coef['coefficient'], color=colors, alpha=0.7)\n",
    "    axes[plot_idx].set_yticks(range(len(top_coef)))\n",
    "    axes[plot_idx].set_yticklabels(top_coef['feature'])\n",
    "    axes[plot_idx].set_xlabel('Coeficiente')\n",
    "    axes[plot_idx].set_title('Coeficientes del Modelo')\n",
    "    axes[plot_idx].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[plot_idx].invert_yaxis()\n",
    "    plot_idx += 1\n",
    "\n",
    "# Plot 4: Comparaci√≥n de importancias (si hay m√∫ltiples m√©todos)\n",
    "if len(feature_importance) > 1:\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Normalizar importancias para comparaci√≥n\n",
    "    for method, df in feature_importance.items():\n",
    "        if method == 'native':\n",
    "            importance_col = 'importance'\n",
    "        elif method == 'permutation':\n",
    "            importance_col = 'importance_mean'\n",
    "        elif method == 'coefficients':\n",
    "            importance_col = 'abs_coefficient'\n",
    "        \n",
    "        # Tomar top 10 features y normalizar\n",
    "        top_10 = df.head(10).copy()\n",
    "        max_val = top_10[importance_col].max()\n",
    "        if max_val > 0:\n",
    "            top_10['normalized'] = top_10[importance_col] / max_val\n",
    "            top_10['method'] = method\n",
    "            comparison_data.append(top_10[['feature', 'normalized', 'method']])\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.concat(comparison_data)\n",
    "        \n",
    "        # Crear pivot para heatmap\n",
    "        pivot_df = comparison_df.pivot(index='feature', columns='method', values='normalized')\n",
    "        \n",
    "        sns.heatmap(pivot_df.fillna(0), annot=True, fmt='.2f', cmap='viridis', \n",
    "                   ax=axes[plot_idx], cbar_kws={'label': 'Importancia Normalizada'})\n",
    "        axes[plot_idx].set_title('Comparaci√≥n de M√©todos de Importancia')\n",
    "        axes[plot_idx].set_xlabel('M√©todo')\n",
    "        axes[plot_idx].set_ylabel('Feature')\n",
    "        plot_idx += 1\n",
    "\n",
    "# Ocultar subplots no utilizados\n",
    "for i in range(plot_idx, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ An√°lisis de importancia guardado como 'feature_importance_analysis.png'\")\n",
    "\n",
    "# Mostrar top features por cada m√©todo\n",
    "print(f\"\\nüìã TOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:\")\n",
    "for method, df in feature_importance.items():\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    if method == 'native':\n",
    "        print(df.head(10)[['feature', 'importance']].to_string(index=False))\n",
    "    elif method == 'permutation':\n",
    "        print(df.head(10)[['feature', 'importance_mean', 'importance_std']].to_string(index=False))\n",
    "    elif method == 'coefficients':\n",
    "        print(df.head(10)[['feature', 'coefficient']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. REGISTRO DE RESULTADOS EN AZURE ML\n",
    "print(\"\\nüîÑ REGISTRO DE RESULTADOS EN AZURE ML\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Crear un nuevo experimento para evaluaci√≥n\n",
    "EVALUATION_EXPERIMENT = \"candidate-evaluation\"\n",
    "\n",
    "with mlflow.start_run(experiment_id=mlflow.create_experiment(EVALUATION_EXPERIMENT) if EVALUATION_EXPERIMENT not in [exp.name for exp in mlflow.search_experiments()] else mlflow.get_experiment_by_name(EVALUATION_EXPERIMENT).experiment_id):\n",
    "    \n",
    "    # Registrar m√©tricas de evaluaci√≥n en MLflow\n",
    "    print(\"üìä Registrando m√©tricas de evaluaci√≥n...\")\n",
    "    \n",
    "    # M√©tricas principales del conjunto de test\n",
    "    mlflow.log_metric(\"test_accuracy\", test_metrics['accuracy'])\n",
    "    mlflow.log_metric(\"test_precision\", test_metrics['precision'])\n",
    "    mlflow.log_metric(\"test_recall\", test_metrics['recall'])\n",
    "    mlflow.log_metric(\"test_f1_macro\", test_metrics['f1_macro'])\n",
    "    mlflow.log_metric(\"test_f1_weighted\", test_metrics['f1_weighted'])\n",
    "    mlflow.log_metric(\"test_auc_roc\", test_metrics['auc_roc'])\n",
    "    mlflow.log_metric(\"test_auc_pr\", test_metrics['auc_pr'])\n",
    "    mlflow.log_metric(\"test_specificity\", test_metrics['specificity'])\n",
    "    mlflow.log_metric(\"test_sensitivity\", test_metrics['sensitivity'])\n",
    "    \n",
    "    # M√©tricas por clase\n",
    "    mlflow.log_metric(\"test_precision_class_0\", test_metrics['precision_class_0'])\n",
    "    mlflow.log_metric(\"test_recall_class_0\", test_metrics['recall_class_0'])\n",
    "    mlflow.log_metric(\"test_f1_class_0\", test_metrics['f1_class_0'])\n",
    "    mlflow.log_metric(\"test_precision_class_1\", test_metrics['precision_class_1'])\n",
    "    mlflow.log_metric(\"test_recall_class_1\", test_metrics['recall_class_1'])\n",
    "    mlflow.log_metric(\"test_f1_class_1\", test_metrics['f1_class_1'])\n",
    "    \n",
    "    # M√©tricas de matriz de confusi√≥n\n",
    "    mlflow.log_metric(\"test_true_positives\", test_metrics['true_positives'])\n",
    "    mlflow.log_metric(\"test_true_negatives\", test_metrics['true_negatives'])\n",
    "    mlflow.log_metric(\"test_false_positives\", test_metrics['false_positives'])\n",
    "    mlflow.log_metric(\"test_false_negatives\", test_metrics['false_negatives'])\n",
    "    \n",
    "    # M√©tricas de estabilidad\n",
    "    for metric, diff in stability_analysis.items():\n",
    "        mlflow.log_metric(f\"stability_{metric}\", diff)\n",
    "    \n",
    "    # M√©tricas de casos mal clasificados\n",
    "    mlflow.log_metric(\"misclassification_rate\", misclassification_analysis['misclassification_rate'])\n",
    "    mlflow.log_metric(\"false_positives_count\", misclassification_analysis['false_positives'])\n",
    "    mlflow.log_metric(\"false_negatives_count\", misclassification_analysis['false_negatives'])\n",
    "    \n",
    "    if 'confidence_stats' in misclassification_analysis:\n",
    "        mlflow.log_metric(\"misclassified_mean_confidence\", misclassification_analysis['confidence_stats']['mean_confidence'])\n",
    "        mlflow.log_metric(\"misclassified_std_confidence\", misclassification_analysis['confidence_stats']['std_confidence'])\n",
    "        mlflow.log_metric(\"high_confidence_errors\", misclassification_analysis['confidence_stats']['high_confidence_errors'])\n",
    "    \n",
    "    # Umbral √≥ptimo\n",
    "    mlflow.log_metric(\"optimal_threshold\", optimal_threshold)\n",
    "    mlflow.log_metric(\"optimal_f1_score\", optimal_f1)\n",
    "    \n",
    "    # Registrar par√°metros del modelo evaluado\n",
    "    mlflow.log_param(\"evaluated_model\", training_info['best_model_name'])\n",
    "    mlflow.log_param(\"model_version\", registered_model.version)\n",
    "    mlflow.log_param(\"original_experiment\", training_info['experiment_name'])\n",
    "    mlflow.log_param(\"original_run_id\", training_info['run_id'])\n",
    "    mlflow.log_param(\"test_size\", len(X_test))\n",
    "    mlflow.log_param(\"validation_size\", len(X_val))\n",
    "    mlflow.log_param(\"feature_count\", len(feature_names))\n",
    "    \n",
    "    # Registrar artefactos (gr√°ficos)\n",
    "    print(\"üñºÔ∏è Registrando visualizaciones...\")\n",
    "    mlflow.log_artifact(\"model_evaluation_dashboard.png\")\n",
    "    mlflow.log_artifact(\"feature_importance_analysis.png\")\n",
    "    mlflow.log_artifact(\"misclassification_analysis.png\")\n",
    "    \n",
    "    # Guardar resultados detallados como JSON\n",
    "    detailed_results = {\n",
    "        'model_info': {\n",
    "            'name': training_info['best_model_name'],\n",
    "            'version': registered_model.version,\n",
    "            'original_experiment': training_info['experiment_name'],\n",
    "            'original_run_id': training_info['run_id']\n",
    "        },\n",
    "        'test_metrics': test_metrics,\n",
    "        'validation_metrics': val_metrics,\n",
    "        'stability_analysis': stability_analysis,\n",
    "        'misclassification_analysis': {\n",
    "            k: v for k, v in misclassification_analysis.items() \n",
    "            if k != 'feature_differences'  # Excluir DataFrame para JSON\n",
    "        },\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'optimal_f1_score': optimal_f1,\n",
    "        'evaluation_timestamp': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Guardar como archivo JSON\n",
    "    with open('evaluation_results.json', 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=2, default=str)\n",
    "    \n",
    "    mlflow.log_artifact('evaluation_results.json')\n",
    "    \n",
    "    # Registrar top features importantes\n",
    "    if 'native' in feature_importance:\n",
    "        top_features = feature_importance['native'].head(10)\n",
    "        for i, (_, row) in enumerate(top_features.iterrows()):\n",
    "            mlflow.log_metric(f\"feature_importance_rank_{i+1}\", row['importance'])\n",
    "            mlflow.log_param(f\"top_feature_{i+1}\", row['feature'])\n",
    "    \n",
    "    current_run = mlflow.active_run()\n",
    "    evaluation_run_id = current_run.info.run_id\n",
    "    \n",
    "    print(f\"‚úÖ Evaluaci√≥n registrada en MLflow\")\n",
    "    print(f\"üìã Run ID: {evaluation_run_id}\")\n",
    "\n",
    "print(f\"\\nüìä RESUMEN FINAL DE EVALUACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üéØ Modelo evaluado: {training_info['best_model_name']}\")\n",
    "print(f\"üìã Versi√≥n: {registered_model.version}\")\n",
    "print(f\"üìä Precisi√≥n en test: {test_metrics['accuracy']:.3f}\")\n",
    "print(f\"üìä F1-Score macro: {test_metrics['f1_macro']:.3f}\")\n",
    "print(f\"üìä AUC-ROC: {test_metrics['auc_roc']:.3f}\")\n",
    "print(f\"üéØ Umbral √≥ptimo: {optimal_threshold:.3f}\")\n",
    "print(f\"‚ùå Tasa de error: {misclassification_analysis['misclassification_rate']:.3f}\")\n",
    "\n",
    "# Evaluaci√≥n de calidad del modelo\n",
    "def evaluate_model_quality(metrics):\n",
    "    \"\"\"Eval√∫a la calidad general del modelo\"\"\"\n",
    "    \n",
    "    quality_score = 0\n",
    "    recommendations = []\n",
    "    \n",
    "    # Criterios de evaluaci√≥n\n",
    "    if metrics['accuracy'] >= 0.85:\n",
    "        quality_score += 2\n",
    "    elif metrics['accuracy'] >= 0.75:\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        recommendations.append(\"Mejorar precisi√≥n general del modelo\")\n",
    "    \n",
    "    if metrics['f1_macro'] >= 0.80:\n",
    "        quality_score += 2\n",
    "    elif metrics['f1_macro'] >= 0.70:\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        recommendations.append(\"Mejorar F1-Score balanceando clases\")\n",
    "    \n",
    "    if metrics['auc_roc'] >= 0.85:\n",
    "        quality_score += 2\n",
    "    elif metrics['auc_roc'] >= 0.75:\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        recommendations.append(\"Mejorar capacidad discriminativa del modelo\")\n",
    "    \n",
    "    # Evaluar balance entre precisi√≥n y recall\n",
    "    precision_recall_diff = abs(metrics['precision'] - metrics['recall'])\n",
    "    if precision_recall_diff <= 0.1:\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        recommendations.append(\"Balancear mejor precisi√≥n y recall\")\n",
    "    \n",
    "    # Clasificaci√≥n de calidad\n",
    "    if quality_score >= 6:\n",
    "        quality_level = \"üåü EXCELENTE\"\n",
    "    elif quality_score >= 4:\n",
    "        quality_level = \"‚úÖ BUENO\"\n",
    "    elif quality_score >= 2:\n",
    "        quality_level = \"‚ö†Ô∏è ACEPTABLE\"\n",
    "    else:\n",
    "        quality_level = \"‚ùå NECESITA MEJORAS\"\n",
    "    \n",
    "    return quality_level, quality_score, recommendations\n",
    "\n",
    "quality_level, quality_score, recommendations = evaluate_model_quality(test_metrics)\n",
    "\n",
    "print(f\"\\nüèÜ EVALUACI√ìN DE CALIDAD DEL MODELO:\")\n",
    "print(f\"üìä Nivel de calidad: {quality_level}\")\n",
    "print(f\"üî¢ Puntuaci√≥n: {quality_score}/7\")\n",
    "\n",
    "if recommendations:\n",
    "    print(f\"\\nüí° RECOMENDACIONES PARA MEJORA:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "else:\n",
    "    print(f\"\\nüéâ ¬°El modelo cumple con todos los criterios de calidad!\")\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluaci√≥n completa finalizada. Archivos generados:\")\n",
    "print(f\"  - model_evaluation_dashboard.png\")\n",
    "print(f\"  - feature_importance_analysis.png\")\n",
    "print(f\"  - misclassification_analysis.png\")\n",
    "print(f\"  - evaluation_results.json\")\n",
    "print(f\"\\nüìã Run de evaluaci√≥n: {evaluation_run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. AN√ÅLISIS DE CASOS MAL CLASIFICADOS\n",
    "print(\"\\n‚ùå AN√ÅLISIS DE CASOS MAL CLASIFICADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def analyze_misclassified_cases(X_test, y_true, y_pred, y_proba, feature_names):\n",
    "    \"\"\"Analiza casos mal clasificados para identificar patrones\"\"\"\n",
    "    \n",
    "    # Identificar casos mal clasificados\n",
    "    misclassified_mask = y_true != y_pred\n",
    "    correctly_classified_mask = y_true == y_pred\n",
    "    \n",
    "    X_misclassified = X_test[misclassified_mask]\n",
    "    X_correct = X_test[correctly_classified_mask]\n",
    "    \n",
    "    y_true_misc = y_true[misclassified_mask]\n",
    "    y_pred_misc = y_pred[misclassified_mask]\n",
    "    y_proba_misc = y_proba[misclassified_mask]\n",
    "    \n",
    "    print(f\"üìä Casos mal clasificados: {len(X_misclassified)} ({len(X_misclassified)/len(X_test)*100:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis por tipo de error\n",
    "    false_positives = (y_true_misc == 0) & (y_pred_misc == 1)\n",
    "    false_negatives = (y_true_misc == 1) & (y_pred_misc == 0)\n",
    "    \n",
    "    print(f\"üî¥ Falsos Positivos: {false_positives.sum()} (predicho apto, real no apto)\")\n",
    "    print(f\"üî¥ Falsos Negativos: {false_negatives.sum()} (predicho no apto, real apto)\")\n",
    "    \n",
    "    analysis_results = {\n",
    "        'total_misclassified': len(X_misclassified),\n",
    "        'false_positives': false_positives.sum(),\n",
    "        'false_negatives': false_negatives.sum(),\n",
    "        'misclassification_rate': len(X_misclassified) / len(X_test)\n",
    "    }\n",
    "    \n",
    "    # An√°lisis de caracter√≠sticas en casos mal clasificados\n",
    "    if len(X_misclassified) > 0:\n",
    "        print(f\"\\nüìà CARACTER√çSTICAS DE CASOS MAL CLASIFICADOS:\")\n",
    "        \n",
    "        # Comparar estad√≠sticas de features entre casos correctos e incorrectos\n",
    "        feature_comparison = []\n",
    "        \n",
    "        for feature in feature_names[:20]:  # Top 20 features para evitar output muy largo\n",
    "            if feature in X_test.columns:\n",
    "                correct_mean = X_correct[feature].mean()\n",
    "                misc_mean = X_misclassified[feature].mean()\n",
    "                \n",
    "                # Test estad√≠stico simple (diferencia relativa)\n",
    "                if correct_mean != 0:\n",
    "                    relative_diff = abs(misc_mean - correct_mean) / abs(correct_mean)\n",
    "                else:\n",
    "                    relative_diff = abs(misc_mean)\n",
    "                \n",
    "                feature_comparison.append({\n",
    "                    'feature': feature,\n",
    "                    'correct_mean': correct_mean,\n",
    "                    'misclassified_mean': misc_mean,\n",
    "                    'relative_difference': relative_diff\n",
    "                })\n",
    "        \n",
    "        # Ordenar por diferencia relativa\n",
    "        feature_comparison_df = pd.DataFrame(feature_comparison)\n",
    "        feature_comparison_df = feature_comparison_df.sort_values('relative_difference', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 features con mayor diferencia entre casos correctos e incorrectos:\")\n",
    "        print(feature_comparison_df.head(10)[['feature', 'correct_mean', 'misclassified_mean', 'relative_difference']].to_string(index=False))\n",
    "        \n",
    "        analysis_results['feature_differences'] = feature_comparison_df\n",
    "    \n",
    "    # An√°lisis de confianza en predicciones incorrectas\n",
    "    if len(y_proba_misc) > 0:\n",
    "        print(f\"\\nüéØ AN√ÅLISIS DE CONFIANZA EN CASOS MAL CLASIFICADOS:\")\n",
    "        \n",
    "        # Distribuci√≥n de probabilidades en casos mal clasificados\n",
    "        conf_stats = {\n",
    "            'mean_confidence': y_proba_misc.mean(),\n",
    "            'median_confidence': np.median(y_proba_misc),\n",
    "            'std_confidence': y_proba_misc.std(),\n",
    "            'high_confidence_errors': (np.abs(y_proba_misc - 0.5) > 0.3).sum()\n",
    "        }\n",
    "        \n",
    "        print(f\"  Confianza promedio: {conf_stats['mean_confidence']:.3f}\")\n",
    "        print(f\"  Confianza mediana: {conf_stats['median_confidence']:.3f}\")\n",
    "        print(f\"  Desviaci√≥n est√°ndar: {conf_stats['std_confidence']:.3f}\")\n",
    "        print(f\"  Errores con alta confianza (>0.8 o <0.2): {conf_stats['high_confidence_errors']}\")\n",
    "        \n",
    "        analysis_results['confidence_stats'] = conf_stats\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Realizar an√°lisis\n",
    "misclassification_analysis = analyze_misclassified_cases(\n",
    "    X_test, y_test_true, y_test_pred, y_test_proba, feature_names\n",
    ")\n",
    "\n",
    "# Visualizar an√°lisis de casos mal clasificados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Distribuci√≥n de probabilidades por tipo de predicci√≥n\n",
    "axes[0,0].hist(y_test_proba[y_test_true == y_test_pred], bins=30, alpha=0.7, label='Correctos', density=True)\n",
    "axes[0,0].hist(y_test_proba[y_test_true != y_test_pred], bins=30, alpha=0.7, label='Incorrectos', density=True)\n",
    "axes[0,0].set_xlabel('Probabilidad Predicha')\n",
    "axes[0,0].set_ylabel('Densidad')\n",
    "axes[0,0].set_title('Distribuci√≥n de Probabilidades')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Boxplot de confianza por tipo de caso\n",
    "correct_conf = np.abs(y_test_proba[y_test_true == y_test_pred] - 0.5)\n",
    "incorrect_conf = np.abs(y_test_proba[y_test_true != y_test_pred] - 0.5)\n",
    "\n",
    "axes[0,1].boxplot([correct_conf, incorrect_conf], labels=['Correctos', 'Incorrectos'])\n",
    "axes[0,1].set_ylabel('Confianza (|prob - 0.5|)')\n",
    "axes[0,1].set_title('Confianza en Predicciones')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Top features con mayor diferencia\n",
    "if 'feature_differences' in misclassification_analysis:\n",
    "    top_diff_features = misclassification_analysis['feature_differences'].head(10)\n",
    "    \n",
    "    x = range(len(top_diff_features))\n",
    "    axes[1,0].bar(x, top_diff_features['relative_difference'])\n",
    "    axes[1,0].set_xticks(x)\n",
    "    axes[1,0].set_xticklabels(top_diff_features['feature'], rotation=45, ha='right')\n",
    "    axes[1,0].set_ylabel('Diferencia Relativa')\n",
    "    axes[1,0].set_title('Features con Mayor Diferencia\\nCorrect vs Misclassified')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Casos por umbral de confianza\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4]\n",
    "correct_counts = []\n",
    "incorrect_counts = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    high_conf_mask = np.abs(y_test_proba - 0.5) > thresh\n",
    "    correct_counts.append(((y_test_true == y_test_pred) & high_conf_mask).sum())\n",
    "    incorrect_counts.append(((y_test_true != y_test_pred) & high_conf_mask).sum())\n",
    "\n",
    "x = np.arange(len(thresholds))\n",
    "width = 0.35\n",
    "\n",
    "axes[1,1].bar(x - width/2, correct_counts, width, label='Correctos', alpha=0.8)\n",
    "axes[1,1].bar(x + width/2, incorrect_counts, width, label='Incorrectos', alpha=0.8)\n",
    "axes[1,1].set_xlabel('Umbral de Confianza')\n",
    "axes[1,1].set_ylabel('N√∫mero de Casos')\n",
    "axes[1,1].set_title('Casos por Nivel de Confianza')\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels([f'>{t}' for t in thresholds])\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('misclassification_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ An√°lisis de casos mal clasificados guardado como 'misclassification_analysis.png'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
